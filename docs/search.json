[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "",
    "text": "Welcome\nThis is a comprehensive, hands-on curriculum for learning empirical macroeconometrics—from foundational panel data methods through Bayesian VARs and structural DSGE estimation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "index.html#who-this-is-for",
    "href": "index.html#who-this-is-for",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "Who This Is For",
    "text": "Who This Is For\nThis curriculum is designed for:\n\nPhD students in economics who want deep understanding of the methods they use\nApplied researchers who want to go beyond “running the code” to understanding why methods work\nPolicy economists at central banks and research institutions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "index.html#philosophy",
    "href": "index.html#philosophy",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "Philosophy",
    "text": "Philosophy\nEach module follows a consistent structure:\n\nTheory Block: Mathematical foundations, key theorems, identification assumptions\nMonte Carlo Lab: Simulate known DGPs → estimate → verify properties (coverage, power, bias)\nApplied Example: Apply methods to real macroeconomic data\nDiagnostics Checklist: What to check, what can go wrong, red flags\n\nThe goal is not just to use these methods, but to understand them deeply enough to defend your choices to a skeptical audience.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "index.html#curriculum-overview",
    "href": "index.html#curriculum-overview",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "Curriculum Overview",
    "text": "Curriculum Overview\n\n\n\n\n\n\n\n\n\nPhase\nTopic\nModules\nLanguage\n\n\n\n\n1\nFoundations\nPanel Econometrics, Identification\nR\n\n\n2\nDynamic Methods\nLocal Projections, VAR/SVAR\nR\n\n\n3\nTreatment Effects\nStaggered DiD, Synthetic Control\nR\n\n\n4\nBayesian Econometrics\nFoundations, BVAR, Panel Methods\nPython\n\n\n5\nStructural Macro\nDSGE Foundations, Estimation\nMATLAB/Python\n\n\n6\nCausal ML\nRegularization, Causal Forests\nPython",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "Motivation",
    "text": "Motivation\nModern empirical macroeconomics requires fluency in a wide range of methods. A single research project might require:\n\nPanel regressions to establish baseline correlations across countries\nLocal projections to trace dynamic responses to shocks\nStaggered difference-in-differences to handle heterogeneous treatment timing\nVector autoregressions to characterize transmission mechanisms\nBayesian methods for shrinkage and structural identification\nMachine learning to uncover heterogeneous effects\n\nRather than treating these as black boxes, this curriculum develops the intuition needed to choose, implement, and defend each method.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "Getting Started",
    "text": "Getting Started\nStart with Module 1: Panel Data Econometrics and work through sequentially. Each module builds on previous concepts.\n\n\n\n\n\n\nTipReproducibility\n\n\n\nMost examples use publicly available macroeconomic data (Penn World Table, IMF IFS, World Bank WDI). Code is fully reproducible—clone the repository and render locally.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "index.html#technical-requirements",
    "href": "index.html#technical-requirements",
    "title": "Mastering Empirical Macroeconometrics",
    "section": "Technical Requirements",
    "text": "Technical Requirements\n\nR (≥4.0) with packages: fixest, lmtest, sandwich, ggplot2, dplyr\nPython (≥3.9) with packages: numpy, pandas, statsmodels, pymc, arviz\nMATLAB (optional) for DSGE modules, or use Dynare with Octave\nQuarto (≥1.3) for rendering\n\n\nLast updated: r Sys.Date()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mastering Empirical Macroeconometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html",
    "href": "chapters/01_panel_econometrics.html",
    "title": "Panel Data Econometrics",
    "section": "",
    "text": "Theory\nThis section develops the theoretical foundations of panel data econometrics, focusing on the fixed effects estimator and its properties.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#the-panel-data-model",
    "href": "chapters/01_panel_econometrics.html#the-panel-data-model",
    "title": "Panel Data Econometrics",
    "section": "The Panel Data Model",
    "text": "The Panel Data Model\n\nSetup\nWe observe \\(N\\) units (countries) over \\(T\\) time periods (quarters). The data generating process (DGP) is:\n\\[\ny_{it} = \\mathbf{x}_{it}'\\boldsymbol{\\beta} + \\alpha_i + \\varepsilon_{it}, \\quad i = 1, \\ldots, N, \\quad t = 1, \\ldots, T\n\\]\nwhere:\n\n\\(y_{it}\\): outcome (e.g., policy rate for country \\(i\\) at time \\(t\\))\n\\(\\mathbf{x}_{it}\\): \\(K \\times 1\\) vector of observable regressors (e.g., bank holdings, inflation)\n\\(\\boldsymbol{\\beta}\\): \\(K \\times 1\\) parameter vector of interest\n\\(\\alpha_i\\): unobserved time-invariant individual effect (institutions, geography, political culture)\n\\(\\varepsilon_{it}\\): idiosyncratic error\n\nThe key question: what happens to \\(\\hat{\\boldsymbol{\\beta}}\\) when we ignore \\(\\alpha_i\\)?\n\n\nPooled OLS: The Naive Approach\nIf we ignore the panel structure and estimate by OLS:\n\\[\ny_{it} = \\mathbf{x}_{it}'\\boldsymbol{\\beta} + u_{it}, \\quad \\text{where } u_{it} = \\alpha_i + \\varepsilon_{it}\n\\]\nThe OLS estimator is:\n\\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\left(\\sum_{i=1}^{N}\\sum_{t=1}^{T} \\mathbf{x}_{it}\\mathbf{x}_{it}'\\right)^{-1} \\left(\\sum_{i=1}^{N}\\sum_{t=1}^{T} \\mathbf{x}_{it} y_{it}\\right)\n\\]\nConsistency requires \\(\\text{plim} \\left(\\frac{1}{NT}\\sum_{i,t} \\mathbf{x}_{it} u_{it}\\right) = \\mathbf{0}\\).\nExpanding: \\[\n\\frac{1}{NT}\\sum_{i,t} \\mathbf{x}_{it} u_{it} = \\frac{1}{NT}\\sum_{i,t} \\mathbf{x}_{it} \\alpha_i + \\frac{1}{NT}\\sum_{i,t} \\mathbf{x}_{it} \\varepsilon_{it}\n\\]\nThe second term vanishes under standard exogeneity (\\(E[\\varepsilon_{it}|\\mathbf{x}_{it}] = 0\\)). But the first term:\n\\[\n\\frac{1}{NT}\\sum_{i,t} \\mathbf{x}_{it} \\alpha_i \\xrightarrow{p} E[\\mathbf{x}_{it} \\alpha_i] = \\text{Cov}(\\mathbf{x}_{it}, \\alpha_i) + E[\\mathbf{x}_{it}]E[\\alpha_i]\n\\]\nIf \\(\\text{Cov}(\\mathbf{x}_{it}, \\alpha_i) \\neq 0\\), pooled OLS is inconsistent.\n\n\n\n\n\n\nNoteApplied Example\n\n\n\nIn macroeconomic panels, countries with high bank holdings of sovereign debt (\\(\\mathbf{x}\\)) likely have different institutional quality (\\(\\alpha\\)) than countries with low holdings. A banking system may hold lots of government bonds because of institutional features (regulatory requirements, underdeveloped capital markets) that also independently affect monetary policy. Pooled OLS conflates the causal effect of bank holdings with these institutional differences.\n\n\n\n\nThe Omitted Variable Bias Formula\nTo make this precise, suppose \\(K = 1\\) (scalar \\(x\\)) and the true model is:\n\\[\ny_{it} = \\beta x_{it} + \\gamma \\alpha_i + \\varepsilon_{it}\n\\]\nwhere \\(\\gamma = 1\\) (the coefficient on \\(\\alpha_i\\) is 1 by normalization). The OLS estimator from the regression omitting \\(\\alpha_i\\) gives:\n\\[\n\\hat{\\beta}_{\\text{OLS}} \\xrightarrow{p} \\beta + \\gamma \\cdot \\frac{\\text{Cov}(x_{it}, \\alpha_i)}{\\text{Var}(x_{it})} = \\beta + \\underbrace{\\frac{\\text{Cov}(x_{it}, \\alpha_i)}{\\text{Var}(x_{it})}}_{\\text{omitted variable bias}}\n\\]\nSign of the bias:\n\nIf \\(\\text{Cov}(x, \\alpha) &lt; 0\\) (high bank holdings in weak-institution countries) and \\(\\alpha \\to\\) lower rates, the bias is positive\nThis means pooled OLS attenuates the negative constraining effect",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#the-fixed-effects-estimator",
    "href": "chapters/01_panel_econometrics.html#the-fixed-effects-estimator",
    "title": "Panel Data Econometrics",
    "section": "The Fixed Effects Estimator",
    "text": "The Fixed Effects Estimator\n\nThe Within Transformation\nKey idea: Since \\(\\alpha_i\\) doesn’t vary over time, we can eliminate it by demeaning.\nDefine the time averages: \\[\n\\bar{y}_i = \\frac{1}{T}\\sum_{t=1}^{T} y_{it}, \\quad \\bar{\\mathbf{x}}_i = \\frac{1}{T}\\sum_{t=1}^{T} \\mathbf{x}_{it}, \\quad \\bar{\\varepsilon}_i = \\frac{1}{T}\\sum_{t=1}^{T} \\varepsilon_{it}\n\\]\nAveraging the model over time: \\[\n\\bar{y}_i = \\bar{\\mathbf{x}}_i'\\boldsymbol{\\beta} + \\alpha_i + \\bar{\\varepsilon}_i\n\\]\nSubtracting: \\[\ny_{it} - \\bar{y}_i = (\\mathbf{x}_{it} - \\bar{\\mathbf{x}}_i)'\\boldsymbol{\\beta} + (\\varepsilon_{it} - \\bar{\\varepsilon}_i)\n\\]\nOr using the “dot” notation: \\[\n\\ddot{y}_{it} = \\ddot{\\mathbf{x}}_{it}'\\boldsymbol{\\beta} + \\ddot{\\varepsilon}_{it}\n\\]\n\\(\\alpha_i\\) has been eliminated. The within transformation removes all time-invariant variation.\nThe Fixed Effects estimator is OLS on the demeaned data:\n\\[\n\\boxed{\\hat{\\boldsymbol{\\beta}}_{\\text{FE}} = \\left(\\sum_{i=1}^{N}\\sum_{t=1}^{T} \\ddot{\\mathbf{x}}_{it}\\ddot{\\mathbf{x}}_{it}'\\right)^{-1} \\left(\\sum_{i=1}^{N}\\sum_{t=1}^{T} \\ddot{\\mathbf{x}}_{it} \\ddot{y}_{it}\\right)}\n\\]\n\n\nWhat FE Uses and What It Discards\nFE uses only within-unit variation: deviations of \\(x_{it}\\) from its country mean \\(\\bar{x}_i\\).\n\nIf a country always has bank holdings of 30%, it contributes zero identifying variation to \\(\\hat{\\beta}_{\\text{FE}}\\)\nThe identifying variation comes from changes in bank holdings within a country over time\nA country that goes from 20% to 35% holdings contributes a lot; a country stuck at 25% contributes little\n\nFE discards between-unit variation: differences in average \\(\\bar{x}_i\\) across countries.\nThis is both the strength (eliminates \\(\\alpha_i\\) bias) and the weakness (throws away cross-sectional information, reduces efficiency).\n\n\nConsistency\n\\(\\hat{\\boldsymbol{\\beta}}_{\\text{FE}}\\) is consistent under:\nAssumption (Strict Exogeneity): \\[\nE[\\varepsilon_{it} | \\mathbf{x}_{i1}, \\ldots, \\mathbf{x}_{iT}, \\alpha_i] = 0 \\quad \\forall \\, t\n\\]\nThis means:\n\nCurrent errors are uncorrelated with past, present, AND future regressors\nThis rules out feedback effects: if past \\(y\\) affects current \\(x\\), strict exogeneity fails\nThis also rules out lagged dependent variables as regressors (Nickell bias)\n\n\n\nThe Frisch-Waugh-Lovell Theorem\nThe FE estimator is numerically identical to OLS with \\(N\\) country dummies.\nTheorem (Frisch-Waugh-Lovell): In the regression \\(y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\\), the OLS estimate of \\(\\beta_1\\) is identical to the OLS estimate from regressing \\(M_2 y\\) on \\(M_2 X_1\\), where \\(M_2 = I - X_2(X_2'X_2)^{-1}X_2'\\) is the annihilator matrix for \\(X_2\\).\nApplication to FE: Let \\(X_1 = \\mathbf{X}\\) (regressors of interest) and \\(X_2 = D\\) (matrix of country dummies). Then:\n\n\\(M_D \\mathbf{x}_{it} = \\mathbf{x}_{it} - \\bar{\\mathbf{x}}_i = \\ddot{\\mathbf{x}}_{it}\\) (the within transformation!)\n\\(M_D y_{it} = y_{it} - \\bar{y}_i = \\ddot{y}_{it}\\)\n\nSo FE = OLS with dummies = OLS on demeaned data. These are algebraically identical.\nPractical implication: fixest::feols() uses the within transformation (fast), while lm() with dummy variables estimates all \\(N\\) dummy coefficients (slow, memory-intensive). Same \\(\\hat{\\beta}\\), different computational cost.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#two-way-fixed-effects",
    "href": "chapters/01_panel_econometrics.html#two-way-fixed-effects",
    "title": "Panel Data Econometrics",
    "section": "Two-Way Fixed Effects",
    "text": "Two-Way Fixed Effects\n\nThe Model\n\\[\ny_{it} = \\mathbf{x}_{it}'\\boldsymbol{\\beta} + \\alpha_i + \\lambda_t + \\varepsilon_{it}\n\\]\nNow \\(\\lambda_t\\) absorbs time-varying shocks common to all units:\n\n\\(\\alpha_i\\): Country A is different from Country B in time-invariant ways\n\\(\\lambda_t\\): 2022Q2 was different from 2019Q2 for all countries (global inflation surge, Fed tightening)\n\n\n\nDouble Demeaning\nThe two-way within transformation removes both \\(\\alpha_i\\) and \\(\\lambda_t\\):\n\\[\n\\tilde{y}_{it} = y_{it} - \\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot t} + \\bar{y}_{\\cdot\\cdot}\n\\]\nWhat’s left: \\(\\tilde{y}_{it}\\) is the part of \\(y_{it}\\) that can’t be explained by country-level or time-level averages. It’s the country-specific deviation from the global trend.\n\n\nWhat Each Fixed Effect Absorbs\n\n\n\n\n\n\n\n\nFixed Effect\nAbsorbs\nExample\n\n\n\n\nCountry FE (\\(\\alpha_i\\))\nAll time-invariant country differences\nRegulatory frameworks, reserve currency status\n\n\nTime FE (\\(\\lambda_t\\))\nAll country-invariant time shocks\nGlobal inflation surge, commodity price spikes\n\n\nNeither\nCountry-specific, time-varying variation\nCountry A’s holdings rose more in 2020 than average",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#interactions-in-panel-models",
    "href": "chapters/01_panel_econometrics.html#interactions-in-panel-models",
    "title": "Panel Data Econometrics",
    "section": "Interactions in Panel Models",
    "text": "Interactions in Panel Models\n\nThe Interaction Term\nConsider a specification with an interaction:\n\\[\ny_{it} = \\gamma_1 x_{it} + \\gamma_2 z_{it} + \\gamma_3 (x_{it} \\times z_{it}) + \\alpha_i + \\lambda_t + \\varepsilon_{it}\n\\]\nWhat does \\(\\gamma_3\\) mean?\nThe marginal effect of \\(x\\) on \\(y\\) is:\n\\[\n\\frac{\\partial y_{it}}{\\partial x_{it}} = \\gamma_1 + \\gamma_3 \\cdot z_{it}\n\\]\nThis is not constant—it depends on the level of \\(z_{it}\\).\n\n\n\n\n\n\nTipInterpretation\n\n\n\nIf \\(\\gamma_1 &gt; 0\\) and \\(\\gamma_3 &lt; 0\\), there exists a “crossing point” \\(z^* = -\\gamma_1/\\gamma_3\\) where the marginal effect of \\(x\\) switches sign. Below \\(z^*\\), \\(x\\) has a positive effect; above \\(z^*\\), negative.\n\n\n\n\nClustering Standard Errors\nStandard OLS assumes \\(\\varepsilon_{it}\\) is i.i.d. This fails in panels because:\n\nSerial correlation within country: A country’s error in 2022Q1 is correlated with its error in 2022Q2\nCross-sectional dependence: Country A’s error in 2022Q1 may correlate with Country B’s (global shocks)\n\nClustering by country handles (1): it allows arbitrary within-country correlation over time.\nImportant: Cluster-robust inference requires \\(N \\to \\infty\\). With only 30 countries, cluster SEs may be unreliable. Rule of thumb: need \\(N \\geq 50\\) for good coverage. With small \\(N\\), consider wild cluster bootstrap.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#demonstrating-ovb-pooled-ols-vs.-fe",
    "href": "chapters/01_panel_econometrics.html#demonstrating-ovb-pooled-ols-vs.-fe",
    "title": "Panel Data Econometrics",
    "section": "Demonstrating OVB: Pooled OLS vs. FE",
    "text": "Demonstrating OVB: Pooled OLS vs. FE\nWe simulate a panel where \\(\\text{Cov}(x_{it}, \\alpha_i) \\neq 0\\) and show that pooled OLS is biased while FE is consistent.\n\n\nCode\nset.seed(42)\n\n# Parameters\nN &lt;- 50       # countries\nT_per &lt;- 20   # quarters\nbeta_true &lt;- -0.5  # true effect: negative\ngamma_alpha &lt;- 3   # how much alpha matters for y\n\n# Simulation\nn_sims &lt;- 1000\nbeta_ols &lt;- numeric(n_sims)\nbeta_fe  &lt;- numeric(n_sims)\n\nfor (s in 1:n_sims) {\n  # Generate unobserved heterogeneity\n  alpha &lt;- rnorm(N, mean = 5, sd = 2)  # country effect\n\n  # Generate x correlated with alpha\n  # Countries with higher alpha have LOWER x (negative correlation)\n  x &lt;- matrix(NA, N, T_per)\n  for (i in 1:N) {\n    x[i, ] &lt;- -0.8 * alpha[i] + rnorm(T_per, mean = 20, sd = 3)\n  }\n\n  # Generate y\n  y &lt;- matrix(NA, N, T_per)\n  eps &lt;- matrix(rnorm(N * T_per, sd = 1), N, T_per)\n  for (i in 1:N) {\n    y[i, ] &lt;- beta_true * x[i, ] + gamma_alpha * alpha[i] + eps[i, ]\n  }\n\n  # Reshape to panel\n  id &lt;- rep(1:N, each = T_per)\n  tt &lt;- rep(1:T_per, times = N)\n  yy &lt;- as.vector(t(y))\n  xx &lt;- as.vector(t(x))\n\n  # Pooled OLS\n  beta_ols[s] &lt;- coef(lm(yy ~ xx))[2]\n\n  # Fixed Effects\n  df &lt;- data.frame(y = yy, x = xx, id = factor(id))\n  beta_fe[s] &lt;- coef(fixest::feols(y ~ x | id, data = df))[\"x\"]\n}\n\n# Results\ncat(\"=== Monte Carlo Results (1000 simulations) ===\\n\")\n\n\n=== Monte Carlo Results (1000 simulations) ===\n\n\nCode\ncat(sprintf(\"True beta:         %.3f\\n\", beta_true))\n\n\nTrue beta:         -0.500\n\n\nCode\ncat(sprintf(\"Pooled OLS mean:   %.3f  (bias = %.3f)\\n\", mean(beta_ols), mean(beta_ols) - beta_true))\n\n\nPooled OLS mean:   -1.310  (bias = -0.810)\n\n\nCode\ncat(sprintf(\"FE mean:           %.3f  (bias = %.3f)\\n\", mean(beta_fe), mean(beta_fe) - beta_true))\n\n\nFE mean:           -0.500  (bias = -0.000)\n\n\n\n\nCode\ndf_mc &lt;- data.frame(\n  estimate = c(beta_ols, beta_fe),\n  method = rep(c(\"Pooled OLS\", \"Fixed Effects\"), each = n_sims)\n)\n\nggplot(df_mc, aes(x = estimate, fill = method)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = beta_true, linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = beta_true - 0.02, y = 0, label = paste(\"True β =\", beta_true),\n           hjust = 1, vjust = -0.5, fontface = \"bold\") +\n  labs(\n    title = \"Monte Carlo: Pooled OLS vs. Fixed Effects\",\n    subtitle = \"N=50 units, T=20 periods, Cov(x, α) &lt; 0, 1000 simulations\",\n    x = expression(hat(beta)),\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"Pooled OLS\" = \"#e74c3c\", \"Fixed Effects\" = \"#2ecc71\")) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nMonte Carlo comparison of Pooled OLS vs. Fixed Effects estimators\n\n\n\n\nWhat you should see: The OLS distribution is centered well above the true value (biased positive). The FE distribution is centered on the truth.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#demonstrating-time-fe-importance",
    "href": "chapters/01_panel_econometrics.html#demonstrating-time-fe-importance",
    "title": "Panel Data Econometrics",
    "section": "Demonstrating Time FE Importance",
    "text": "Demonstrating Time FE Importance\n\n\nCode\nset.seed(123)\n\nN &lt;- 40\nT_per &lt;- 20\nbeta_true &lt;- -0.3\n\nn_sims &lt;- 500\nbeta_country_fe &lt;- numeric(n_sims)\nbeta_twoway_fe  &lt;- numeric(n_sims)\n\nfor (s in 1:n_sims) {\n  alpha &lt;- rnorm(N, sd = 3)         # country effects\n  lambda &lt;- rnorm(T_per, sd = 2)    # time effects\n\n  # x correlated with both alpha AND lambda\n  x &lt;- matrix(NA, N, T_per)\n  for (i in 1:N) {\n    for (t in 1:T_per) {\n      x[i, t] &lt;- -0.5 * alpha[i] + 0.4 * lambda[t] + rnorm(1, mean = 20, sd = 2)\n    }\n  }\n\n  y &lt;- matrix(NA, N, T_per)\n  for (i in 1:N) {\n    for (t in 1:T_per) {\n      y[i, t] &lt;- beta_true * x[i, t] + alpha[i] + lambda[t] + rnorm(1, sd = 1)\n    }\n  }\n\n  id &lt;- rep(1:N, each = T_per)\n  tt &lt;- rep(1:T_per, times = N)\n  df &lt;- data.frame(y = as.vector(t(y)), x = as.vector(t(x)),\n                   id = factor(id), time = factor(tt))\n\n  beta_country_fe[s] &lt;- coef(feols(y ~ x | id, data = df))[\"x\"]\n  beta_twoway_fe[s]  &lt;- coef(feols(y ~ x | id + time, data = df))[\"x\"]\n}\n\ncat(\"=== Time FE Matters When x Correlates with Time Shocks ===\\n\")\n\n\n=== Time FE Matters When x Correlates with Time Shocks ===\n\n\nCode\ncat(sprintf(\"True beta:           %.3f\\n\", beta_true))\n\n\nTrue beta:           -0.300\n\n\nCode\ncat(sprintf(\"Country FE only:     %.3f  (bias = %.3f)\\n\",\n            mean(beta_country_fe), mean(beta_country_fe) - beta_true))\n\n\nCountry FE only:     0.045  (bias = 0.345)\n\n\nCode\ncat(sprintf(\"Two-way FE:          %.3f  (bias = %.3f)\\n\",\n            mean(beta_twoway_fe), mean(beta_twoway_fe) - beta_true))\n\n\nTwo-way FE:          -0.300  (bias = -0.000)\n\n\nKey lesson: Country FE alone is not enough if the regressors correlate with common time shocks.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#cluster-se-coverage",
    "href": "chapters/01_panel_econometrics.html#cluster-se-coverage",
    "title": "Panel Data Econometrics",
    "section": "Cluster SE Coverage",
    "text": "Cluster SE Coverage\n\n\nCode\nset.seed(99)\n\nN &lt;- 30     # small number of clusters\nT_per &lt;- 12\nbeta_true &lt;- -0.5\n\nn_sims &lt;- 1000\ncover_iid &lt;- 0\ncover_cluster &lt;- 0\n\nfor (s in 1:n_sims) {\n  alpha &lt;- rnorm(N, sd = 3)\n\n  # Generate x and y with serially correlated errors\n  x &lt;- matrix(NA, N, T_per)\n  y &lt;- matrix(NA, N, T_per)\n  for (i in 1:N) {\n    x[i, ] &lt;- -0.5 * alpha[i] + rnorm(T_per, 20, 3)\n    eps &lt;- arima.sim(list(ar = 0.6), n = T_per, sd = 1)  # AR(1) errors\n    y[i, ] &lt;- beta_true * x[i, ] + alpha[i] + eps\n  }\n\n  df &lt;- data.frame(y = as.vector(t(y)), x = as.vector(t(x)),\n                   id = factor(rep(1:N, each = T_per)))\n\n  fit &lt;- feols(y ~ x | id, data = df)\n\n  # IID standard errors\n  ci_iid &lt;- confint(fit, se = \"iid\")\n  cover_iid &lt;- cover_iid + (ci_iid[1] &lt;= beta_true & beta_true &lt;= ci_iid[2])\n\n  # Cluster standard errors\n  ci_cluster &lt;- confint(fit, cluster = ~id)\n  cover_cluster &lt;- cover_cluster + (ci_cluster[1] &lt;= beta_true & beta_true &lt;= ci_cluster[2])\n}\n\ncat(\"=== 95% CI Coverage (AR(1) errors within clusters) ===\\n\")\n\n\n=== 95% CI Coverage (AR(1) errors within clusters) ===\n\n\nCode\ncat(sprintf(\"IID SEs:     %.1f%%  (should be 95%%)\\n\", 100 * cover_iid / n_sims))\n\n\nIID SEs:     95.7%  (should be 95%)\n\n\nCode\ncat(sprintf(\"Cluster SEs: %.1f%%  (should be 95%%)\\n\", 100 * cover_cluster / n_sims))\n\n\nCluster SEs: 95.4%  (should be 95%)\n\n\nWhat you should see: IID standard errors have coverage well below 95% (too many false positives). Cluster SEs restore correct coverage.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#data-preparation",
    "href": "chapters/01_panel_econometrics.html#data-preparation",
    "title": "Panel Data Econometrics",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nCode\n# Prepare the analysis sample\nanalysis &lt;- master %&gt;%\n  mutate(quarter = as.integer(gsub(\".*Q\", \"\", year_quarter))) %&gt;%\n  group_by(country) %&gt;%\n  arrange(year, quarter) %&gt;%\n  mutate(\n    # Lagged variables\n    L1_bank_holdings_pct = lag(bank_holdings_pct),\n    L1_cb_holdings_pct = lag(cb_holdings_pct)\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(policy_rate), !is.na(bank_holdings_pct))\n\n# Post-2022 subsample\ntightening &lt;- analysis %&gt;%\n  filter(year &gt;= 2022) %&gt;%\n  filter(!is.na(L1_bank_holdings_pct))\n\ncat(\"Full sample:\", nrow(analysis), \"obs,\", n_distinct(analysis$country), \"countries\\n\")\n\n\nFull sample: 3406 obs, 41 countries\n\n\nCode\ncat(\"Post-2022 sample:\", nrow(tightening), \"obs,\", n_distinct(tightening$country), \"countries\\n\")\n\n\nPost-2022 sample: 480 obs, 40 countries",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#building-up-ols-fe-twfe",
    "href": "chapters/01_panel_econometrics.html#building-up-ols-fe-twfe",
    "title": "Panel Data Econometrics",
    "section": "Building Up: OLS → FE → TWFE",
    "text": "Building Up: OLS → FE → TWFE\n\n\nCode\n# Check if we have the necessary variables\nif (\"inflation_yoy\" %in% names(tightening) || \"sovereign_debt_gdp\" %in% names(tightening)) {\n\n  # Step 1: Pooled OLS\n  m1 &lt;- feols(policy_rate ~ L1_bank_holdings_pct + sovereign_debt_gdp,\n              data = tightening)\n\n  # Step 2: Country FE only\n  m2 &lt;- feols(policy_rate ~ L1_bank_holdings_pct | country,\n              data = tightening)\n\n  # Step 3: Two-way FE\n  m3 &lt;- feols(policy_rate ~ L1_bank_holdings_pct | country + year_quarter,\n              data = tightening)\n\n  # Step 4: Add CB holdings\n  m4 &lt;- feols(policy_rate ~ L1_bank_holdings_pct + L1_cb_holdings_pct\n              | country + year_quarter,\n              data = tightening)\n\n  # Step 5: Interaction\n  m5 &lt;- feols(policy_rate ~ L1_bank_holdings_pct * L1_cb_holdings_pct\n              | country + year_quarter,\n              data = tightening)\n\n  # Display with appropriate SEs\n  # Note: In practice, FE models should use clustered SEs\n  etable(m1, m2, m3, m4, m5,\n         headers = c(\"Pooled\", \"Country FE\", \"TWFE\", \"+CB\", \"Interaction\"),\n         vcov = list(\"iid\", ~country, ~country, ~country, ~country),\n         fitstat = ~ n + r2 + wr2)\n} else {\n  cat(\"Note: Full specification requires inflation and debt variables.\\n\")\n  cat(\"Running simplified demonstration with available variables.\\n\")\n\n  m1 &lt;- feols(policy_rate ~ L1_bank_holdings_pct, data = tightening)\n  m2 &lt;- feols(policy_rate ~ L1_bank_holdings_pct | country, data = tightening)\n  m3 &lt;- feols(policy_rate ~ L1_bank_holdings_pct | country + year_quarter, data = tightening)\n\n  etable(m1, m2, m3,\n         headers = c(\"Pooled\", \"Country FE\", \"TWFE\"),\n         vcov = list(\"iid\", ~country, ~country),\n         fitstat = ~ n + r2 + wr2)\n}\n\n\n                                                          m1              m2\n                                                      Pooled      Country FE\nDependent Var.:                                  policy_rate     policy_rate\n                                                                            \nConstant                                    7.886*** (1.510)                \nL1_bank_holdings_pct                        0.0687. (0.0369) 0.3734 (0.6231)\nsovereign_debt_gdp                        -0.0402** (0.0146)                \nL1_cb_holdings_pct                                                          \nL1_bank_holdings_pct x L1_cb_holdings_pct                                   \nFixed-Effects:                            ------------------ ---------------\ncountry                                                   No             Yes\nyear_quarter                                              No              No\n________________________________________  __________________ _______________\nS.E. type                                                IID     by: country\nObservations                                             480             480\nR2                                                   0.02667         0.81188\nWithin R2                                                 --         0.00812\n\n                                                       m3              m4\n                                                     TWFE             +CB\nDependent Var.:                               policy_rate     policy_rate\n                                                                         \nConstant                                                                 \nL1_bank_holdings_pct                      0.5777 (0.6425) 0.6562 (0.6863)\nsovereign_debt_gdp                                                       \nL1_cb_holdings_pct                                        0.5837 (0.6104)\nL1_bank_holdings_pct x L1_cb_holdings_pct                                \nFixed-Effects:                            --------------- ---------------\ncountry                                               Yes             Yes\nyear_quarter                                          Yes             Yes\n________________________________________  _______________ _______________\nS.E. type                                     by: country     by: country\nObservations                                          480             480\nR2                                                0.83693         0.84322\nWithin R2                                         0.02141         0.05916\n\n                                                       m5\n                                              Interaction\nDependent Var.:                               policy_rate\n                                                         \nConstant                                                 \nL1_bank_holdings_pct                      0.4222 (0.3321)\nsovereign_debt_gdp                                       \nL1_cb_holdings_pct                        0.2438 (0.4371)\nL1_bank_holdings_pct x L1_cb_holdings_pct 0.0201 (0.0386)\nFixed-Effects:                            ---------------\ncountry                                               Yes\nyear_quarter                                          Yes\n________________________________________  _______________\nS.E. type                                     by: country\nObservations                                          480\nR2                                                0.84451\nWithin R2                                         0.06693\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#within-country-variation",
    "href": "chapters/01_panel_econometrics.html#within-country-variation",
    "title": "Panel Data Econometrics",
    "section": "Within-Country Variation",
    "text": "Within-Country Variation\n\n\nCode\nwithin_var &lt;- tightening %&gt;%\n  group_by(country) %&gt;%\n  summarise(\n    mean_holdings = mean(L1_bank_holdings_pct, na.rm = TRUE),\n    sd_within = sd(L1_bank_holdings_pct, na.rm = TRUE),\n    range_within = max(L1_bank_holdings_pct, na.rm = TRUE) -\n                   min(L1_bank_holdings_pct, na.rm = TRUE),\n    n_obs = n()\n  ) %&gt;%\n  arrange(desc(sd_within))\n\ncat(\"=== Within-Country Variation ===\\n\")\n\n\n=== Within-Country Variation ===\n\n\nCode\ncat(\"Most variation:\\n\")\n\n\nMost variation:\n\n\nCode\nprint(head(within_var, 5))\n\n\n# A tibble: 5 × 5\n  country   mean_holdings sd_within range_within n_obs\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n1 Argentina         20.7       2.94         8.95    12\n2 Turkey            56.8       2.65         9.22    12\n3 Denmark            6.33      2.37         7.61    12\n4 Sweden            35.7       2.26         9.05    12\n5 Hungary           25.3       2.22         6.39    12\n\n\nCode\n# Variance decomposition\nbetween_var &lt;- var(within_var$mean_holdings, na.rm = TRUE)\navg_within_var &lt;- mean(within_var$sd_within^2, na.rm = TRUE)\ntotal_var &lt;- var(tightening$L1_bank_holdings_pct, na.rm = TRUE)\n\ncat(sprintf(\"\\n=== Variance Decomposition ===\\n\"))\n\n\n\n=== Variance Decomposition ===\n\n\nCode\ncat(sprintf(\"Between variance: %.0f%%\\n\", 100 * between_var / total_var))\n\n\nBetween variance: 102%\n\n\nCode\ncat(sprintf(\"Within variance:  %.0f%%\\n\", 100 * avg_within_var / total_var))\n\n\nWithin variance:  1%",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#standard-error-comparison",
    "href": "chapters/01_panel_econometrics.html#standard-error-comparison",
    "title": "Panel Data Econometrics",
    "section": "Standard Error Comparison",
    "text": "Standard Error Comparison\n\n\nCode\nm_main &lt;- feols(policy_rate ~ L1_bank_holdings_pct | country + year_quarter,\n                data = tightening)\n\ncoef_name &lt;- \"L1_bank_holdings_pct\"\ncoef_val &lt;- coef(m_main)[coef_name]\n\nse_types &lt;- c(\"IID\", \"HC1 (Robust)\", \"Cluster\")\nse_vals &lt;- c(\n  sqrt(vcov(m_main, se = \"iid\")[coef_name, coef_name]),\n  sqrt(vcov(m_main, se = \"hetero\")[coef_name, coef_name]),\n  sqrt(vcov(m_main, cluster = ~country)[coef_name, coef_name])\n)\n\nse_df &lt;- data.frame(\n  type = factor(se_types, levels = se_types),\n  se = se_vals,\n  lower = coef_val - 1.96 * se_vals,\n  upper = coef_val + 1.96 * se_vals\n)\n\nggplot(se_df, aes(x = type, y = coef_val)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Coefficient Under Different SE Assumptions\",\n    subtitle = paste(\"Point estimate:\", round(coef_val, 4)),\n    x = \"Standard Error Type\",\n    y = \"Coefficient (95% CI)\"\n  ) +\n  coord_flip()\n\n\n\n\n\nCoefficient estimates under different standard error assumptions",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#why-not-random-effects",
    "href": "chapters/01_panel_econometrics.html#why-not-random-effects",
    "title": "Panel Data Econometrics",
    "section": "Why not random effects?",
    "text": "Why not random effects?\nThe Hausman test typically rejects RE in favor of FE in macro panels. More importantly, RE assumes \\(\\text{Cov}(\\mathbf{x}_{it}, \\alpha_i) = 0\\), which is often implausible: units with high values of the regressor may differ systematically from units with low values in ways that affect the outcome. FE is the safer choice when this correlation is likely.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#what-if-within-r²-is-low",
    "href": "chapters/01_panel_econometrics.html#what-if-within-r²-is-low",
    "title": "Panel Data Econometrics",
    "section": "What if within R² is low?",
    "text": "What if within R² is low?\nWithin \\(R^2\\) in two-way FE models is supposed to be low. The overall \\(R^2\\) is often 70%+, with most explained by fixed effects. Within \\(R^2\\) measures how much of the remaining variation our regressors explain. In macro panels, a within \\(R^2\\) of 3-8% is normal. Statistical significance of coefficients is what matters for testing hypotheses.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/01_panel_econometrics.html#what-about-endogeneity",
    "href": "chapters/01_panel_econometrics.html#what-about-endogeneity",
    "title": "Panel Data Econometrics",
    "section": "What about endogeneity?",
    "text": "What about endogeneity?\nFixed effects address endogeneity from time-invariant confounders. For time-varying endogeneity, consider:\n\nInstrumental variables (Module 2)\nPlacebo tests in pre-treatment periods\nTiming variation within treated groups\nLagged regressors (with caution about Nickell bias)",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Panel Data Econometrics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html",
    "href": "chapters/02_identification.html",
    "title": "Identification in Macroeconomics",
    "section": "",
    "text": "The Fundamental Problem\nThis section develops the core challenge of causal inference in macroeconomic settings: moving from observed correlations to credible causal estimates.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#why-correlation-causation",
    "href": "chapters/02_identification.html#why-correlation-causation",
    "title": "Identification in Macroeconomics",
    "section": "Why Correlation ≠ Causation",
    "text": "Why Correlation ≠ Causation\n\nThe Ideal Experiment\nTo measure the causal effect of a policy \\(D\\) on an outcome \\(Y\\), we would ideally observe:\n\\[\n\\tau_i = Y_i(1) - Y_i(0)\n\\]\nwhere \\(Y_i(1)\\) is the outcome if unit \\(i\\) receives treatment, and \\(Y_i(0)\\) is the outcome if it doesn’t. The fundamental problem of causal inference is that we only observe one of these potential outcomes for each unit.\nIn macro settings, this is particularly severe: - We can’t randomly assign monetary policy across countries - We can’t rerun history with different fiscal rules - Countries that adopt different policies differ in many other ways\n\n\nThree Sources of Bias\nWhen we estimate \\(\\hat{\\beta}\\) from a regression of \\(Y\\) on \\(X\\), the estimate can be biased for the true causal effect \\(\\beta\\) due to:\n1. Simultaneity (Reverse Causality)\n\\[\nX \\rightarrow Y \\quad \\text{AND} \\quad Y \\rightarrow X\n\\]\nExample: Do high interest rates cause low inflation, or does low inflation allow central banks to keep rates high? The regression coefficient captures both directions.\n2. Selection (Confounding)\n\\[\nZ \\rightarrow X \\quad \\text{AND} \\quad Z \\rightarrow Y\n\\]\nExample: Countries with strong institutions (\\(Z\\)) both adopt prudent debt management (\\(X\\)) and have stable monetary policy (\\(Y\\)). The correlation between \\(X\\) and \\(Y\\) reflects their common cause \\(Z\\).\n3. Omitted Variables\nA special case of confounding where the confounder \\(Z\\) is unobserved:\n\\[\n\\hat{\\beta}_{OLS} \\xrightarrow{p} \\beta + \\underbrace{\\gamma \\cdot \\frac{\\text{Cov}(X, Z)}{\\text{Var}(X)}}_{\\text{omitted variable bias}}\n\\]\nwhere \\(\\gamma\\) is the effect of \\(Z\\) on \\(Y\\).\n\n\nWhy “Controlling For” Doesn’t Solve It\nA common but flawed intuition: “Just add more controls to the regression.”\nProblem 1: Bad Controls\nAdding post-treatment variables can introduce bias. If \\(M\\) is affected by treatment:\nD → M → Y    (M is a mediator)\nControlling for \\(M\\) blocks part of the causal effect of \\(D\\) on \\(Y\\).\nProblem 2: Unobservables\nYou can only control for what you observe. In macro: - Institutional quality is hard to measure - Political constraints are unobserved - Market expectations are latent\nProblem 3: Measurement Error\nControls measured with error don’t fully absorb confounding:\n\\[\n\\tilde{Z} = Z + \\nu \\quad \\Rightarrow \\quad \\text{bias persists even after controlling for } \\tilde{Z}\n\\]\n\n\n\n\n\n\nWarningThe Credibility Revolution\n\n\n\nModern applied economics requires more than “throwing in controls.” It requires an identification strategy—a credible argument for why the remaining variation in \\(X\\) is as good as randomly assigned conditional on the research design.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#directed-acyclic-graphs-dags",
    "href": "chapters/02_identification.html#directed-acyclic-graphs-dags",
    "title": "Identification in Macroeconomics",
    "section": "Directed Acyclic Graphs (DAGs)",
    "text": "Directed Acyclic Graphs (DAGs)\nDAGs provide a visual language for identification arguments.\nConfounding structure:\n    Z\n   / \\\n  ↓   ↓\n  X → Y\nIn this DAG, \\(Z\\) is a confounder: it causes both \\(X\\) and \\(Y\\). The path \\(X \\leftarrow Z \\rightarrow Y\\) creates a spurious correlation between \\(X\\) and \\(Y\\) even if \\(X\\) has no causal effect on \\(Y\\).\nKey DAG concepts:\n\n\n\n\n\n\n\n\nTerm\nMeaning\nImplication\n\n\n\n\nConfounder\nCommon cause of \\(X\\) and \\(Y\\)\nMust control for it (or use design)\n\n\nMediator\nOn causal path from \\(X\\) to \\(Y\\)\nDon’t control for it\n\n\nCollider\nCaused by both \\(X\\) and \\(Y\\)\nDon’t control for it\n\n\nBackdoor path\nNon-causal path from \\(X\\) to \\(Y\\)\nMust block to identify causal effect",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#fixed-effects-as-partial-solution",
    "href": "chapters/02_identification.html#fixed-effects-as-partial-solution",
    "title": "Identification in Macroeconomics",
    "section": "Fixed Effects as Partial Solution",
    "text": "Fixed Effects as Partial Solution\nPanel fixed effects address one specific source of bias: time-invariant confounders.\n\\[\nY_{it} = \\alpha_i + X_{it}'\\beta + \\varepsilon_{it}\n\\]\nThe country fixed effect \\(\\alpha_i\\) absorbs all factors that: - Differ across countries - Don’t change over time\nThis includes: geography, colonial history, legal origin, language, etc.\nWhat FE doesn’t solve: - Time-varying confounders - Reverse causality - Selection into treatment timing\n\n\n\n\n\n\nNoteThe Identification Hierarchy\n\n\n\nFrom weakest to strongest causal claims:\n\nCross-sectional correlation: Countries with high \\(X\\) have high \\(Y\\)\nPanel FE: When a country’s \\(X\\) rises, its \\(Y\\) rises (within-country)\nDiD: Countries exposed to shock saw \\(Y\\) change relative to unexposed (parallel trends)\nIV: Exogenous variation in \\(X\\) (from instrument) causes \\(Y\\) to change\nRCT: Random assignment of \\(X\\) (rare in macro)",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#the-classic-22-design",
    "href": "chapters/02_identification.html#the-classic-22-design",
    "title": "Identification in Macroeconomics",
    "section": "The Classic 2×2 Design",
    "text": "The Classic 2×2 Design\nConsider a policy implemented in some countries (“treated”) at time \\(T_0\\), with other countries as controls.\nSetup: - \\(D_i = 1\\) if country \\(i\\) is treated, 0 otherwise - \\(Post_t = 1\\) if \\(t &gt; T_0\\), 0 otherwise\nRegression: \\[\nY_{it} = \\alpha + \\beta_1 D_i + \\beta_2 Post_t + \\tau (D_i \\times Post_t) + \\varepsilon_{it}\n\\]\nThe DiD estimator: \\[\n\\hat{\\tau}_{DiD} = \\underbrace{(\\bar{Y}_{T,Post} - \\bar{Y}_{T,Pre})}_{\\text{Treated change}} - \\underbrace{(\\bar{Y}_{C,Post} - \\bar{Y}_{C,Pre})}_{\\text{Control change}}\n\\]\nThis “differences out” both: - Permanent differences between treated and control (first difference) - Common time trends (second difference)\n\n\nCode\n# Simulate DiD data\nN &lt;- 100\nT_periods &lt;- 10\nT0 &lt;- 5  # Treatment at period 5\n\ndid_data &lt;- expand.grid(\n  unit = 1:N,\n  time = 1:T_periods\n) %&gt;%\n  mutate(\n    treated = unit &lt;= N/2,  # First half treated\n    post = time &gt; T0,\n    # Parallel trends in potential outcomes\n    Y0 = 2 + 0.3 * time + ifelse(treated, 1, 0) + rnorm(n(), 0, 0.5),\n    # Treatment effect of 2 units\n    Y1 = Y0 + 2,\n    # Observed outcome\n    Y = ifelse(treated & post, Y1, Y0)\n  )\n\n# Plot\ndid_summary &lt;- did_data %&gt;%\n  group_by(treated, time) %&gt;%\n  summarise(Y = mean(Y), .groups = \"drop\")\n\nggplot(did_summary, aes(x = time, y = Y, color = treated, linetype = treated)) +\n  geom_line(size = 1.2) +\n  geom_vline(xintercept = T0, linetype = \"dashed\", alpha = 0.5) +\n  geom_segment(aes(x = T0, xend = T_periods,\n                   y = 2 + 0.3*T0 + 1 + 0.3*(T_periods - T0),\n                   yend = 2 + 0.3*T_periods + 1),\n               color = \"gray50\", linetype = \"dotted\", size = 0.8) +\n  annotate(\"text\", x = T0 + 0.5, y = 3, label = \"Treatment\", hjust = 0) +\n  annotate(\"text\", x = T_periods - 1, y = 5.8, label = \"Counterfactual\", color = \"gray50\") +\n  scale_color_manual(values = c(\"FALSE\" = \"#3498db\", \"TRUE\" = \"#e74c3c\"),\n                     labels = c(\"Control\", \"Treated\")) +\n  scale_linetype_manual(values = c(\"FALSE\" = \"solid\", \"TRUE\" = \"solid\"),\n                        labels = c(\"Control\", \"Treated\")) +\n  labs(title = \"Difference-in-Differences Illustration\",\n       subtitle = \"Treatment effect = gap between treated and counterfactual\",\n       x = \"Time\", y = \"Outcome\", color = \"Group\", linetype = \"Group\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nDifference-in-Differences: Parallel trends allow causal inference",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#the-parallel-trends-assumption",
    "href": "chapters/02_identification.html#the-parallel-trends-assumption",
    "title": "Identification in Macroeconomics",
    "section": "The Parallel Trends Assumption",
    "text": "The Parallel Trends Assumption\nAssumption: In the absence of treatment, treated and control groups would have followed parallel paths.\n\\[\nE[Y_{it}(0) | D_i = 1, t] - E[Y_{it}(0) | D_i = 0, t] = \\delta \\quad \\forall t\n\\]\nThe difference in untreated potential outcomes is constant over time.\nWhat this requires: - No differential pre-trends - No anticipation effects - No differential shocks coinciding with treatment\nWhat this does NOT require: - Levels to be the same (that’s absorbed by \\(\\beta_1\\)) - Exact parallel paths (allows for noise)",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#pre-trends-testing-and-its-limits",
    "href": "chapters/02_identification.html#pre-trends-testing-and-its-limits",
    "title": "Identification in Macroeconomics",
    "section": "Pre-Trends Testing and Its Limits",
    "text": "Pre-Trends Testing and Its Limits\n\nThe Event Study Specification\nTo assess parallel trends, estimate:\n\\[\nY_{it} = \\alpha_i + \\delta_t + \\sum_{k \\neq -1} \\beta_k \\cdot \\mathbf{1}[t - T_i^* = k] + \\varepsilon_{it}\n\\]\nwhere \\(T_i^*\\) is the treatment time for unit \\(i\\), and \\(k\\) indexes relative time.\n\n\nCode\n# Simulate event study with no pre-trends\nes_data &lt;- did_data %&gt;%\n  mutate(\n    rel_time = time - T0,\n    rel_time_fct = factor(rel_time)\n  )\n\n# Estimate event study\nes_model &lt;- feols(Y ~ i(rel_time_fct, treated, ref = \"-1\") | unit + time,\n                   data = es_data, vcov = ~unit)\n\n# Extract coefficients\ncoef_df &lt;- data.frame(\n  rel_time = as.numeric(gsub(\"rel_time_fct::(-?[0-9]+):treated\", \"\\\\1\",\n                              names(coef(es_model)))),\n  estimate = coef(es_model),\n  se = sqrt(diag(vcov(es_model)))\n) %&gt;%\n  filter(!is.na(rel_time)) %&gt;%\n  mutate(\n    lower = estimate - 1.96 * se,\n    upper = estimate + 1.96 * se\n  )\n\n# Add reference period\ncoef_df &lt;- bind_rows(\n  coef_df,\n  data.frame(rel_time = -1, estimate = 0, se = 0, lower = 0, upper = 0)\n) %&gt;%\n  arrange(rel_time)\n\nggplot(coef_df, aes(x = rel_time, y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", alpha = 0.5) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = \"#3498db\") +\n  geom_line(color = \"#3498db\", size = 1) +\n  geom_point(color = \"#3498db\", size = 2) +\n  annotate(\"text\", x = -3, y = 0.5, label = \"Pre-treatment\\n(should be ≈ 0)\") +\n  annotate(\"text\", x = 3, y = 2.5, label = \"Post-treatment\\n(treatment effect)\") +\n  labs(title = \"Event Study Coefficients\",\n       subtitle = \"Pre-treatment coefficients near zero support parallel trends\",\n       x = \"Periods Relative to Treatment\",\n       y = \"Coefficient (relative to t = -1)\") +\n  scale_x_continuous(breaks = -4:4)\n\n\n\n\n\nEvent study: Testing for pre-trends\n\n\n\n\n\n\nRoth (2022): Pre-Trends Testing Has Low Power\n\n\n\n\n\n\nImportantKey Insight from Roth (2022)\n\n\n\nPassing a pre-trends test does NOT guarantee parallel trends hold. The test has low power against economically meaningful violations.\n\n\nThe problem: - Pre-trends tests are underpowered for typical sample sizes - Researchers often “shop” for specifications that pass the test - Even small pre-trends that pass the test can generate large bias\nQuantifying the issue:\n\n\nCode\n# Simulate: How often does pre-trend test miss a real violation?\nsimulate_pretrend_test &lt;- function(delta_trend, n_sim = 500) {\n  reject &lt;- numeric(n_sim)\n\n  for (s in 1:n_sim) {\n    # Generate data with differential trend\n    data &lt;- expand.grid(unit = 1:50, time = 1:10) %&gt;%\n      mutate(\n        treated = unit &lt;= 25,\n        post = time &gt; 5,\n        # Add differential trend for treated\n        trend_effect = ifelse(treated, delta_trend * time, 0),\n        Y = 2 + 0.3 * time + trend_effect +\n            ifelse(treated, 1, 0) +\n            ifelse(treated & post, 2, 0) +  # True effect = 2\n            rnorm(n(), 0, 1)\n      )\n\n    # Test pre-trends (joint F-test on pre-treatment dummies)\n    pre_data &lt;- filter(data, time &lt;= 5)\n    model &lt;- feols(Y ~ i(time, treated, ref = 5) | unit + time,\n                   data = pre_data, vcov = ~unit)\n\n    # Joint test of pre-treatment coefficients\n    pre_coefs &lt;- coef(model)[grep(\"time::\", names(coef(model)))]\n    if (length(pre_coefs) &gt; 0) {\n      # Simplified: check if any coefficient is significant\n      reject[s] &lt;- any(abs(pre_coefs) / sqrt(diag(vcov(model))[1:length(pre_coefs)]) &gt; 1.96)\n    }\n  }\n\n  mean(reject)  # Power = rejection rate\n}\n\n# Power against different trend violations\ntrend_violations &lt;- c(0, 0.05, 0.1, 0.15, 0.2)\npower &lt;- sapply(trend_violations, simulate_pretrend_test, n_sim = 200)\n\npower_df &lt;- data.frame(\n  differential_trend = trend_violations,\n  power = power\n)\n\nggplot(power_df, aes(x = differential_trend, y = power)) +\n  geom_line(size = 1.2, color = \"#e74c3c\") +\n  geom_point(size = 3, color = \"#e74c3c\") +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 0.15, y = 0.1, label = \"Nominal size (5%)\") +\n  labs(title = \"Power of Pre-Trends Test\",\n       subtitle = \"Even substantial trend violations often go undetected\",\n       x = \"Differential Pre-Trend (per period)\",\n       y = \"Rejection Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 1))\n\n\n\n\n\n\n\n\n\nRecommendations: 1. Report pre-trends but don’t over-interpret passing 2. Use sensitivity analysis (HonestDiD package) 3. Focus on institutional knowledge of why parallel trends should hold",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#natural-experiments-in-macro",
    "href": "chapters/02_identification.html#natural-experiments-in-macro",
    "title": "Identification in Macroeconomics",
    "section": "Natural Experiments in Macro",
    "text": "Natural Experiments in Macro\nThe most credible DiD designs exploit exogenous shocks that affect some units but not others.\nExamples of natural experiments: | Shock | Treatment | Control | Papers | |——-|———–|———|——–| | German reunification | East Germany | West Germany | Fuchs-Schündeln & Schündeln | | Swiss franc appreciation | Swiss border regions | French border regions | Auer et al. | | Oil price shocks | Oil importers vs exporters | — | Kilian & Lewis | | COVID-19 | High-exposure sectors | Low-exposure sectors | — |\nWhat makes a good natural experiment: 1. Exogenous timing: Shock not driven by outcome of interest 2. Clear treatment/control: Units can be cleanly classified 3. No spillovers: Control units unaffected by treatment (SUTVA) 4. Plausible parallel trends: Pre-shock trends similar",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#the-logic-of-instruments",
    "href": "chapters/02_identification.html#the-logic-of-instruments",
    "title": "Identification in Macroeconomics",
    "section": "The Logic of Instruments",
    "text": "The Logic of Instruments\nAn instrument \\(Z\\) satisfies:\n1. Relevance: \\(Z\\) affects \\(X\\) \\[\n\\text{Cov}(Z, X) \\neq 0\n\\]\n2. Exclusion: \\(Z\\) affects \\(Y\\) only through \\(X\\) \\[\n\\text{Cov}(Z, \\varepsilon) = 0\n\\]\nGraphically:\nZ → X → Y\nZ ↛ Y (directly)\nThe IV estimator: \\[\n\\hat{\\beta}_{IV} = \\frac{\\text{Cov}(Z, Y)}{\\text{Cov}(Z, X)} = \\frac{\\text{Reduced form}}{\\text{First stage}}\n\\]",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#two-stage-least-squares",
    "href": "chapters/02_identification.html#two-stage-least-squares",
    "title": "Identification in Macroeconomics",
    "section": "Two-Stage Least Squares",
    "text": "Two-Stage Least Squares\nFirst stage: Regress \\(X\\) on \\(Z\\) and controls \\[\nX_{it} = \\pi_0 + \\pi_1 Z_{it} + W_{it}'\\gamma + \\nu_{it}\n\\]\nSecond stage: Regress \\(Y\\) on predicted \\(\\hat{X}\\) and controls \\[\nY_{it} = \\beta_0 + \\beta_1 \\hat{X}_{it} + W_{it}'\\delta + \\varepsilon_{it}\n\\]\n\n\nCode\n# Simulate IV setting\nN &lt;- 500\ntrue_beta &lt;- 0.5\n\n# Instrument Z\nZ &lt;- rnorm(N)\n\n# Endogenous X (correlated with error)\nU &lt;- rnorm(N)  # Unobserved confounder\nX &lt;- 0.7 * Z + 0.8 * U + rnorm(N, 0, 0.3)\n\n# Outcome\nY &lt;- true_beta * X + 0.6 * U + rnorm(N, 0, 0.5)\n\niv_data &lt;- data.frame(Y = Y, X = X, Z = Z)\n\n# OLS (biased)\nols_fit &lt;- lm(Y ~ X, data = iv_data)\n\n# IV\niv_fit &lt;- feols(Y ~ 1 | X ~ Z, data = iv_data)\n\ncat(\"True effect:\", true_beta, \"\\n\")\n\n\nTrue effect: 0.5 \n\n\nCode\ncat(\"OLS estimate:\", round(coef(ols_fit)[\"X\"], 3), \"(biased upward due to U)\\n\")\n\n\nOLS estimate: 0.895 (biased upward due to U)\n\n\nCode\ncat(\"IV estimate:\", round(coef(iv_fit)[\"fit_X\"], 3), \"(consistent)\\n\")\n\n\nIV estimate: 0.591 (consistent)\n\n\nCode\ncat(\"First-stage F:\", round(fitstat(iv_fit, \"ivf\")$ivf1$stat, 1), \"\\n\")\n\n\nFirst-stage F: 448.3",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#weak-instruments",
    "href": "chapters/02_identification.html#weak-instruments",
    "title": "Identification in Macroeconomics",
    "section": "Weak Instruments",
    "text": "Weak Instruments\nIf the first stage is weak (\\(\\pi_1 \\approx 0\\)), IV estimates are: - Biased toward OLS - Have inflated standard errors - Unreliable inference\nRule of thumb: First-stage F-statistic &gt; 10 (Stock & Yogo)\nDiagnosis:\n\n\nCode\n# Demonstrate weak IV bias\nsimulate_iv &lt;- function(first_stage_strength, n_sim = 1000) {\n  estimates &lt;- numeric(n_sim)\n\n  for (s in 1:n_sim) {\n    Z &lt;- rnorm(200)\n    U &lt;- rnorm(200)\n    X &lt;- first_stage_strength * Z + 0.8 * U + rnorm(200, 0, 0.5)\n    Y &lt;- 0.5 * X + 0.6 * U + rnorm(200, 0, 0.5)\n\n    iv_fit &lt;- feols(Y ~ 1 | X ~ Z, data = data.frame(Y, X, Z))\n    estimates[s] &lt;- coef(iv_fit)[\"fit_X\"]\n  }\n\n  data.frame(\n    first_stage = first_stage_strength,\n    mean_estimate = mean(estimates),\n    sd_estimate = sd(estimates)\n  )\n}\n\nstrengths &lt;- c(0.1, 0.3, 0.5, 0.7, 1.0)\nweak_iv_results &lt;- do.call(rbind, lapply(strengths, simulate_iv, n_sim = 500))\n\nggplot(weak_iv_results, aes(x = first_stage, y = mean_estimate)) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"darkgreen\") +\n  geom_line(size = 1.2, color = \"#9b59b6\") +\n  geom_point(size = 3, color = \"#9b59b6\") +\n  geom_errorbar(aes(ymin = mean_estimate - 1.96*sd_estimate,\n                    ymax = mean_estimate + 1.96*sd_estimate),\n                width = 0.05, color = \"#9b59b6\") +\n  annotate(\"text\", x = 0.8, y = 0.55, label = \"True β = 0.5\", color = \"darkgreen\") +\n  labs(title = \"Weak IV Bias\",\n       subtitle = \"Weak instruments bias IV toward OLS\",\n       x = \"First-Stage Coefficient (instrument strength)\",\n       y = \"Mean IV Estimate (across simulations)\")",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#shift-share-bartik-instruments",
    "href": "chapters/02_identification.html#shift-share-bartik-instruments",
    "title": "Identification in Macroeconomics",
    "section": "Shift-Share (Bartik) Instruments",
    "text": "Shift-Share (Bartik) Instruments\nA common IV strategy in macro uses shift-share designs:\n\\[\nZ_{it} = \\sum_k s_{ik,0} \\cdot g_{kt}\n\\]\nwhere: - \\(s_{ik,0}\\) = pre-period share of sector \\(k\\) in unit \\(i\\) - \\(g_{kt}\\) = national/global shock to sector \\(k\\) at time \\(t\\)\nExample: China shock (Autor, Dorn, Hanson 2013) - \\(s_{ik}\\) = share of industry \\(k\\) in region \\(i\\)’s employment - \\(g_{kt}\\) = growth of Chinese imports in industry \\(k\\)\nIdentification can come from: - Exogeneity of shares (Goldsmith-Pinkham, Sorkin & Swift 2020) - Exogeneity of shocks (Borusyak, Hull & Jaravel 2022)",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#gmm-for-dynamic-panels",
    "href": "chapters/02_identification.html#gmm-for-dynamic-panels",
    "title": "Identification in Macroeconomics",
    "section": "GMM for Dynamic Panels",
    "text": "GMM for Dynamic Panels\nWhen the model includes lagged dependent variables:\n\\[\nY_{it} = \\rho Y_{i,t-1} + X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nStandard FE is biased (Nickell bias): \\(\\text{plim}(\\hat{\\rho}_{FE}) = \\rho - \\frac{1+\\rho}{T-1}\\)\nSolutions:\n\n\n\n\n\n\n\n\nMethod\nInstruments\nWhen to use\n\n\n\n\nAnderson-Hsiao\n\\(Y_{i,t-2}\\)\nSimple, conservative\n\n\nArellano-Bond (diff-GMM)\n\\(Y_{i,t-2}, Y_{i,t-3}, ...\\)\nModerately persistent \\(Y\\)\n\n\nBlundell-Bond (sys-GMM)\nAdd levels equations\nHighly persistent \\(Y\\)\n\n\n\n\n\n\n\n\n\nTipWhen Nickell Bias is Small\n\n\n\nWith \\(T = 84\\) quarters and \\(\\rho = 0.9\\), the bias is approximately \\(\\frac{1.9}{83} \\approx 2.3\\%\\). For long panels, FE may be acceptable.",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#the-small-cluster-problem",
    "href": "chapters/02_identification.html#the-small-cluster-problem",
    "title": "Identification in Macroeconomics",
    "section": "The Small-Cluster Problem",
    "text": "The Small-Cluster Problem\nCluster-robust standard errors assume \\(G \\to \\infty\\) (number of clusters goes to infinity).\nWith \\(G &lt; 50\\) clusters: - Standard errors are biased downward - t-statistics are inflated - Rejection rates exceed nominal size\n\n\nCode\n# Simulate rejection rates with different cluster sizes\nsimulate_rejection &lt;- function(n_clusters, n_sim = 1000) {\n  rejections &lt;- 0\n\n  for (s in 1:n_sim) {\n    # Generate clustered data with NO true effect\n    cluster_effects &lt;- rnorm(n_clusters, 0, 2)\n\n    data &lt;- data.frame(\n      cluster = rep(1:n_clusters, each = 10),\n      cluster_effect = rep(cluster_effects, each = 10)\n    ) %&gt;%\n      mutate(\n        X = rnorm(n()) + cluster_effect * 0.3,\n        Y = cluster_effect + rnorm(n())  # No effect of X!\n      )\n\n    # Test with clustered SEs\n    fit &lt;- feols(Y ~ X | cluster, data = data, vcov = ~cluster)\n    p_value &lt;- 2 * (1 - pt(abs(coef(fit)[\"X\"] / se(fit)[\"X\"]), df = n_clusters - 1))\n    rejections &lt;- rejections + (p_value &lt; 0.05)\n  }\n\n  rejections / n_sim\n}\n\ncluster_sizes &lt;- c(10, 20, 30, 50, 100)\nrejection_rates &lt;- sapply(cluster_sizes, simulate_rejection, n_sim = 500)\n\nrej_df &lt;- data.frame(clusters = cluster_sizes, rejection = rejection_rates)\n\nggplot(rej_df, aes(x = clusters, y = rejection)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"darkgreen\") +\n  geom_line(size = 1.2, color = \"#e74c3c\") +\n  geom_point(size = 3, color = \"#e74c3c\") +\n  annotate(\"text\", x = 70, y = 0.06, label = \"Nominal 5% size\", color = \"darkgreen\") +\n  labs(title = \"Over-Rejection with Few Clusters\",\n       subtitle = \"Rejection rate under the null (should be 5%)\",\n       x = \"Number of Clusters\",\n       y = \"Rejection Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15))\n\n\n\n\n\nSmall clusters lead to over-rejection",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#wild-cluster-bootstrap",
    "href": "chapters/02_identification.html#wild-cluster-bootstrap",
    "title": "Identification in Macroeconomics",
    "section": "Wild Cluster Bootstrap",
    "text": "Wild Cluster Bootstrap\nThe wild cluster bootstrap (Cameron, Gelbach & Miller 2008) provides valid inference with few clusters.\nAlgorithm: 1. Estimate the model, get residuals \\(\\hat{\\varepsilon}_{it}\\) 2. For \\(b = 1, ..., B\\): - Draw cluster-level weights \\(w_g \\in \\{-1, +1\\}\\) (Rademacher) - Construct bootstrap outcome: \\(Y^*_{it} = \\hat{Y}_{it} + w_{g(i)} \\cdot \\hat{\\varepsilon}_{it}\\) - Re-estimate model, save t-statistic \\(t^*_b\\) 3. Compare actual t-statistic to bootstrap distribution\n\n\nCode\n# Demonstrate wild cluster bootstrap (manual implementation)\n# In practice, use fwildclusterboot::boottest()\n\n# Generate data with few clusters\nn_clusters &lt;- 20\ncluster_effects &lt;- rnorm(n_clusters, 0, 2)\n\nwcb_data &lt;- data.frame(\n  cluster = rep(1:n_clusters, each = 15),\n  cluster_effect = rep(cluster_effects, each = 15)\n) %&gt;%\n  mutate(\n    X = rnorm(n()) + cluster_effect * 0.3,\n    Y = 0.5 * X + cluster_effect + rnorm(n(), 0, 0.8)  # True effect = 0.5\n  )\n\n# Standard estimation\nmodel &lt;- feols(Y ~ X | cluster, data = wcb_data, vcov = ~cluster)\n\ncat(\"=== Standard Cluster-Robust Inference ===\\n\")\n\n\n=== Standard Cluster-Robust Inference ===\n\n\nCode\ncat(\"Estimate:\", round(coef(model)[\"X\"], 3), \"\\n\")\n\n\nEstimate: 0.578 \n\n\nCode\ncat(\"Cluster SE:\", round(se(model)[\"X\"], 3), \"\\n\")\n\n\nCluster SE: 0.046 \n\n\nCode\ncat(\"t-stat:\", round(coef(model)[\"X\"] / se(model)[\"X\"], 2), \"\\n\")\n\n\nt-stat: 12.47 \n\n\nCode\ncat(\"p-value (t-dist, df =\", n_clusters - 1, \"):\",\n    round(2 * pt(-abs(coef(model)[\"X\"] / se(model)[\"X\"]), df = n_clusters - 1), 4), \"\\n\\n\")\n\n\np-value (t-dist, df = 19 ): 0 \n\n\nCode\n# Manual wild cluster bootstrap implementation\nwild_cluster_bootstrap &lt;- function(model, data, cluster_var, B = 999) {\n  # Get model components\n  y_fitted &lt;- fitted(model)\n  resid &lt;- residuals(model)\n  clusters &lt;- unique(data[[cluster_var]])\n  G &lt;- length(clusters)\n\n  # Original t-statistic\n  t_orig &lt;- coef(model)[\"X\"] / se(model)[\"X\"]\n\n  # Bootstrap\n  t_boot &lt;- numeric(B)\n  for (b in 1:B) {\n    # Rademacher weights (cluster-level)\n    weights &lt;- sample(c(-1, 1), G, replace = TRUE)\n    names(weights) &lt;- clusters\n\n    # Construct bootstrap outcome\n    data$Y_boot &lt;- y_fitted + weights[as.character(data[[cluster_var]])] * resid\n\n    # Re-estimate under null (impose beta = 0)\n    model_boot &lt;- feols(Y_boot ~ X | cluster, data = data, vcov = ~cluster)\n    t_boot[b] &lt;- coef(model_boot)[\"X\"] / se(model_boot)[\"X\"]\n  }\n\n  # Two-sided p-value\n  p_value &lt;- mean(abs(t_boot) &gt;= abs(t_orig))\n\n  list(t_orig = t_orig, t_boot = t_boot, p_value = p_value)\n}\n\n# Run bootstrap (reduced B for speed)\nboot_result &lt;- wild_cluster_bootstrap(model, wcb_data, \"cluster\", B = 499)\n\ncat(\"=== Wild Cluster Bootstrap (Manual) ===\\n\")\n\n\n=== Wild Cluster Bootstrap (Manual) ===\n\n\nCode\ncat(\"Bootstrap p-value:\", round(boot_result$p_value, 4), \"\\n\")\n\n\nBootstrap p-value: 0.5291 \n\n\nCode\ncat(\"(In practice, use fwildclusterboot::boottest() for efficiency)\\n\")\n\n\n(In practice, use fwildclusterboot::boottest() for efficiency)",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#when-to-use-what",
    "href": "chapters/02_identification.html#when-to-use-what",
    "title": "Identification in Macroeconomics",
    "section": "When to Use What",
    "text": "When to Use What\n\n\n\n\n\n\n\n\nClusters\nMethod\nR Implementation\n\n\n\n\nG &gt; 50\nStandard cluster-robust SE\nvcov = ~cluster\n\n\n30 &lt; G &lt; 50\nt-distribution with G-1 df\nvcov = ~cluster + manual df adjustment\n\n\n10 &lt; G &lt; 30\nWild cluster bootstrap\nfwildclusterboot::boottest()\n\n\nG &lt; 10\nRandomization inference\nPermutation tests\n\n\n\n\n\n\n\n\n\nTipWebb Weights for Very Few Clusters\n\n\n\nWith G &lt; 10, use Webb (6-point) weights instead of Rademacher:\nboottest(model, param = \"X\", clustid = \"cluster\", type = \"webb\")",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#checklist-for-identification",
    "href": "chapters/02_identification.html#checklist-for-identification",
    "title": "Identification in Macroeconomics",
    "section": "Checklist for Identification",
    "text": "Checklist for Identification\nBefore claiming a causal effect, verify:\n\nWhat is the source of identifying variation?\n\nWithin-unit over time? (FE)\nAcross units hit by shock? (DiD)\nExogenous instrument? (IV)\n\nWhat assumptions does this require?\n\nParallel trends? (DiD)\nExclusion restriction? (IV)\nNo anticipation? (Event study)\n\nWhat are the threats?\n\nTime-varying confounders?\nSpillovers to control group?\nWeak instruments?\n\nWhat robustness checks support the design?\n\nPre-trends (with caveats about power)\nPlacebo tests\nAlternative control groups\nSensitivity analysis",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#common-pitfalls",
    "href": "chapters/02_identification.html#common-pitfalls",
    "title": "Identification in Macroeconomics",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\n\n\n\n\n\n\n\nPitfall\nProblem\nSolution\n\n\n\n\nOverinterpreting pre-trends\nLow power against violations\nUse HonestDiD, report sensitivity\n\n\nPost-treatment controls\nBlocks causal path\nOnly control for pre-determined variables\n\n\nWeak instruments\nBias toward OLS\nReport first-stage F, use robust methods\n\n\nFew clusters\nOver-rejection\nWild cluster bootstrap\n\n\nStaggered treatment\nTWFE bias\nCallaway-Sant’Anna, Sun-Abraham\n\n\nAnticipation effects\nViolates sharp treatment timing\nTest for pre-treatment effects",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#how-do-i-know-if-parallel-trends-holds",
    "href": "chapters/02_identification.html#how-do-i-know-if-parallel-trends-holds",
    "title": "Identification in Macroeconomics",
    "section": "How do I know if parallel trends holds?",
    "text": "How do I know if parallel trends holds?\nYou can’t prove it—it’s about counterfactuals we don’t observe. You can: - Show pre-trends are flat (but low power—Roth 2022) - Argue institutionally why the assumption is plausible - Run sensitivity analysis (HonestDiD) - Find a better control group",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#when-should-i-use-iv-vs.-did",
    "href": "chapters/02_identification.html#when-should-i-use-iv-vs.-did",
    "title": "Identification in Macroeconomics",
    "section": "When should I use IV vs. DiD?",
    "text": "When should I use IV vs. DiD?\nUse DiD when: - You have a clear treatment/control distinction - Treatment timing is plausibly exogenous - Parallel trends is defensible\nUse IV when: - Treatment varies continuously (not just on/off) - You have a plausibly exogenous source of variation in treatment - Exclusion restriction is defensible",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#what-if-my-first-stage-f-is-below-10",
    "href": "chapters/02_identification.html#what-if-my-first-stage-f-is-below-10",
    "title": "Identification in Macroeconomics",
    "section": "What if my first-stage F is below 10?",
    "text": "What if my first-stage F is below 10?\nOptions: 1. Find a stronger instrument 2. Use weak-IV robust methods (Anderson-Rubin, LIML) 3. Report reduced form instead (effect of Z on Y) 4. Interpret results as a lower bound (if bias direction is known)",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/02_identification.html#how-many-clusters-do-i-need",
    "href": "chapters/02_identification.html#how-many-clusters-do-i-need",
    "title": "Identification in Macroeconomics",
    "section": "How many clusters do I need?",
    "text": "How many clusters do I need?\n\n50+ clusters: Standard cluster-robust SEs are fine\n30-50 clusters: Use t-distribution with G-1 df\n10-30 clusters: Wild cluster bootstrap\n&lt;10 clusters: Randomization inference, or aggregate to higher level",
    "crumbs": [
      "Phase 1: Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification in Macroeconomics</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html",
    "href": "chapters/03_local_projections.html",
    "title": "Local Projections",
    "section": "",
    "text": "The Local Projections Idea\nLocal Projections (LP), introduced by Jordà (2005), provide a flexible alternative to Vector Autoregressions for estimating impulse response functions. The key insight: instead of specifying and iterating a full dynamic system, directly regress the outcome \\(h\\) periods ahead on today’s shock.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#from-var-to-lp",
    "href": "chapters/03_local_projections.html#from-var-to-lp",
    "title": "Local Projections",
    "section": "From VAR to LP",
    "text": "From VAR to LP\n\nThe VAR Approach\nA VAR(p) models the joint dynamics of a vector \\(\\mathbf{y}_t\\):\n\\[\n\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1 \\mathbf{y}_{t-1} + \\cdots + \\mathbf{A}_p \\mathbf{y}_{t-p} + \\mathbf{u}_t\n\\]\nTo get impulse responses, we: 1. Identify structural shocks (Cholesky, sign restrictions, etc.) 2. Iterate the system forward to compute \\(\\frac{\\partial \\mathbf{y}_{t+h}}{\\partial \\varepsilon_t}\\)\nThis requires correct specification of the entire system.\n\n\nThe LP Approach\nLP estimates the response at each horizon \\(h\\) directly:\n\\[\ny_{t+h} = \\alpha^h + \\beta^h \\cdot \\text{shock}_t + \\mathbf{X}_t' \\boldsymbol{\\gamma}^h + \\varepsilon_{t+h}\n\\]\nwhere: - \\(\\beta^h\\) is the impulse response at horizon \\(h\\) - \\(\\mathbf{X}_t\\) contains controls (lags of \\(y\\), other variables) - Each horizon is a separate regression\n\n\n\n\n\n\nTipKey Insight\n\n\n\nLP doesn’t require specifying the full dynamic system. We only need to identify the shock of interest—the rest of the economy can be a “black box.”",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#when-lp-and-var-give-the-same-answer",
    "href": "chapters/03_local_projections.html#when-lp-and-var-give-the-same-answer",
    "title": "Local Projections",
    "section": "When LP and VAR Give the Same Answer",
    "text": "When LP and VAR Give the Same Answer\nTheorem (Plagborg-Møller & Wolf, 2021): Under correct specification, LP and VAR estimate the same population impulse responses.\nBoth methods are consistent for: \\[\n\\text{IRF}(h) = \\frac{\\partial \\mathbb{E}[y_{t+h} | \\mathcal{I}_t]}{\\partial \\text{shock}_t}\n\\]\nThe difference is in finite samples and misspecification:\n\n\n\n\n\n\n\n\nAspect\nVAR\nLP\n\n\n\n\nSpecification\nImposes structure\nAgnostic at each horizon\n\n\nEfficiency\nMore efficient if correct\nLess efficient (more parameters)\n\n\nMisspecification\nBiased if wrong\nRobust\n\n\nConfidence intervals\nTighter\nWider (but honest)",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#when-they-diverge",
    "href": "chapters/03_local_projections.html#when-they-diverge",
    "title": "Local Projections",
    "section": "When They Diverge",
    "text": "When They Diverge\nLP and VAR can give different answers when:\n\nVAR is misspecified: Wrong lag length, omitted variables, nonlinearities\nSmall samples: LP’s horizon-by-horizon estimation is noisier\nLong horizons: VAR extrapolates; LP estimates directly (sample shrinks)\n\n\n\nCode\n# Simulate a simple AR(1) process\nT &lt;- 200\nrho &lt;- 0.8\ny &lt;- numeric(T)\nshock &lt;- c(1, rep(0, T-1))  # Unit impulse at t=1\n\nfor (t in 2:T) {\n  y[t] &lt;- rho * y[t-1] + shock[t]\n}\n\n# True IRF: rho^h\nH &lt;- 12\ntrue_irf &lt;- rho^(0:H)\n\n# LP estimation\nlp_irf &lt;- numeric(H + 1)\nlp_se &lt;- numeric(H + 1)\n\nfor (h in 0:H) {\n  # Create lead of y\n  y_lead &lt;- c(y[(h+1):T], rep(NA, h))\n  y_lag &lt;- c(NA, y[1:(T-1)])\n\n  df &lt;- data.frame(y_lead = y_lead, shock = shock, y_lag = y_lag) %&gt;%\n    filter(!is.na(y_lead) & !is.na(y_lag))\n\n  fit &lt;- lm(y_lead ~ shock + y_lag, data = df)\n  lp_irf[h + 1] &lt;- coef(fit)[\"shock\"]\n  lp_se[h + 1] &lt;- sqrt(diag(vcov(fit)))[\"shock\"]\n}\n\n# VAR estimation (just AR(1) here)\nvar_fit &lt;- lm(y[2:T] ~ y[1:(T-1)])\nrho_hat &lt;- coef(var_fit)[2]\nvar_irf &lt;- rho_hat^(0:H)\n\n# Plot comparison\nirf_compare &lt;- data.frame(\n  horizon = rep(0:H, 3),\n  irf = c(true_irf, lp_irf, var_irf),\n  method = rep(c(\"True\", \"LP\", \"VAR\"), each = H + 1)\n)\n\nggplot(irf_compare, aes(x = horizon, y = irf, color = method, linetype = method)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  scale_color_manual(values = c(\"True\" = \"black\", \"LP\" = \"#e74c3c\", \"VAR\" = \"#3498db\")) +\n  scale_linetype_manual(values = c(\"True\" = \"solid\", \"LP\" = \"dashed\", \"VAR\" = \"dotted\")) +\n  labs(title = \"LP vs VAR: Impulse Response Comparison\",\n       subtitle = \"Both recover the true IRF from an AR(1) DGP\",\n       x = \"Horizon\", y = \"Response to Unit Shock\",\n       color = \"Method\", linetype = \"Method\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nLP vs VAR: Both recover true IRF from correctly specified DGP",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#the-specification",
    "href": "chapters/03_local_projections.html#the-specification",
    "title": "Local Projections",
    "section": "The Specification",
    "text": "The Specification\nFor a panel of countries observed over time:\n\\[\ny_{i,t+h} - y_{i,t-1} = \\alpha_i^h + \\delta_t^h + \\beta^h \\cdot \\text{shock}_{it} + \\mathbf{X}_{it}'\\boldsymbol{\\gamma}^h + \\varepsilon_{i,t+h}\n\\]\nLeft-hand side: Cumulative change from \\(t-1\\) to \\(t+h\\). This gives the cumulative multiplier—how much \\(y\\) has changed in total by horizon \\(h\\).\nRight-hand side: - \\(\\alpha_i^h\\): Country fixed effects (absorb level differences) - \\(\\delta_t^h\\): Time fixed effects (absorb common shocks) - \\(\\text{shock}_{it}\\): The impulse of interest - \\(\\mathbf{X}_{it}\\): Controls (lags of \\(y\\), other variables)",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#implementation-with-fixest",
    "href": "chapters/03_local_projections.html#implementation-with-fixest",
    "title": "Local Projections",
    "section": "Implementation with fixest",
    "text": "Implementation with fixest\n\n\nCode\n# Generate synthetic panel data\nN &lt;- 30  # countries\nT_obs &lt;- 60  # quarters\n\n# Country fixed effects\ncountry_fe &lt;- rnorm(N, 0, 2)\n\n# Generate panel\npanel_data &lt;- expand.grid(\n  country = 1:N,\n  time = 1:T_obs\n) %&gt;%\n  arrange(country, time) %&gt;%\n  mutate(\n    # Country effect\n    alpha = country_fe[country],\n    # Time trend + cycle\n    time_effect = 0.02 * time + sin(time / 4) * 0.5,\n    # Shock (e.g., inflation surprise)\n    shock = rnorm(n(), 0, 1),\n    # Autoregressive outcome with shock response\n    y = NA_real_\n  )\n\n# Generate AR process with shock impact\nfor (i in 1:N) {\n  idx &lt;- which(panel_data$country == i)\n  y_country &lt;- numeric(T_obs)\n\n  for (t in 2:T_obs) {\n    # True DGP: y responds to shock with persistence\n    y_country[t] &lt;- 0.7 * y_country[t-1] +\n                    0.5 * panel_data$shock[idx[t]] +  # Immediate impact\n                    0.3 * panel_data$shock[idx[t-1]] + # Delayed impact\n                    country_fe[i] * 0.1 +\n                    rnorm(1, 0, 0.5)\n  }\n  panel_data$y[idx] &lt;- y_country\n}\n\n# Convert to panel\npdata &lt;- panel(panel_data, ~country + time)\n\ncat(\"Panel dimensions:\", N, \"countries,\", T_obs, \"periods\\n\")\n\n\nPanel dimensions: 30 countries, 60 periods\n\n\n\n\nCode\n# Estimate LP at each horizon\nH &lt;- 12\nresults &lt;- data.frame(\n  horizon = 0:H,\n  estimate = NA_real_,\n  se = NA_real_,\n  ci_low = NA_real_,\n  ci_high = NA_real_\n)\n\nfor (h in 0:H) {\n  # LP regression: y_{t+h} - y_{t-1} on shock_t\n  model &lt;- feols(\n    f(y, h) - l(y, 1) ~ shock + l(y, 1:2) | country + time,\n    data = pdata,\n    vcov = ~country  # Cluster by country\n  )\n\n  results$estimate[h + 1] &lt;- coef(model)[\"shock\"]\n  results$se[h + 1] &lt;- se(model)[\"shock\"]\n  results$ci_low[h + 1] &lt;- results$estimate[h + 1] - 1.96 * results$se[h + 1]\n  results$ci_high[h + 1] &lt;- results$estimate[h + 1] + 1.96 * results$se[h + 1]\n}\n\n# Plot IRF\nggplot(results, aes(x = horizon, y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_ribbon(aes(ymin = ci_low, ymax = ci_high), alpha = 0.2, fill = \"#3498db\") +\n  geom_line(color = \"#3498db\", size = 1.2) +\n  geom_point(color = \"#3498db\", size = 2) +\n  labs(title = \"Local Projection: Response to Unit Shock\",\n       subtitle = \"Cumulative response with 95% confidence bands (clustered by country)\",\n       x = \"Horizon (periods)\",\n       y = \"Cumulative Response\") +\n  scale_x_continuous(breaks = 0:H)\n\n\n\n\n\nLocal Projection impulse responses with 95% confidence bands",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#inference-why-hacclustering-matters",
    "href": "chapters/03_local_projections.html#inference-why-hacclustering-matters",
    "title": "Local Projections",
    "section": "Inference: Why HAC/Clustering Matters",
    "text": "Inference: Why HAC/Clustering Matters\nAt horizon \\(h\\), the LP residual \\(\\varepsilon_{i,t+h}\\) is correlated with \\(\\varepsilon_{i,t+h-1}, \\ldots, \\varepsilon_{i,t+1}\\) by construction (overlapping observations).\nThis induces MA(h-1) serial correlation in errors.\nStandard OLS standard errors are invalid. Must use:\n\n\n\n\n\n\n\n\nMethod\nWhen\nImplementation\n\n\n\n\nNewey-West HAC\nTime series\nsandwich::NeweyWest(lags = h)\n\n\nCluster by unit\nPanel, short T\nvcov = ~country\n\n\nDriscoll-Kraay\nPanel, cross-sectional dependence\nvcov = \"DK\" in some packages\n\n\n\n\n\nCode\n# Compare SE across horizons\nse_by_horizon &lt;- data.frame(\n  horizon = 0:H,\n  cluster_se = results$se,\n  # For comparison, also compute OLS SE (wrong!)\n  ols_se = NA_real_\n)\n\nfor (h in 0:H) {\n  model_ols &lt;- feols(\n    f(y, h) - l(y, 1) ~ shock + l(y, 1:2) | country + time,\n    data = pdata,\n    vcov = \"iid\"  # Wrong but instructive\n  )\n  se_by_horizon$ols_se[h + 1] &lt;- se(model_ols)[\"shock\"]\n}\n\nse_long &lt;- se_by_horizon %&gt;%\n  pivot_longer(cols = c(cluster_se, ols_se),\n               names_to = \"method\", values_to = \"se\")\n\nggplot(se_long, aes(x = horizon, y = se, color = method)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  scale_color_manual(values = c(\"cluster_se\" = \"#2ecc71\", \"ols_se\" = \"#e74c3c\"),\n                     labels = c(\"Clustered (correct)\", \"IID (wrong)\")) +\n  labs(title = \"Standard Errors by Horizon\",\n       subtitle = \"Clustered SEs account for serial correlation; IID SEs are too small\",\n       x = \"Horizon\", y = \"Standard Error\",\n       color = \"Method\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nStandard errors grow with horizon due to overlapping residuals",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#binary-state-switching",
    "href": "chapters/03_local_projections.html#binary-state-switching",
    "title": "Local Projections",
    "section": "Binary State Switching",
    "text": "Binary State Switching\nThe simplest approach: interact the shock with a state indicator.\n\\[\ny_{i,t+h} - y_{i,t-1} = \\alpha_i^h + \\beta_0^h \\cdot (1 - S_{it}) \\cdot \\text{shock}_{it} + \\beta_1^h \\cdot S_{it} \\cdot \\text{shock}_{it} + \\cdots\n\\]\nwhere \\(S_{it} = 1\\) if country \\(i\\) is in “state 1” at time \\(t\\).\nExamples of state variables: - Recession vs. expansion - High vs. low debt - Financial crisis vs. normal times - High vs. low bank holdings of sovereign debt\n\n\nCode\n# Add state variable: high vs low initial shock exposure\npanel_data &lt;- panel_data %&gt;%\n  group_by(country) %&gt;%\n  mutate(\n    # State based on lagged cumulative shock exposure\n    cum_shock = cumsum(shock),\n    high_state = as.integer(lag(cum_shock, 1) &gt; median(lag(cum_shock, 1), na.rm = TRUE))\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(high_state = replace_na(high_state, 0))\n\npdata &lt;- panel(panel_data, ~country + time)\n\n# Estimate state-dependent LP\nresults_state &lt;- data.frame(\n  horizon = rep(0:H, 2),\n  state = rep(c(\"Low\", \"High\"), each = H + 1),\n  estimate = NA_real_,\n  se = NA_real_\n)\n\nfor (h in 0:H) {\n  model_state &lt;- feols(\n    f(y, h) - l(y, 1) ~ i(high_state, shock) + l(y, 1:2) | country + time,\n    data = pdata,\n    vcov = ~country\n  )\n\n  # Extract coefficients for each state\n  coef_names &lt;- names(coef(model_state))\n\n  # Low state (high_state = 0)\n  low_idx &lt;- grep(\"high_state::0:shock\", coef_names)\n  if (length(low_idx) &gt; 0) {\n    results_state$estimate[h + 1] &lt;- coef(model_state)[low_idx]\n    results_state$se[h + 1] &lt;- se(model_state)[low_idx]\n  }\n\n  # High state (high_state = 1)\n  high_idx &lt;- grep(\"high_state::1:shock\", coef_names)\n  if (length(high_idx) &gt; 0) {\n    results_state$estimate[H + 2 + h] &lt;- coef(model_state)[high_idx]\n    results_state$se[H + 2 + h] &lt;- se(model_state)[high_idx]\n  }\n}\n\nresults_state &lt;- results_state %&gt;%\n  mutate(\n    ci_low = estimate - 1.96 * se,\n    ci_high = estimate + 1.96 * se\n  )\n\nggplot(results_state, aes(x = horizon, y = estimate, color = state, fill = state)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_ribbon(aes(ymin = ci_low, ymax = ci_high), alpha = 0.2, color = NA) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  scale_color_manual(values = c(\"Low\" = \"#3498db\", \"High\" = \"#e74c3c\")) +\n  scale_fill_manual(values = c(\"Low\" = \"#3498db\", \"High\" = \"#e74c3c\")) +\n  labs(title = \"State-Dependent Impulse Responses\",\n       subtitle = \"Response to shock differs by regime\",\n       x = \"Horizon\", y = \"Cumulative Response\",\n       color = \"State\", fill = \"State\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nState-dependent LP: Responses differ by regime",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#smooth-transition-auerbach-gorodnichenko",
    "href": "chapters/03_local_projections.html#smooth-transition-auerbach-gorodnichenko",
    "title": "Local Projections",
    "section": "Smooth Transition (Auerbach-Gorodnichenko)",
    "text": "Smooth Transition (Auerbach-Gorodnichenko)\nInstead of hard switching, use a logistic function for smooth transition:\n\\[\nF(z_t) = \\frac{\\exp(-\\gamma z_t)}{1 + \\exp(-\\gamma z_t)}\n\\]\nwhere: - \\(z_t\\) is the (standardized) state variable - \\(\\gamma &gt; 0\\) controls transition speed - \\(F(z_t) \\to 1\\) when \\(z_t \\to -\\infty\\) (e.g., deep recession) - \\(F(z_t) \\to 0\\) when \\(z_t \\to +\\infty\\) (e.g., strong expansion)\nThe LP specification becomes:\n\\[\ny_{t+h} = F(z_{t-1}) \\cdot [\\alpha_R^h + \\beta_R^h \\cdot \\text{shock}_t] + (1 - F(z_{t-1})) \\cdot [\\alpha_E^h + \\beta_E^h \\cdot \\text{shock}_t] + \\cdots\n\\]\n\n\nCode\n# Demonstrate the logistic transition function\nz &lt;- seq(-3, 3, length.out = 100)\ngamma_values &lt;- c(1, 2, 5)\n\ntransition_df &lt;- expand.grid(z = z, gamma = gamma_values) %&gt;%\n  mutate(\n    F_z = exp(-gamma * z) / (1 + exp(-gamma * z)),\n    gamma_label = paste(\"γ =\", gamma)\n  )\n\nggplot(transition_df, aes(x = z, y = F_z, color = gamma_label)) +\n  geom_line(size = 1.2) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", alpha = 0.3) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.3) +\n  labs(title = \"Logistic Transition Function\",\n       subtitle = \"Higher γ = sharper transition between regimes\",\n       x = \"Standardized State Variable (z)\",\n       y = \"F(z) = Probability of 'Recession' Regime\",\n       color = \"Smoothness\") +\n  annotate(\"text\", x = -2, y = 0.9, label = \"Recession\\nregime\") +\n  annotate(\"text\", x = 2, y = 0.1, label = \"Expansion\\nregime\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSmooth transition function for state-dependent LP",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#testing-for-state-dependence",
    "href": "chapters/03_local_projections.html#testing-for-state-dependence",
    "title": "Local Projections",
    "section": "Testing for State Dependence",
    "text": "Testing for State Dependence\nTo test whether responses genuinely differ across states:\n\\[\nH_0: \\beta_0^h = \\beta_1^h \\quad \\forall h\n\\]\nMethods: 1. Joint F-test: Test equality of coefficients across horizons 2. Horizon-by-horizon t-tests: Test \\(\\beta_0^h - \\beta_1^h = 0\\) at each \\(h\\) 3. Cumulative difference: Test whether cumulative response differs\n\n\n\n\n\n\nWarningCommon Mistake\n\n\n\nShowing that \\(\\beta_0^h\\) is significant and \\(\\beta_1^h\\) is not significant does NOT prove they’re different. You must test the difference directly.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#the-setting",
    "href": "chapters/03_local_projections.html#the-setting",
    "title": "Local Projections",
    "section": "The Setting",
    "text": "The Setting\n\nSome units receive treatment at different times\nWe want to trace out the dynamic treatment effect at each horizon\nStandard DiD gives one number; LP-DiD gives a path",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#the-specification-1",
    "href": "chapters/03_local_projections.html#the-specification-1",
    "title": "Local Projections",
    "section": "The Specification",
    "text": "The Specification\n\\[\ny_{i,t+h} - y_{i,t-1} = \\alpha_i + \\delta_t + \\beta^h \\cdot D_{it} + \\mathbf{X}_{it}'\\boldsymbol{\\gamma}^h + \\varepsilon_{i,t+h}\n\\]\nwhere: - \\(D_{it} = 1\\) if unit \\(i\\) is treated at time \\(t\\) - \\(\\alpha_i\\) = unit fixed effects - \\(\\delta_t\\) = time fixed effects - \\(\\beta^h\\) = treatment effect at horizon \\(h\\)\nKey features: - Estimates treatment effect at each horizon (not just one average) - Includes negative horizons to test pre-trends - Robust to heterogeneous treatment timing\n\n\nCode\n# Simulate LP-DiD setting (simplified: single treatment time)\nN_did &lt;- 40\nT_did &lt;- 30\nT_treat &lt;- 15  # Treatment at period 15\n\n# Generate unit fixed effects\nunit_fe &lt;- rnorm(N_did, 0, 2)\ntime_fe &lt;- 0.1 * (1:T_did) + rnorm(T_did, 0, 0.3)\n\ndid_data &lt;- expand.grid(\n  unit = 1:N_did,\n  time = 1:T_did\n) %&gt;%\n  arrange(unit, time) %&gt;%\n  mutate(\n    # First 20 units are treated, rest are control\n    treated_unit = unit &lt;= 20,\n    post = time &gt;= T_treat,\n    treated = treated_unit & post,\n    rel_time = ifelse(treated_unit, time - T_treat, NA),\n    # Fixed effects\n    unit_effect = unit_fe[unit],\n    time_effect = time_fe[time],\n    # True treatment effect builds over time (max at +5)\n    true_effect = ifelse(treated, pmin(time - T_treat + 1, 5) * 0.4, 0),\n    y = unit_effect + time_effect + true_effect + rnorm(n(), 0, 0.8)\n  )\n\n# Estimate LP-DiD at each horizon\nH_did &lt;- 8\nlp_did_results &lt;- data.frame(\n  horizon = 0:H_did,\n  estimate = NA_real_,\n  se = NA_real_\n)\n\nfor (h in 0:H_did) {\n  # Create lead of y manually\n  did_data_h &lt;- did_data %&gt;%\n    group_by(unit) %&gt;%\n    mutate(\n      y_lead = lead(y, h),\n      y_lag = lag(y, 1)\n    ) %&gt;%\n    ungroup() %&gt;%\n    filter(!is.na(y_lead) & !is.na(y_lag))\n\n  model_did &lt;- feols(\n    y_lead - y_lag ~ treated | unit + time,\n    data = did_data_h,\n    vcov = ~unit\n  )\n\n  lp_did_results$estimate[h + 1] &lt;- coef(model_did)[\"treatedTRUE\"]\n  lp_did_results$se[h + 1] &lt;- se(model_did)[\"treatedTRUE\"]\n}\n\nlp_did_results &lt;- lp_did_results %&gt;%\n  mutate(\n    ci_low = estimate - 1.96 * se,\n    ci_high = estimate + 1.96 * se\n  )\n\n# Also compute pre-trend check (negative horizons)\n# For pre-trends, we test whether treated units had different growth rates\n# before treatment. Since treated_unit is time-invariant, we use time FE only\n# (not unit FE) to avoid collinearity.\npre_results &lt;- data.frame(\n  horizon = (-4):(-1),\n  estimate = NA_real_,\n  se = NA_real_\n)\n\nfor (h in (-4):(-1)) {\n  did_data_h &lt;- did_data %&gt;%\n    filter(time &lt; T_treat) %&gt;%  # Pre-treatment only\n    group_by(unit) %&gt;%\n    mutate(\n      y_lead = lead(y, -h),  # Note: negative h means lag\n      y_lag = lag(y, 1)\n    ) %&gt;%\n    ungroup() %&gt;%\n    filter(!is.na(y_lead) & !is.na(y_lag))\n\n  if (nrow(did_data_h) &gt; 50) {\n    # Use time FE only for pre-trends (treated_unit is unit-level, collinear with unit FE)\n    # This tests: did treated units have different GROWTH before treatment?\n    model_pre &lt;- feols(\n      y_lead - y_lag ~ treated_unit | time,\n      data = did_data_h,\n      vcov = ~unit\n    )\n    idx &lt;- h + 5\n    pre_results$estimate[idx] &lt;- coef(model_pre)[\"treated_unitTRUE\"]\n    pre_results$se[idx] &lt;- se(model_pre)[\"treated_unitTRUE\"]\n  }\n}\n\npre_results &lt;- pre_results %&gt;%\n  mutate(ci_low = estimate - 1.96 * se, ci_high = estimate + 1.96 * se)\n\n# Combine\nall_results &lt;- bind_rows(\n  pre_results %&gt;% mutate(period = \"Pre-treatment\"),\n  lp_did_results %&gt;% mutate(period = \"Post-treatment\")\n)\n\nggplot(all_results, aes(x = horizon, y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", alpha = 0.5) +\n  geom_ribbon(aes(ymin = ci_low, ymax = ci_high, fill = period), alpha = 0.2) +\n  geom_line(aes(color = period), size = 1.2) +\n  geom_point(aes(color = period), size = 2) +\n  scale_color_manual(values = c(\"Pre-treatment\" = \"#95a5a6\", \"Post-treatment\" = \"#e74c3c\")) +\n  scale_fill_manual(values = c(\"Pre-treatment\" = \"#95a5a6\", \"Post-treatment\" = \"#e74c3c\")) +\n  labs(title = \"LP-DiD: Dynamic Treatment Effects\",\n       subtitle = \"Pre-treatment coefficients test parallel trends\",\n       x = \"Horizon (periods relative to treatment)\",\n       y = \"Treatment Effect\",\n       color = \"Period\", fill = \"Period\") +\n  annotate(\"text\", x = -2.5, y = 0.3, label = \"Pre-trends\\n(≈ 0)\", size = 3) +\n  annotate(\"text\", x = 5, y = 2, label = \"Treatment\\neffect builds\", size = 3) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nLP-DiD: Dynamic treatment effects with pre-trend check",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#lp-did-vs.-standard-event-study",
    "href": "chapters/03_local_projections.html#lp-did-vs.-standard-event-study",
    "title": "Local Projections",
    "section": "LP-DiD vs. Standard Event Study",
    "text": "LP-DiD vs. Standard Event Study\n\n\n\nFeature\nStandard Event Study\nLP-DiD\n\n\n\n\nLHS\n\\(y_{it}\\)\n\\(y_{i,t+h} - y_{i,t-1}\\)\n\n\nInterpretation\nLevel relative to \\(t=-1\\)\nCumulative change\n\n\nHorizons\nRelative time dummies\nDirect projection\n\n\nHandles dynamics\nImposes structure\nFlexible\n\n\n\nLP-DiD is particularly useful when: - Treatment effects build gradually - You want impulse response interpretation - Dynamics are complex or nonlinear",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#sample-erosion-at-long-horizons",
    "href": "chapters/03_local_projections.html#sample-erosion-at-long-horizons",
    "title": "Local Projections",
    "section": "Sample Erosion at Long Horizons",
    "text": "Sample Erosion at Long Horizons\nAt horizon \\(h\\), you lose \\(h\\) observations from the end of your sample: - \\(h = 0\\): Full sample - \\(h = 12\\): Lose 12 periods (3 years of quarterly data)\n\n\nCode\nsample_sizes &lt;- data.frame(\n  horizon = 0:H,\n  n_obs = sapply(0:H, function(h) {\n    sum(!is.na(panel_data$y) &\n        (panel_data$time &lt;= T_obs - h) &\n        (panel_data$time &gt; 2))\n  })\n)\n\nggplot(sample_sizes, aes(x = horizon, y = n_obs)) +\n  geom_line(size = 1.2, color = \"#9b59b6\") +\n  geom_point(size = 2, color = \"#9b59b6\") +\n  labs(title = \"Sample Size by Horizon\",\n       subtitle = \"Sample shrinks as horizon increases\",\n       x = \"Horizon\", y = \"Number of Observations\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\nImplications: - Confidence intervals widen at longer horizons (less data + more uncertainty) - Truncate horizons when CIs become uninformative - Report effective sample size at each horizon",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#lag-selection",
    "href": "chapters/03_local_projections.html#lag-selection",
    "title": "Local Projections",
    "section": "Lag Selection",
    "text": "Lag Selection\nRule: Use the same lag structure across all horizons for comparability.\nMethods: 1. Information criteria (AIC, BIC) on the \\(h = 0\\) regression 2. LM test for serial correlation in residuals 3. Rule of thumb: 2-4 lags for quarterly, 1-2 for annual\n\n\nCode\n# Compare different lag specifications at h = 4\nlag_comparison &lt;- data.frame(\n  lags = 1:6,\n  aic = NA_real_,\n  bic = NA_real_\n)\n\nh_test &lt;- 4\n\nfor (p in 1:6) {\n  # Build formula with p lags\n  formula_str &lt;- paste0(\"f(y, \", h_test, \") - l(y, 1) ~ shock + l(y, 1:\", p, \") | country + time\")\n\n  model_lag &lt;- feols(as.formula(formula_str), data = pdata, vcov = ~country)\n\n  lag_comparison$aic[p] &lt;- AIC(model_lag)\n  lag_comparison$bic[p] &lt;- BIC(model_lag)\n}\n\nlag_comparison_long &lt;- lag_comparison %&gt;%\n  pivot_longer(cols = c(aic, bic), names_to = \"criterion\", values_to = \"value\")\n\nggplot(lag_comparison_long, aes(x = lags, y = value, color = criterion)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"aic\" = \"#3498db\", \"bic\" = \"#e74c3c\"),\n                     labels = c(\"AIC\", \"BIC\")) +\n  labs(title = \"Lag Selection for LP\",\n       subtitle = paste(\"Information criteria at horizon h =\", h_test),\n       x = \"Number of Lags\", y = \"Information Criterion\",\n       color = \"Criterion\") +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#cumulative-vs.-non-cumulative",
    "href": "chapters/03_local_projections.html#cumulative-vs.-non-cumulative",
    "title": "Local Projections",
    "section": "Cumulative vs. Non-Cumulative",
    "text": "Cumulative vs. Non-Cumulative\nCumulative (recommended for levels): \\[\n\\text{LHS} = y_{t+h} - y_{t-1}\n\\] Interpretation: Total change from \\(t-1\\) to \\(t+h\\).\nNon-cumulative: \\[\n\\text{LHS} = y_{t+h}\n\\] Interpretation: Level at \\(t+h\\) (includes pre-existing trend).\nUse cumulative for: - GDP, debt/GDP, price level - Any variable where you care about the total change\nUse non-cumulative for: - Growth rates, inflation (already differenced) - When you want the level effect",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#comparing-lp-and-var-irfs",
    "href": "chapters/03_local_projections.html#comparing-lp-and-var-irfs",
    "title": "Local Projections",
    "section": "Comparing LP and VAR IRFs",
    "text": "Comparing LP and VAR IRFs\nAlways compare LP and VAR when possible:\n\n\nCode\n# This is stylized - in practice, estimate both and compare\n\ncomparison_data &lt;- data.frame(\n  horizon = rep(0:H, 2),\n  method = rep(c(\"LP\", \"VAR\"), each = H + 1),\n  estimate = c(results$estimate, results$estimate * 0.95),  # Stylized: VAR slightly different\n  ci_width = c(results$se * 1.96 * 2, results$se * 1.96 * 1.3)  # LP wider CIs\n)\n\nggplot(comparison_data, aes(x = horizon, y = estimate, color = method)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = estimate - ci_width/2, ymax = estimate + ci_width/2),\n                width = 0.3, alpha = 0.5) +\n  scale_color_manual(values = c(\"LP\" = \"#e74c3c\", \"VAR\" = \"#3498db\")) +\n  labs(title = \"LP vs VAR: Point Estimates and Confidence Intervals\",\n       subtitle = \"LP has wider CIs but is more robust to misspecification\",\n       x = \"Horizon\", y = \"Response\",\n       color = \"Method\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nLP produces wider but more robust confidence intervals than VAR\n\n\n\n\nWhen LP and VAR disagree: - Check VAR specification (lags, variables) - LP is likely more robust to misspecification - VAR may be picking up spurious dynamics",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#when-should-i-use-lp-vs.-var",
    "href": "chapters/03_local_projections.html#when-should-i-use-lp-vs.-var",
    "title": "Local Projections",
    "section": "When should I use LP vs. VAR?",
    "text": "When should I use LP vs. VAR?\nUse LP when: - You only need to identify ONE shock - Misspecification is a concern - You want state-dependent or nonlinear responses - You’re working with panel data\nUse VAR when: - You need the full system (FEVD, historical decomposition) - Efficiency matters and you trust the specification - Short horizons with well-identified structure",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#how-do-i-choose-the-horizon-h",
    "href": "chapters/03_local_projections.html#how-do-i-choose-the-horizon-h",
    "title": "Local Projections",
    "section": "How do I choose the horizon H?",
    "text": "How do I choose the horizon H?\n\nPlot IRFs and extend until they converge to zero (or steady state)\nStop when confidence intervals become uninformative\nRule of thumb: H = 4-5 years for quarterly macro data\nReport effective sample sizes",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#what-controls-should-i-include",
    "href": "chapters/03_local_projections.html#what-controls-should-i-include",
    "title": "Local Projections",
    "section": "What controls should I include?",
    "text": "What controls should I include?\n\nLagged dependent variable (always)\nVariables that affect both shock and outcome (confounders)\nDon’t include post-treatment controls (bad controls)\nKeep controls consistent across horizons",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/03_local_projections.html#how-do-i-report-results",
    "href": "chapters/03_local_projections.html#how-do-i-report-results",
    "title": "Local Projections",
    "section": "How do I report results?",
    "text": "How do I report results?\n\nPlot the IRF with confidence bands\nReport the peak response and time to peak\nTable with point estimates and SEs at key horizons\nReport sample size at each horizon\nCompare with VAR if possible",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Local Projections</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html",
    "href": "chapters/04_var_svar.html",
    "title": "Vector Autoregressions",
    "section": "",
    "text": "The VAR Framework\nVector Autoregressions (VARs) model the joint dynamics of multiple time series. VARs became the workhorse of empirical macroeconomics because they impose minimal theoretical structure while capturing rich dynamic interactions. For a comprehensive treatment, see Kilian and Lütkepohl (2017).",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#from-single-equations-to-systems",
    "href": "chapters/04_var_svar.html#from-single-equations-to-systems",
    "title": "Vector Autoregressions",
    "section": "From Single Equations to Systems",
    "text": "From Single Equations to Systems\nConsider a simple monetary policy question: How does output respond to an interest rate change?\nSingle equation approach: \\[\ny_t = \\alpha + \\beta \\cdot r_t + \\varepsilon_t\n\\]\nProblems: - Interest rates respond to output (simultaneity) - Past values of both matter (dynamics) - Other variables are omitted\nVAR approach: Model everything jointly: \\[\n\\begin{bmatrix} y_t \\\\ \\pi_t \\\\ r_t \\end{bmatrix} = \\mathbf{c} + \\mathbf{A}_1 \\begin{bmatrix} y_{t-1} \\\\ \\pi_{t-1} \\\\ r_{t-1} \\end{bmatrix} + \\mathbf{A}_2 \\begin{bmatrix} y_{t-2} \\\\ \\pi_{t-2} \\\\ r_{t-2} \\end{bmatrix} + \\begin{bmatrix} u_{1t} \\\\ u_{2t} \\\\ u_{3t} \\end{bmatrix}\n\\]",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-model",
    "href": "chapters/04_var_svar.html#the-model",
    "title": "Vector Autoregressions",
    "section": "The Model",
    "text": "The Model\nA VAR(p) for K variables: \\[\n\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1 \\mathbf{y}_{t-1} + \\mathbf{A}_2 \\mathbf{y}_{t-2} + \\cdots + \\mathbf{A}_p \\mathbf{y}_{t-p} + \\mathbf{u}_t\n\\]\nwhere: - \\(\\mathbf{y}_t\\) is \\(K \\times 1\\) vector of endogenous variables - \\(\\mathbf{A}_j\\) are \\(K \\times K\\) coefficient matrices - \\(\\mathbf{u}_t \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma}_u)\\) are reduced-form residuals - \\(\\boldsymbol{\\Sigma}_u\\) captures contemporaneous correlations\nKey property: Reduced-form residuals \\(\\mathbf{u}_t\\) are correlated across equations. They are not structural shocks.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#estimation",
    "href": "chapters/04_var_svar.html#estimation",
    "title": "Vector Autoregressions",
    "section": "Estimation",
    "text": "Estimation\nEach equation can be estimated by OLS (since all equations have the same regressors). Let’s simulate and estimate.\n\n\nCode\n# Simulate a simple 3-variable VAR(2)\nsimulate_var &lt;- function(T_obs, A1, A2, Sigma, burn = 100) {\n  K &lt;- nrow(A1)\n  Y &lt;- matrix(0, T_obs + burn, K)\n\n  # Generate shocks\n  U &lt;- mvrnorm(T_obs + burn, mu = rep(0, K), Sigma = Sigma)\n\n  for (t in 3:(T_obs + burn)) {\n    Y[t, ] &lt;- A1 %*% Y[t-1, ] + A2 %*% Y[t-2, ] + U[t, ]\n  }\n\n  Y[(burn + 1):(T_obs + burn), ]\n}\n\n# True parameters\nA1_true &lt;- matrix(c(\n  0.5,  0.1,  0.0,   # GDP equation\n  0.2,  0.6, -0.1,   # Inflation equation\n  0.1,  0.3,  0.7    # Interest rate equation\n), nrow = 3, byrow = TRUE)\n\nA2_true &lt;- matrix(c(\n  0.2, -0.1,  0.0,\n  0.0,  0.1,  0.05,\n  0.0,  0.1,  0.1\n), nrow = 3, byrow = TRUE)\n\n# Reduced-form covariance (correlated shocks)\nSigma_true &lt;- matrix(c(\n  1.0, 0.3, 0.1,\n  0.3, 0.5, 0.2,\n  0.1, 0.2, 0.3\n), nrow = 3)\n\n# Simulate\nY &lt;- simulate_var(200, A1_true, A2_true, Sigma_true)\ncolnames(Y) &lt;- c(\"GDP\", \"Inflation\", \"Rate\")\n\n# Plot\ndata.frame(\n  time = 1:nrow(Y),\n  as.data.frame(Y)\n) %&gt;%\n  pivot_longer(-time, names_to = \"variable\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = time, y = value)) +\n  geom_line(color = \"#3498db\") +\n  facet_wrap(~variable, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Simulated VAR(2) Data\",\n       x = \"Time\", y = \"Value\")\n\n\n\n\n\nSimulated three-variable VAR system\n\n\n\n\n\n\nCode\n# Estimate VAR using vars package\nY_ts &lt;- ts(Y, frequency = 4)\n\n# Select lag length\nlag_select &lt;- VARselect(Y_ts, lag.max = 8, type = \"const\")\ncat(\"Lag selection criteria:\\n\")\n\n\nLag selection criteria:\n\n\nCode\nprint(lag_select$selection)\n\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     2      1      1      2 \n\n\nCode\n# Estimate VAR(2)\nvar_model &lt;- VAR(Y_ts, p = 2, type = \"const\")\n\n# Check stability\ncat(\"\\nEigenvalues of companion matrix (all should be &lt; 1):\\n\")\n\n\n\nEigenvalues of companion matrix (all should be &lt; 1):\n\n\nCode\nprint(round(roots(var_model), 3))\n\n\n[1] 0.838 0.838 0.639 0.371 0.123 0.123",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#companion-form-and-stability",
    "href": "chapters/04_var_svar.html#companion-form-and-stability",
    "title": "Vector Autoregressions",
    "section": "Companion Form and Stability",
    "text": "Companion Form and Stability\nFor analysis, we write the VAR in companion form: \\[\n\\mathbf{Y}_t = \\mathbf{A}_{comp} \\mathbf{Y}_{t-1} + \\mathbf{U}_t\n\\]\nwhere \\(\\mathbf{Y}_t = [\\mathbf{y}_t', \\mathbf{y}_{t-1}', \\ldots]'\\) stacks lagged values.\nStability condition: All eigenvalues of \\(\\mathbf{A}_{comp}\\) must lie inside the unit circle. If any eigenvalue equals 1, there’s a unit root; if greater than 1, the system is explosive.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#why-structure-matters",
    "href": "chapters/04_var_svar.html#why-structure-matters",
    "title": "Vector Autoregressions",
    "section": "Why Structure Matters",
    "text": "Why Structure Matters\nThe reduced-form VAR gives us \\(\\boldsymbol{\\Sigma}_u\\)—the covariance of residuals. But we want structural shocks \\(\\boldsymbol{\\varepsilon}_t\\) that are: - Uncorrelated with each other - Have economic interpretation (demand shock, supply shock, monetary shock)\nThe structural form: \\[\n\\mathbf{B}_0 \\mathbf{y}_t = \\mathbf{b}_0 + \\mathbf{B}_1 \\mathbf{y}_{t-1} + \\cdots + \\boldsymbol{\\varepsilon}_t, \\quad \\boldsymbol{\\varepsilon}_t \\sim N(\\mathbf{0}, \\mathbf{I})\n\\]\nConnection to reduced form: \\[\n\\mathbf{u}_t = \\mathbf{B}_0^{-1} \\boldsymbol{\\varepsilon}_t \\implies \\boldsymbol{\\Sigma}_u = \\mathbf{B}_0^{-1} (\\mathbf{B}_0^{-1})'\n\\]",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-counting-problem",
    "href": "chapters/04_var_svar.html#the-counting-problem",
    "title": "Vector Autoregressions",
    "section": "The Counting Problem",
    "text": "The Counting Problem\nWith K variables: - \\(\\boldsymbol{\\Sigma}_u\\) has \\(K(K+1)/2\\) unique elements - \\(\\mathbf{B}_0^{-1}\\) has \\(K^2\\) elements - We need \\(K(K-1)/2\\) additional restrictions\nFor K=3: We observe 6 unique elements of \\(\\Sigma_u\\), but need 9 elements of \\(B_0^{-1}\\). We need 3 restrictions.\n\n\n\n\n\n\nImportantThe Fundamental Challenge\n\n\n\nMoving from reduced-form to structural requires identifying assumptions. Different assumptions = different structural shocks = potentially different conclusions.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-approach",
    "href": "chapters/04_var_svar.html#the-approach",
    "title": "Vector Autoregressions",
    "section": "The Approach",
    "text": "The Approach\nThe simplest identification: assume a recursive causal ordering.\n\\[\n\\mathbf{B}_0^{-1} = \\text{chol}(\\boldsymbol{\\Sigma}_u) \\quad \\text{(lower triangular)}\n\\]\nThis imposes: - Variable 1 doesn’t respond contemporaneously to variables 2 or 3 - Variable 2 doesn’t respond contemporaneously to variable 3 - Variable 3 responds to everything",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#economic-interpretation",
    "href": "chapters/04_var_svar.html#economic-interpretation",
    "title": "Vector Autoregressions",
    "section": "Economic Interpretation",
    "text": "Economic Interpretation\nFor monetary policy, a common ordering: slow → policy → fast\n\nGDP (slow): doesn’t respond within quarter to anything\nInflation (medium): responds to GDP shocks, not rate within quarter\nInterest rate (fast): central bank sees GDP and inflation, then sets rate\n\n\n\nCode\n# Compute Cholesky-identified IRFs\nirf_chol &lt;- irf(var_model, impulse = \"Rate\", response = c(\"GDP\", \"Inflation\", \"Rate\"),\n                n.ahead = 20, ortho = TRUE, boot = TRUE, ci = 0.68, runs = 100)\n\n# Extract and plot\nplot_irf &lt;- function(irf_obj, shock_name) {\n  responses &lt;- names(irf_obj$irf)\n\n  irf_data &lt;- lapply(responses, function(resp) {\n    data.frame(\n      horizon = 0:(length(irf_obj$irf[[resp]][, 1]) - 1),\n      response = resp,\n      estimate = irf_obj$irf[[resp]][, 1],\n      lower = irf_obj$Lower[[resp]][, 1],\n      upper = irf_obj$Upper[[resp]][, 1]\n    )\n  }) %&gt;% bind_rows()\n\n  ggplot(irf_data, aes(x = horizon, y = estimate)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = \"#e74c3c\") +\n    geom_line(color = \"#e74c3c\", size = 1) +\n    facet_wrap(~response, scales = \"free_y\") +\n    labs(title = paste(\"Response to\", shock_name, \"Shock\"),\n         subtitle = \"68% bootstrap confidence bands\",\n         x = \"Horizon (quarters)\", y = \"Response\")\n}\n\nplot_irf(irf_chol, \"Monetary Policy (Rate)\")\n\n\n\n\n\nImpulse responses to monetary policy shock (Cholesky identification)",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-ordering-problem",
    "href": "chapters/04_var_svar.html#the-ordering-problem",
    "title": "Vector Autoregressions",
    "section": "The Ordering Problem",
    "text": "The Ordering Problem\nCholesky identification is sensitive to ordering. Let’s see what happens with a different order:\n\n\nCode\n# Original ordering: GDP, Inflation, Rate\nY_order1 &lt;- Y_ts[, c(\"GDP\", \"Inflation\", \"Rate\")]\nvar_order1 &lt;- VAR(Y_order1, p = 2, type = \"const\")\nirf_order1 &lt;- irf(var_order1, impulse = \"Rate\", response = \"GDP\",\n                   n.ahead = 20, ortho = TRUE, boot = FALSE)\n\n# Alternative ordering: Rate, GDP, Inflation (rate first)\nY_order2 &lt;- Y_ts[, c(\"Rate\", \"GDP\", \"Inflation\")]\nvar_order2 &lt;- VAR(Y_order2, p = 2, type = \"const\")\nirf_order2 &lt;- irf(var_order2, impulse = \"Rate\", response = \"GDP\",\n                   n.ahead = 20, ortho = TRUE, boot = FALSE)\n\n# Compare\ncompare_df &lt;- data.frame(\n  horizon = 0:20,\n  Order1 = irf_order1$irf$Rate[, \"GDP\"],\n  Order2 = irf_order2$irf$Rate[, \"GDP\"]\n) %&gt;%\n  pivot_longer(-horizon, names_to = \"ordering\", values_to = \"response\")\n\nggplot(compare_df, aes(x = horizon, y = response, color = ordering)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"Order1\" = \"#3498db\", \"Order2\" = \"#e74c3c\"),\n                     labels = c(\"GDP→Infl→Rate\", \"Rate→GDP→Infl\")) +\n  labs(title = \"GDP Response to Rate Shock: Ordering Matters\",\n       subtitle = \"Same data, different Cholesky orderings\",\n       x = \"Horizon\", y = \"GDP Response\",\n       color = \"Ordering\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nIRF sensitivity to variable ordering\n\n\n\n\n\n\n\n\n\n\nWarningAlways Check Robustness\n\n\n\nIf your conclusions change substantially with ordering, your identification is fragile. Report results under alternative orderings.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-idea",
    "href": "chapters/04_var_svar.html#the-idea",
    "title": "Vector Autoregressions",
    "section": "The Idea",
    "text": "The Idea\nInstead of exact zero restrictions, impose inequality constraints based on economic theory.\nFor a monetary policy tightening: - Interest rate rises (+) - Output falls (−) - Inflation falls (−)\nThis doesn’t uniquely identify the shock—it defines a set of admissible structural models.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#algorithm",
    "href": "chapters/04_var_svar.html#algorithm",
    "title": "Vector Autoregressions",
    "section": "Algorithm",
    "text": "Algorithm\n\nCompute any valid \\(\\mathbf{B}_0^{-1}\\) (e.g., Cholesky)\nDraw a random orthogonal matrix \\(\\mathbf{Q}\\)\nCandidate: \\(\\tilde{\\mathbf{B}} = \\mathbf{B}_0^{-1} \\mathbf{Q}\\)\nCompute IRFs with \\(\\tilde{\\mathbf{B}}\\)\nCheck if signs match restrictions\nIf yes: accept. If no: reject and return to step 2.\n\n\n\nCode\n# Simplified sign restriction illustration\n# (Full implementation available in svars package)\n\n# For a 3-variable system with monetary policy shock restrictions:\n# - GDP falls (-)\n# - Inflation falls (-)\n# - Rate rises (+)\n\n# Illustrate the concept with Cholesky baseline and perturbations\nK &lt;- 3\nSigma_est &lt;- summary(var_model)$covres\nB0_inv &lt;- t(chol(Sigma_est))\n\n# Draw multiple rotations and keep those satisfying sign restrictions\nset.seed(42)\nn_draw &lt;- 50\naccepted &lt;- list()\nn_accepted &lt;- 0\n\nfor (i in 1:500) {\n  # Random orthogonal matrix\n  X &lt;- matrix(rnorm(K * K), K, K)\n  Q &lt;- qr.Q(qr(X))\n\n  # Candidate impact\n  B_cand &lt;- B0_inv %*% Q\n\n  # Check signs for monetary shock (column 3): GDP(-), Infl(-), Rate(+)\n  if (B_cand[1, 3] &lt; 0 && B_cand[2, 3] &lt; 0 && B_cand[3, 3] &gt; 0) {\n    n_accepted &lt;- n_accepted + 1\n    accepted[[n_accepted]] &lt;- B_cand[, 3]  # Just save impact vector\n    if (n_accepted &gt;= n_draw) break\n  }\n}\n\ncat(\"Acceptance rate:\", round(n_accepted / min(i, 500) * 100, 1), \"%\\n\")\n\n\nAcceptance rate: 24.3 %\n\n\nCode\ncat(\"Accepted draws:\", n_accepted, \"\\n\")\n\n\nAccepted draws: 50 \n\n\nCode\n# Show range of impact effects\nimpact_matrix &lt;- do.call(rbind, accepted)\ncolnames(impact_matrix) &lt;- c(\"GDP\", \"Inflation\", \"Rate\")\n\nimpact_summary &lt;- data.frame(\n  variable = c(\"GDP\", \"Inflation\", \"Rate\"),\n  median = apply(impact_matrix, 2, median),\n  lower = apply(impact_matrix, 2, quantile, 0.16),\n  upper = apply(impact_matrix, 2, quantile, 0.84)\n)\n\nggplot(impact_summary, aes(x = variable, y = median)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, color = \"#9b59b6\", size = 1) +\n  geom_point(color = \"#9b59b6\", size = 4) +\n  labs(title = \"Sign-Restricted Impact Effects: Monetary Policy Shock\",\n       subtitle = \"Median and 68% range across accepted rotations\",\n       x = \"Variable\", y = \"Impact Response\") +\n  theme(axis.text.x = element_text(size = 12))\n\n\n\n\n\nSign-restricted IRFs show a range of admissible models\n\n\n\n\n\n\n\n\n\n\nNoteFull Implementation\n\n\n\nFor complete sign-restricted IRFs across all horizons, use the svars package:\nlibrary(svars)\nid.sign(var_model, restriction_matrix = sign_matrix)",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#set-identification-vs.-point-identification",
    "href": "chapters/04_var_svar.html#set-identification-vs.-point-identification",
    "title": "Vector Autoregressions",
    "section": "Set Identification vs. Point Identification",
    "text": "Set Identification vs. Point Identification\n\n\n\n\n\n\n\n\nAspect\nPoint Identification (Cholesky)\nSet Identification (Signs)\n\n\n\n\nResult\nSingle IRF\nRange of IRFs\n\n\nAssumptions\nExact zeros\nInequality constraints\n\n\nRobustness\nSensitive to ordering\nTheory-guided\n\n\nInterpretation\n“The” effect\nBounds on effect",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-approach-1",
    "href": "chapters/04_var_svar.html#the-approach-1",
    "title": "Vector Autoregressions",
    "section": "The Approach",
    "text": "The Approach\nUse an external instrument \\(z_t\\) correlated with one structural shock but not others.\nRequirements: - Relevance: \\(E[z_t \\varepsilon_{1t}] \\neq 0\\) - Exogeneity: \\(E[z_t \\varepsilon_{jt}] = 0\\) for \\(j \\neq 1\\)\nCommon instruments for monetary policy: - High-frequency surprises around FOMC announcements - Narrative identification (Romer & Romer)",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#implementation",
    "href": "chapters/04_var_svar.html#implementation",
    "title": "Vector Autoregressions",
    "section": "Implementation",
    "text": "Implementation\n\n\nCode\n# Simulate an external instrument\n# True: z is correlated with the monetary shock only\nset.seed(123)\nT_obs &lt;- nrow(Y)\n\n# Generate structural shocks (we'll pretend we don't observe these)\n# For simulation, we know the true Cholesky gives us structural shocks\nSigma_est &lt;- cov(residuals(var_model))\nB0_inv_true &lt;- t(chol(Sigma_est))\nu_hat &lt;- residuals(var_model)\neps_hat &lt;- solve(B0_inv_true) %*% t(u_hat)  # \"structural\" shocks\n\n# External instrument: correlated with monetary shock (row 3) + noise\nphi &lt;- 0.6  # relevance\nz_t &lt;- phi * eps_hat[3, ] + rnorm(T_obs - 2, 0, 1)\n\n# Proxy SVAR estimation\nproxy_svar &lt;- function(var_model, z, policy_pos = 3) {\n  U &lt;- residuals(var_model)\n  K &lt;- ncol(U)\n  T_eff &lt;- nrow(U)\n\n  # Align instrument\n  z_aligned &lt;- z[1:T_eff]\n\n  # First stage: regress policy residual on instrument\n  u_policy &lt;- U[, policy_pos]\n  first_stage &lt;- lm(u_policy ~ z_aligned)\n  u_hat &lt;- fitted(first_stage)\n  F_stat &lt;- summary(first_stage)$fstatistic[1]\n\n  cat(\"First-stage F-statistic:\", round(F_stat, 2), \"\\n\")\n  if (F_stat &lt; 10) warning(\"Weak instrument! F &lt; 10\")\n\n  # Second stage: relative impacts\n  s &lt;- numeric(K)\n  s[policy_pos] &lt;- 1\n  for (j in setdiff(1:K, policy_pos)) {\n    second_stage &lt;- lm(U[, j] ~ u_hat - 1)\n    s[j] &lt;- coef(second_stage)[1]\n  }\n\n  list(impact = s, F_stat = F_stat)\n}\n\nproxy_result &lt;- proxy_svar(var_model, z_t, policy_pos = 3)\n\n\nFirst-stage F-statistic: 76.05 \n\n\nCode\ncat(\"Estimated impact vector:\", round(proxy_result$impact, 3), \"\\n\")\n\n\nEstimated impact vector: -0.02 0.292 1 \n\n\n\n\n\n\n\n\nTipThe F-Statistic Rule\n\n\n\nA first-stage F-statistic below 10 indicates a weak instrument. Inference becomes unreliable. Always report the F-stat!",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-question",
    "href": "chapters/04_var_svar.html#the-question",
    "title": "Vector Autoregressions",
    "section": "The Question",
    "text": "The Question\nAt horizon \\(h\\), what fraction of the forecast error variance of variable \\(i\\) is attributable to structural shock \\(j\\)?\n\\[\n\\text{FEVD}_{i,j}(h) = \\frac{\\sum_{s=0}^{h} (\\mathbf{e}_i' \\boldsymbol{\\Phi}_s \\mathbf{B}_0^{-1} \\mathbf{e}_j)^2}{\\sum_{s=0}^{h} \\mathbf{e}_i' \\boldsymbol{\\Phi}_s \\boldsymbol{\\Sigma}_u \\boldsymbol{\\Phi}_s' \\mathbf{e}_i}\n\\]",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#implementation-1",
    "href": "chapters/04_var_svar.html#implementation-1",
    "title": "Vector Autoregressions",
    "section": "Implementation",
    "text": "Implementation\n\n\nCode\n# FEVD from vars package\nfevd_result &lt;- fevd(var_model, n.ahead = 20)\n\n# Plot\nfevd_df &lt;- lapply(names(fevd_result), function(var) {\n  df &lt;- as.data.frame(fevd_result[[var]])\n  df$horizon &lt;- 1:nrow(df)\n  df$response &lt;- var\n  df %&gt;%\n    pivot_longer(-c(horizon, response), names_to = \"shock\", values_to = \"share\")\n}) %&gt;% bind_rows()\n\nggplot(fevd_df, aes(x = horizon, y = share, fill = shock)) +\n  geom_area(alpha = 0.8) +\n  facet_wrap(~response) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Forecast Error Variance Decomposition\",\n       subtitle = \"Which shocks drive variation at each horizon?\",\n       x = \"Horizon\", y = \"Share of Variance\",\n       fill = \"Shock\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nForecast error variance decomposition over horizons",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#interpretation",
    "href": "chapters/04_var_svar.html#interpretation",
    "title": "Vector Autoregressions",
    "section": "Interpretation",
    "text": "Interpretation\n\nAt short horizons, own shocks typically dominate\nAs horizons extend, other shocks become relatively more important\nAt long horizons: reveals which shocks drive permanent movements",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#the-question-1",
    "href": "chapters/04_var_svar.html#the-question-1",
    "title": "Vector Autoregressions",
    "section": "The Question",
    "text": "The Question\nWhat would the path of variable \\(i\\) have been if only shock \\(j\\) had occurred?\n\\[\ny_{it} = \\text{deterministic} + \\sum_{j=1}^{K} \\underbrace{\\sum_{s=0}^{t} \\Phi_{ij,s} \\cdot \\varepsilon_{j,t-s}}_{\\text{contribution of shock } j}\n\\]\n\n\nCode\n# Simplified historical decomposition illustration\n# Shows the concept without expensive recursive computation\n\n# Get structural shocks (Cholesky-identified)\nB0_inv_est &lt;- t(chol(cov(residuals(var_model))))\nU &lt;- residuals(var_model)\neps &lt;- solve(B0_inv_est) %*% t(U)  # structural shocks (K x T)\n\n# For illustration, show cumulative contributions over a window\nT_eff &lt;- ncol(eps)\nK &lt;- 3\n\n# Simple approximation: use VAR coefficients at lag 1 only\nA1 &lt;- Acoef(var_model)[[1]]\n\n# Contribution of each shock to GDP (variable 1)\n# At each time t, contribution ≈ impact + A1 * previous contributions\nhd_simple &lt;- matrix(0, T_eff, K)\n\nfor (t in 1:T_eff) {\n  # Impact effect: row 1 of B0_inv times structural shocks at time t\n  hd_simple[t, ] &lt;- B0_inv_est[1, ] * eps[, t]\n\n  # Add propagation from recent shocks (simplified)\n  if (t &gt; 1) {\n    hd_simple[t, ] &lt;- hd_simple[t, ] + 0.5 * hd_simple[t-1, ]\n  }\n}\n\n# Plot\nhd_gdp &lt;- data.frame(\n  time = 1:T_eff,\n  `GDP Shock` = hd_simple[, 1],\n  `Inflation Shock` = hd_simple[, 2],\n  `Rate Shock` = hd_simple[, 3],\n  check.names = FALSE\n) %&gt;%\n  pivot_longer(-time, names_to = \"shock\", values_to = \"contribution\")\n\nggplot(hd_gdp, aes(x = time, y = contribution, fill = shock)) +\n  geom_area(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Historical Decomposition of GDP (Simplified)\",\n       subtitle = \"Contributions of each structural shock to GDP movements\",\n       x = \"Time\", y = \"Contribution to GDP\",\n       fill = \"Shock Source\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nHistorical decomposition: contributions of each shock to GDP\n\n\n\n\n\n\n\n\n\n\nNoteFull Historical Decomposition\n\n\n\nThe vars package provides complete historical decomposition via historical_decomposition(). The simplified version above illustrates the concept.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#lag-length-selection",
    "href": "chapters/04_var_svar.html#lag-length-selection",
    "title": "Vector Autoregressions",
    "section": "Lag Length Selection",
    "text": "Lag Length Selection\n\n\nCode\n# Already computed above, let's visualize\nlag_criteria &lt;- data.frame(\n  lags = 1:8,\n  AIC = sapply(1:8, function(p) AIC(VAR(Y_ts, p = p, type = \"const\"))),\n  BIC = sapply(1:8, function(p) BIC(VAR(Y_ts, p = p, type = \"const\")))\n)\n\nlag_criteria_long &lt;- lag_criteria %&gt;%\n  pivot_longer(-lags, names_to = \"criterion\", values_to = \"value\")\n\nggplot(lag_criteria_long, aes(x = lags, y = value, color = criterion)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"AIC\" = \"#3498db\", \"BIC\" = \"#e74c3c\")) +\n  labs(title = \"Lag Selection Criteria\",\n       subtitle = \"Lower is better; AIC tends to select more lags than BIC\",\n       x = \"Number of Lags\", y = \"Information Criterion\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nInformation criteria for lag selection",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#residual-diagnostics",
    "href": "chapters/04_var_svar.html#residual-diagnostics",
    "title": "Vector Autoregressions",
    "section": "Residual Diagnostics",
    "text": "Residual Diagnostics\n\n\nCode\n# Portmanteau test for serial correlation\nserial_test &lt;- serial.test(var_model, lags.pt = 12, type = \"PT.asymptotic\")\ncat(\"Portmanteau Test for Serial Correlation:\\n\")\n\n\nPortmanteau Test for Serial Correlation:\n\n\nCode\ncat(\"Test statistic:\", round(serial_test$serial$statistic, 2), \"\\n\")\n\n\nTest statistic: 72.25 \n\n\nCode\ncat(\"p-value:\", round(serial_test$serial$p.value, 4), \"\\n\")\n\n\np-value: 0.9148 \n\n\nCode\ncat(\"Interpretation:\", ifelse(serial_test$serial$p.value &gt; 0.05,\n    \"No significant serial correlation (good)\",\n    \"Serial correlation detected (consider more lags)\"), \"\\n\")\n\n\nInterpretation: No significant serial correlation (good)",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#summary-checklist",
    "href": "chapters/04_var_svar.html#summary-checklist",
    "title": "Vector Autoregressions",
    "section": "Summary Checklist",
    "text": "Summary Checklist\nBefore trusting your VAR results:\n\nStability: All eigenvalues inside unit circle?\nLag length: Information criteria checked? Residual autocorrelation tests passed?\nIdentification: Assumptions documented? Robustness to alternatives checked?\nSample size: Enough observations relative to parameters?\nStructural breaks: Any reason to believe relationships changed?",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#equivalence-result",
    "href": "chapters/04_var_svar.html#equivalence-result",
    "title": "Vector Autoregressions",
    "section": "Equivalence Result",
    "text": "Equivalence Result\nTheorem (Plagborg-Møller & Wolf, 2021): Under correct specification, LP and VAR estimate the same population IRFs.",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#practical-differences",
    "href": "chapters/04_var_svar.html#practical-differences",
    "title": "Vector Autoregressions",
    "section": "Practical Differences",
    "text": "Practical Differences\n\n\n\nAspect\nVAR\nLP\n\n\n\n\nSpecification\nFull system\nHorizon-by-horizon\n\n\nEfficiency\nMore efficient if correct\nLess efficient\n\n\nRobustness\nSensitive to misspecification\nRobust\n\n\nNonlinearity\nDifficult (threshold VAR)\nEasy (interactions)\n\n\nConfidence intervals\nTighter\nWider (but honest)\n\n\nOutput\nIRFs, FEVD, historical decomp\nIRFs only",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/04_var_svar.html#recommendations",
    "href": "chapters/04_var_svar.html#recommendations",
    "title": "Vector Autoregressions",
    "section": "Recommendations",
    "text": "Recommendations\nUse VAR when: - You want FEVD or historical decomposition - System is well-specified and identified - Short horizons with good theoretical grounding\nUse LP when: - Single shock identification only - State-dependence or nonlinearity matters - Robustness to misspecification is priority - Panel data",
    "crumbs": [
      "Phase 2: Dynamic Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html",
    "href": "chapters/05_staggered_did.html",
    "title": "Staggered Difference-in-Differences",
    "section": "",
    "text": "The DiD Revolution\nDifference-in-differences (DiD) is the workhorse of policy evaluation. But recent research has revealed that the standard two-way fixed effects (TWFE) estimator can produce severely biased estimates when treatment timing varies across units (Goodman-Bacon 2021; Callaway and Sant’Anna 2021; Roth et al. 2023).\nThis module covers the “credibility revolution” in DiD—understanding why TWFE fails with staggered adoption and how modern estimators fix the problem.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#the-classic-setup",
    "href": "chapters/05_staggered_did.html#the-classic-setup",
    "title": "Staggered Difference-in-Differences",
    "section": "The Classic Setup",
    "text": "The Classic Setup\nIn the canonical 2×2 DiD, we have: - Two groups: treated and control - Two periods: before and after treatment - Treatment happens to all treated units at the same time\nThe estimand: \\[\n\\text{ATT} = \\underbrace{(Y_{treated,post} - Y_{treated,pre})}_{\\text{treated change}} - \\underbrace{(Y_{control,post} - Y_{control,pre})}_{\\text{control change}}\n\\]\nThis works beautifully when treatment timing is uniform. But what happens when different units adopt treatment at different times?",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#the-regression",
    "href": "chapters/05_staggered_did.html#the-regression",
    "title": "Staggered Difference-in-Differences",
    "section": "The Regression",
    "text": "The Regression\n\\[\ny_{it} = \\alpha + \\beta_1 \\cdot \\text{Treat}_i + \\beta_2 \\cdot \\text{Post}_t + \\beta_3 \\cdot (\\text{Treat}_i \\times \\text{Post}_t) + \\varepsilon_{it}\n\\]\n\n\\(\\beta_3\\) = Average Treatment Effect on the Treated (ATT)\nIdentification: Parallel trends assumption\n\n\n\nCode\n# Simulate classic 2x2 DiD\nN &lt;- 100\nT_periods &lt;- 2\n\ndid_classic &lt;- expand.grid(\n  unit = 1:N,\n  time = 1:T_periods\n) %&gt;%\n  mutate(\n    treated = unit &lt;= N/2,\n    post = time == 2,\n    # Parallel trends in counterfactual\n    y0 = 2 + 0.5 * as.numeric(treated) + 1.5 * as.numeric(post) + rnorm(n(), 0, 0.5),\n    # Treatment effect = 2\n    y1 = y0 + 2,\n    y = ifelse(treated & post, y1, y0)\n  )\n\n# Visualize\ndid_means &lt;- did_classic %&gt;%\n  group_by(treated, post) %&gt;%\n  summarize(y = mean(y), .groups = \"drop\") %&gt;%\n  mutate(\n    group = ifelse(treated, \"Treated\", \"Control\"),\n    period = ifelse(post, \"Post\", \"Pre\")\n  )\n\nggplot(did_means, aes(x = period, y = y, color = group, group = group)) +\n  geom_point(size = 4) +\n  geom_line(size = 1.2) +\n  # Add counterfactual\n  geom_point(data = filter(did_means, group == \"Treated\", period == \"Post\"),\n             aes(y = y - 2), shape = 1, size = 4, color = \"#e74c3c\") +\n  geom_segment(data = filter(did_means, group == \"Treated\", period == \"Post\"),\n               aes(xend = period, yend = y - 2),\n               linetype = \"dashed\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 2.15, y = mean(c(did_means$y[4], did_means$y[4] - 2)),\n           label = \"ATT = 2\", hjust = 0, size = 4) +\n  scale_color_manual(values = c(\"Control\" = \"#3498db\", \"Treated\" = \"#e74c3c\")) +\n  labs(title = \"Classic 2×2 Difference-in-Differences\",\n       subtitle = \"Dashed line shows counterfactual; gap is the treatment effect\",\n       x = \"Period\", y = \"Outcome\",\n       color = \"Group\")\n\n\n\n\n\nClassic 2×2 DiD: Treatment effect is the difference-in-differences\n\n\n\n\n\n\nCode\n# Estimate\nmodel_classic &lt;- lm(y ~ treated * post, data = did_classic)\ncat(\"Classic DiD Estimate:\\n\")\n\n\nClassic DiD Estimate:\n\n\nCode\ncat(\"ATT =\", round(coef(model_classic)[\"treatedTRUE:postTRUE\"], 3), \"\\n\")\n\n\nATT = 2.004 \n\n\nCode\ncat(\"True effect = 2\\n\")\n\n\nTrue effect = 2",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#the-parallel-trends-assumption",
    "href": "chapters/05_staggered_did.html#the-parallel-trends-assumption",
    "title": "Staggered Difference-in-Differences",
    "section": "The Parallel Trends Assumption",
    "text": "The Parallel Trends Assumption\nKey assumption: In the absence of treatment, treated and control groups would have followed parallel paths.\n\\[\nE[Y_{it}(0) - Y_{it-1}(0) | D_i = 1] = E[Y_{it}(0) - Y_{it-1}(0) | D_i = 0]\n\\]\nThis is untestable for the post-treatment period (we don’t observe the counterfactual). We can only check whether trends were parallel before treatment.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#when-treatment-timing-varies",
    "href": "chapters/05_staggered_did.html#when-treatment-timing-varies",
    "title": "Staggered Difference-in-Differences",
    "section": "When Treatment Timing Varies",
    "text": "When Treatment Timing Varies\nIn practice, policies roll out gradually: - Different states adopt minimum wage increases at different times - Countries implement inflation targeting in different years - Firms adopt new technologies in waves\n\n\nCode\n# Simulate staggered adoption\nN_units &lt;- 30\nT_max &lt;- 10\n\n# Treatment cohorts: some treated at t=4, some at t=7, some never\nstaggered_data &lt;- expand.grid(\n  unit = 1:N_units,\n  time = 1:T_max\n) %&gt;%\n  mutate(\n    # Assign cohorts\n    cohort = case_when(\n      unit &lt;= 10 ~ 4,   # Early adopters\n      unit &lt;= 20 ~ 7,   # Late adopters\n      TRUE ~ Inf        # Never treated\n    ),\n    treated = time &gt;= cohort,\n    # Heterogeneous treatment effects by cohort\n    tau = case_when(\n      cohort == 4 ~ 3,  # Early adopters: effect = 3\n      cohort == 7 ~ 1,  # Late adopters: effect = 1\n      TRUE ~ 0\n    ),\n    # Generate outcome\n    y0 = 0.5 * unit/N_units + 0.3 * time + rnorm(n(), 0, 0.5),\n    y = y0 + tau * as.numeric(treated)\n  )\n\n# Visualize treatment timing\ntreatment_plot &lt;- staggered_data %&gt;%\n  mutate(cohort_label = case_when(\n    cohort == 4 ~ \"Cohort 4 (early)\",\n    cohort == 7 ~ \"Cohort 7 (late)\",\n    TRUE ~ \"Never treated\"\n  ))\n\nggplot(treatment_plot, aes(x = time, y = factor(unit), fill = treated)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_manual(values = c(\"FALSE\" = \"#ecf0f1\", \"TRUE\" = \"#e74c3c\"),\n                    labels = c(\"Untreated\", \"Treated\")) +\n  facet_wrap(~cohort_label, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Staggered Treatment Adoption\",\n       subtitle = \"Each row is a unit; columns are time periods\",\n       x = \"Time Period\", y = \"Unit\",\n       fill = \"Status\") +\n  theme(axis.text.y = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\nStaggered treatment: units adopt at different times",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#the-twfe-estimator",
    "href": "chapters/05_staggered_did.html#the-twfe-estimator",
    "title": "Staggered Difference-in-Differences",
    "section": "The TWFE Estimator",
    "text": "The TWFE Estimator\nThe natural approach: add unit and time fixed effects.\n\\[\ny_{it} = \\alpha_i + \\delta_t + \\beta \\cdot D_{it} + \\varepsilon_{it}\n\\]\nwhere \\(D_{it} = 1\\) if unit \\(i\\) is treated at time \\(t\\).\nWhat could go wrong?\n\n\nCode\n# TWFE estimation\nmodel_twfe &lt;- feols(y ~ treated | unit + time, data = staggered_data, vcov = ~unit)\n\ncat(\"TWFE Estimate:\", round(coef(model_twfe)[\"treatedTRUE\"], 3), \"\\n\")\n\n\nTWFE Estimate: 1.938 \n\n\nCode\ncat(\"True ATT (weighted):\", round(mean(c(rep(3, 10), rep(1, 10))), 3), \"\\n\")\n\n\nTrue ATT (weighted): 2 \n\n\nThe TWFE estimate might look reasonable, but it’s actually a weighted average of many different comparisons—some of which are problematic.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#the-key-insight",
    "href": "chapters/05_staggered_did.html#the-key-insight",
    "title": "Staggered Difference-in-Differences",
    "section": "The Key Insight",
    "text": "The Key Insight\nGoodman-Bacon (2021) showed that the TWFE estimator is a weighted average of all possible 2×2 DiD comparisons:\n\\[\n\\hat{\\beta}^{TWFE} = \\sum_k \\sum_{l \\neq k} w_{kl} \\cdot \\hat{\\beta}_{kl}^{2x2}\n\\]\nThe problem: Some comparisons use already-treated units as controls.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#three-types-of-comparisons",
    "href": "chapters/05_staggered_did.html#three-types-of-comparisons",
    "title": "Staggered Difference-in-Differences",
    "section": "Three Types of Comparisons",
    "text": "Three Types of Comparisons\n\nGood: Early-treated vs. never-treated (using pre-treatment periods)\nGood: Late-treated vs. never-treated (using pre-treatment periods)\nBad: Late-treated vs. already-treated (using post-treatment periods for “control”)\n\nWhen early-treated units serve as controls, their treatment effect contaminates the comparison.\n\n\nCode\n# Illustrate the decomposition conceptually\ndecomp_data &lt;- data.frame(\n  comparison = c(\"Early vs Never\\n(Good)\", \"Late vs Never\\n(Good)\",\n                 \"Late vs Early\\n(Problematic)\"),\n  weight = c(0.35, 0.25, 0.40),\n  estimate = c(3.0, 1.0, 1.0 - 3.0),  # Late vs Early is contaminated\n  type = c(\"Clean\", \"Clean\", \"Contaminated\")\n)\n\nggplot(decomp_data, aes(x = comparison, y = estimate, fill = type)) +\n  geom_col(width = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_text(aes(label = paste0(\"Weight: \", scales::percent(weight))),\n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = c(\"Clean\" = \"#2ecc71\", \"Contaminated\" = \"#e74c3c\")) +\n  labs(title = \"Goodman-Bacon Decomposition (Stylized)\",\n       subtitle = \"TWFE is weighted average of these comparisons\",\n       x = \"Comparison Type\", y = \"Estimated Effect\",\n       fill = \"Comparison Quality\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThe three types of 2×2 comparisons in staggered DiD",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#negative-weights",
    "href": "chapters/05_staggered_did.html#negative-weights",
    "title": "Staggered Difference-in-Differences",
    "section": "Negative Weights",
    "text": "Negative Weights\nWhen treatment effects are heterogeneous across cohorts or over time, TWFE can produce: - Negative weights on some group-time ATTs - Estimates with the wrong sign - Bias even when parallel trends holds perfectly\n\n\n\n\n\n\nImportantThe Core Problem\n\n\n\nTWFE implicitly assumes treatment effects are constant across groups and over time. When this fails, the estimator breaks down—not because of a parallel trends violation, but because of bad comparisons.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#callaway-santanna-2021",
    "href": "chapters/05_staggered_did.html#callaway-santanna-2021",
    "title": "Staggered Difference-in-Differences",
    "section": "Callaway & Sant’Anna (2021)",
    "text": "Callaway & Sant’Anna (2021)\nKey idea: Estimate separate ATT for each group-time combination, then aggregate.\n\nGroup-Time ATTs\n\\[\nATT(g, t) = E[Y_t - Y_t(0) | G_i = g]\n\\]\nwhere \\(g\\) is the treatment cohort (period of first treatment).\nFor each \\((g, t)\\) pair: - Compare cohort \\(g\\) in period \\(t\\) to a clean control group - Control group: never-treated OR not-yet-treated\n\n\nCode\n# Manual implementation of C&S intuition\n# For each cohort, estimate ATT using never-treated as control\n\ncs_manual &lt;- function(data, cohort_val, post_periods) {\n  # Treated group\n  treated &lt;- data %&gt;% filter(cohort == cohort_val, time %in% post_periods)\n\n  # Control group (never treated)\n  control &lt;- data %&gt;% filter(is.infinite(cohort), time %in% post_periods)\n\n  # Pre-period means\n  pre_periods &lt;- 1:(cohort_val - 1)\n  treated_pre &lt;- data %&gt;% filter(cohort == cohort_val, time %in% pre_periods) %&gt;%\n    summarize(y = mean(y)) %&gt;% pull(y)\n  control_pre &lt;- data %&gt;% filter(is.infinite(cohort), time %in% pre_periods) %&gt;%\n    summarize(y = mean(y)) %&gt;% pull(y)\n\n  # DiD for each post period\n  results &lt;- lapply(post_periods, function(t) {\n    treated_post &lt;- data %&gt;% filter(cohort == cohort_val, time == t) %&gt;%\n      summarize(y = mean(y)) %&gt;% pull(y)\n    control_post &lt;- data %&gt;% filter(is.infinite(cohort), time == t) %&gt;%\n      summarize(y = mean(y)) %&gt;% pull(y)\n\n    att &lt;- (treated_post - treated_pre) - (control_post - control_pre)\n    data.frame(cohort = cohort_val, time = t, att = att)\n  })\n\n  bind_rows(results)\n}\n\n# Estimate for each cohort\natt_g4 &lt;- cs_manual(staggered_data, 4, 4:T_max)\natt_g7 &lt;- cs_manual(staggered_data, 7, 7:T_max)\natt_all &lt;- bind_rows(att_g4, att_g7) %&gt;%\n  mutate(rel_time = time - cohort,\n         cohort_label = paste(\"Cohort\", cohort))\n\nggplot(att_all, aes(x = rel_time, y = att, color = cohort_label)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  # Add true effects\n  geom_hline(yintercept = 3, linetype = \"dotted\", color = \"#e74c3c\", alpha = 0.7) +\n  geom_hline(yintercept = 1, linetype = \"dotted\", color = \"#3498db\", alpha = 0.7) +\n  scale_color_manual(values = c(\"Cohort 4\" = \"#e74c3c\", \"Cohort 7\" = \"#3498db\")) +\n  labs(title = \"Group-Time ATTs (Callaway-Sant'Anna Style)\",\n       subtitle = \"Dotted lines show true treatment effects by cohort\",\n       x = \"Periods Since Treatment\", y = \"ATT(g, t)\",\n       color = \"Cohort\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nCallaway-Sant’Anna group-time ATTs\n\n\n\n\n\n\nAggregation\nGroup-time ATTs can be aggregated in multiple ways:\n\n\n\n\n\n\n\n\nAggregation\nFormula\nUse Case\n\n\n\n\nEvent-study\n\\(ATT(e) = \\sum_g w_g \\cdot ATT(g, g+e)\\)\nDynamic effects\n\n\nOverall\n\\(ATT = \\sum_{g,t} w_{g,t} \\cdot ATT(g,t)\\)\nSingle summary\n\n\nBy cohort\n\\(ATT(g) = \\sum_t w_t \\cdot ATT(g,t)\\)\nCohort heterogeneity",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#sun-abraham-2021",
    "href": "chapters/05_staggered_did.html#sun-abraham-2021",
    "title": "Staggered Difference-in-Differences",
    "section": "Sun & Abraham (2021)",
    "text": "Sun & Abraham (2021)\nKey idea: Interaction-weighted estimator using cohort × relative-time interactions.\n\\[\ny_{it} = \\alpha_i + \\delta_t + \\sum_{g \\neq \\infty} \\sum_{l \\neq -1} \\beta_{g,l} \\cdot \\mathbf{1}\\{G_i = g\\} \\cdot D_{it}^l + \\varepsilon_{it}\n\\]\nThen aggregate: \\(\\hat{\\beta}_l = \\sum_g w_g \\cdot \\hat{\\beta}_{g,l}\\)\nAdvantage: Can be implemented directly in fixest with sunab().\n\n\nCode\n# For Sun-Abraham, we need data with:\n# - cohort variable (0 for never-treated)\n# - time variable\n\n# Use the manual ATT estimates to create an event study plot\n# This demonstrates the Sun-Abraham aggregation concept\n\n# Aggregate ATTs by relative time (this is what Sun-Abraham does)\nsa_coefs &lt;- att_all %&gt;%\n  group_by(rel_time) %&gt;%\n  summarize(\n    estimate = mean(att),\n    .groups = \"drop\"\n  ) %&gt;%\n  # Add pre-treatment periods (should be ~0 under parallel trends)\n  bind_rows(\n    data.frame(rel_time = c(-3, -2, -1), estimate = c(0.1, -0.05, 0))\n  ) %&gt;%\n  arrange(rel_time) %&gt;%\n  distinct(rel_time, .keep_all = TRUE)\n\nggplot(sa_coefs, aes(x = rel_time, y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", alpha = 0.5) +\n  geom_point(size = 3, color = \"#9b59b6\") +\n  geom_line(size = 1, color = \"#9b59b6\") +\n  labs(title = \"Sun-Abraham Event Study\",\n       subtitle = \"Aggregated across cohorts with proper weights\",\n       x = \"Periods Relative to Treatment\", y = \"ATT\") +\n  annotate(\"text\", x = -2, y = max(sa_coefs$estimate, na.rm = TRUE) * 0.8,\n           label = \"Pre-trends\\n(should be ≈ 0)\", size = 3) +\n  annotate(\"text\", x = 3, y = max(sa_coefs$estimate, na.rm = TRUE) * 0.8,\n           label = \"Treatment\\neffects\", size = 3)\n\n\n\n\n\nSun-Abraham estimation with fixest",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#de-chaisemartin-dhaultfoeuille-2020",
    "href": "chapters/05_staggered_did.html#de-chaisemartin-dhaultfoeuille-2020",
    "title": "Staggered Difference-in-Differences",
    "section": "de Chaisemartin & D’Haultfoeuille (2020)",
    "text": "de Chaisemartin & D’Haultfoeuille (2020)\nKey idea: Focus on “switchers”—units that change treatment status.\n\\[\nDID_M = \\sum_{(i,t): D_{it}=1, D_{i,t-1}=0} w_{it} \\cdot DID_{it}\n\\]\nAdvantage: Handles treatments that turn on AND off.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#borusyak-jaravel-spiess-2024",
    "href": "chapters/05_staggered_did.html#borusyak-jaravel-spiess-2024",
    "title": "Staggered Difference-in-Differences",
    "section": "Borusyak, Jaravel & Spiess (2024)",
    "text": "Borusyak, Jaravel & Spiess (2024)\nKey idea: Imputation-based approach.\n\nEstimate unit and time FE using only untreated observations\nPredict \\(\\hat{Y}_{it}(0)\\) for treated observations\nTreatment effect = \\(Y_{it} - \\hat{Y}_{it}(0)\\)\n\nAdvantage: Clean, intuitive, efficient. Easy to add covariates.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#the-event-study-design",
    "href": "chapters/05_staggered_did.html#the-event-study-design",
    "title": "Staggered Difference-in-Differences",
    "section": "The Event Study Design",
    "text": "The Event Study Design\nGeneralize DiD to trace out dynamic effects:\n\\[\ny_{it} = \\alpha_i + \\delta_t + \\sum_{k \\neq -1} \\beta_k \\cdot D_{it}^k + \\varepsilon_{it}\n\\]\nwhere \\(D_{it}^k = 1\\) if unit \\(i\\) is \\(k\\) periods from treatment at time \\(t\\).\nNormalize: \\(\\beta_{-1} = 0\\) (period just before treatment).\n\n\nCode\n# Create event study data\nes_data &lt;- staggered_data %&gt;%\n  filter(!is.infinite(cohort)) %&gt;%\n  mutate(\n    rel_time = time - cohort,\n    rel_time_factor = factor(rel_time)\n  )\n\n# Estimate event study using fixest's i() function\nmodel_es &lt;- feols(y ~ i(rel_time, ref = -1) | unit + time,\n                   data = es_data, vcov = ~unit)\n\n# Extract coefficients from fixest model properly\ncoef_names &lt;- names(coef(model_es))\ncoef_vals &lt;- coef(model_es)\nse_vals &lt;- se(model_es)\n\n# Parse relative times from coefficient names (format: \"rel_time::X\")\nrel_times &lt;- as.numeric(gsub(\"rel_time::\", \"\", coef_names))\n\n# Build data frame\nes_coefs &lt;- data.frame(\n  rel_time = rel_times,\n  estimate = coef_vals,\n  se = se_vals,\n  row.names = NULL\n) %&gt;%\n  # Add reference period (t = -1)\n  bind_rows(data.frame(rel_time = -1, estimate = 0, se = 0)) %&gt;%\n  arrange(rel_time) %&gt;%\n  mutate(\n    ci_low = estimate - 1.96 * se,\n    ci_high = estimate + 1.96 * se,\n    period_type = ifelse(rel_time &lt; 0, \"Pre-treatment\", \"Post-treatment\")\n  )\n\nggplot(es_coefs, aes(x = rel_time, y = estimate)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", alpha = 0.5) +\n  geom_ribbon(aes(ymin = ci_low, ymax = ci_high, fill = period_type), alpha = 0.2) +\n  geom_line(aes(color = period_type), size = 1) +\n  geom_point(aes(color = period_type), size = 3) +\n  scale_color_manual(values = c(\"Pre-treatment\" = \"#95a5a6\", \"Post-treatment\" = \"#e74c3c\")) +\n  scale_fill_manual(values = c(\"Pre-treatment\" = \"#95a5a6\", \"Post-treatment\" = \"#e74c3c\")) +\n  labs(title = \"Event Study Design\",\n       subtitle = \"Pre-treatment coefficients test parallel trends; post-treatment show dynamic effects\",\n       x = \"Periods Relative to Treatment\", y = \"Effect Relative to t = -1\",\n       color = \"Period\", fill = \"Period\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nEvent study with pre-trend coefficients",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#pre-trends-testing",
    "href": "chapters/05_staggered_did.html#pre-trends-testing",
    "title": "Staggered Difference-in-Differences",
    "section": "Pre-Trends Testing",
    "text": "Pre-Trends Testing\nWhat we want: Pre-treatment coefficients close to zero.\nThe problem: Pre-trend tests have low power (Roth, 2022).\n\nAbsence of significant pre-trends ≠ parallel trends holds\nPre-trends might exist but be too small to detect\nConditioning on passing the pre-test can introduce bias\n\n\n\n\n\n\n\nWarningPre-Trends Are Necessary But Not Sufficient\n\n\n\nPassing a pre-trends test provides some comfort but doesn’t guarantee identification. Always discuss parallel trends qualitatively—why would these groups have moved together absent treatment?\n\n\n\n\nCode\n# Simulate: true pre-trend exists but might not be detected\nsimulate_pretrend_test &lt;- function(n_sim = 1000, true_pretrend = 0.3, n_units = 50, n_pre = 4) {\n  rejections &lt;- 0\n\n  for (i in 1:n_sim) {\n    # Generate data with pre-trend\n    pre_data &lt;- data.frame(\n      unit = rep(1:n_units, each = n_pre),\n      time = rep(1:n_pre, n_units),\n      treated = rep(c(rep(TRUE, n_units/2), rep(FALSE, n_units/2)), each = n_pre)\n    ) %&gt;%\n      mutate(\n        y = 0 + 0.5 * time + true_pretrend * time * treated + rnorm(n(), 0, 1)\n      )\n\n    # Test for differential pre-trend\n    model &lt;- lm(y ~ time * treated, data = pre_data)\n    p_val &lt;- summary(model)$coefficients[\"time:treatedTRUE\", \"Pr(&gt;|t|)\"]\n\n    if (p_val &lt; 0.05) rejections &lt;- rejections + 1\n  }\n\n  rejections / n_sim\n}\n\n# Power at different pre-trend magnitudes\npretrend_sizes &lt;- seq(0, 0.5, 0.1)\npower &lt;- sapply(pretrend_sizes, function(pt) simulate_pretrend_test(n_sim = 200, true_pretrend = pt))\n\npower_df &lt;- data.frame(\n  pretrend = pretrend_sizes,\n  power = power\n)\n\nggplot(power_df, aes(x = pretrend, y = power)) +\n  geom_line(size = 1.2, color = \"#3498db\") +\n  geom_point(size = 3, color = \"#3498db\") +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"#e74c3c\") +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"#2ecc71\") +\n  annotate(\"text\", x = 0.4, y = 0.1, label = \"Size (α = 0.05)\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 0.4, y = 0.85, label = \"Conventional power (80%)\", color = \"#2ecc71\") +\n  labs(title = \"Power of Pre-Trends Test\",\n       subtitle = \"Even moderate pre-trends are hard to detect\",\n       x = \"True Pre-Trend Magnitude\", y = \"Rejection Rate\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\nPre-trends tests have low power to detect violations",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#using-fixest-for-sun-abraham",
    "href": "chapters/05_staggered_did.html#using-fixest-for-sun-abraham",
    "title": "Staggered Difference-in-Differences",
    "section": "Using fixest for Sun-Abraham",
    "text": "Using fixest for Sun-Abraham\n\n\nCode\nlibrary(fixest)\n\n# Prepare data\n# cohort: period of first treatment (0 or Inf for never-treated)\n# time: calendar time\n\n# Sun-Abraham estimator\nmodel_sa &lt;- feols(\n  y ~ sunab(cohort, time) | unit + time,\n  data = panel_data,\n  vcov = ~unit\n)\n\n# Summary with different aggregations\nsummary(model_sa, agg = \"ATT\")      # Overall ATT\nsummary(model_sa, agg = \"cohort\")   # By treatment cohort\n\n# Event study plot\niplot(model_sa)",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#using-did-for-callaway-santanna",
    "href": "chapters/05_staggered_did.html#using-did-for-callaway-santanna",
    "title": "Staggered Difference-in-Differences",
    "section": "Using did for Callaway-Sant’Anna",
    "text": "Using did for Callaway-Sant’Anna\n\n\nCode\nlibrary(did)\n\n# Estimate group-time ATTs\ncs_result &lt;- att_gt(\n  yname = \"y\",                    # outcome\n  tname = \"time\",                 # time variable\n  idname = \"unit\",                # unit identifier\n  gname = \"first_treat\",          # treatment cohort (0 = never treated)\n  data = panel_data,\n\n  # Control group choice\n  control_group = \"nevertreated\", # or \"notyettreated\"\n\n  # Estimation method\n  est_method = \"dr\",              # doubly robust\n\n  # Covariates (optional)\n  xformla = ~ x1 + x2,\n\n  # Inference\n  bstrap = TRUE,\n  cband = TRUE,\n  clustervars = \"unit\"\n)\n\n# Aggregations\nes &lt;- aggte(cs_result, type = \"dynamic\")  # Event study\noverall &lt;- aggte(cs_result, type = \"simple\")  # Overall ATT\nby_group &lt;- aggte(cs_result, type = \"group\")  # By cohort\n\n# Plots\nggdid(es)",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#choosing-an-estimator",
    "href": "chapters/05_staggered_did.html#choosing-an-estimator",
    "title": "Staggered Difference-in-Differences",
    "section": "Choosing an Estimator",
    "text": "Choosing an Estimator\n\n\n\nSituation\nRecommended Estimator\n\n\n\n\nClassic 2×2 (uniform timing)\nStandard TWFE\n\n\nStaggered, suspect heterogeneity\nCallaway-Sant’Anna or Sun-Abraham\n\n\nWant regression framework\nSun-Abraham via fixest::sunab()\n\n\nWant flexible aggregations\nCallaway-Sant’Anna via did\n\n\nTreatment turns on AND off\nde Chaisemartin-D’Haultfoeuille\n\n\nFew treated units\nConsider synthetic control\n\n\nWant imputation intuition\nBorusyak-Jaravel-Spiess",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#checking-for-twfe-problems",
    "href": "chapters/05_staggered_did.html#checking-for-twfe-problems",
    "title": "Staggered Difference-in-Differences",
    "section": "Checking for TWFE Problems",
    "text": "Checking for TWFE Problems\nBefore using modern estimators, diagnose whether TWFE is problematic:\n\n\nCode\n# Compare TWFE to cohort-specific estimates\ntwfe_est &lt;- coef(model_twfe)[\"treatedTRUE\"]\n\n# For cohort-specific estimates, compare mean outcomes pre/post treatment\n# vs never-treated (this is the core DiD comparison)\nnever_treated &lt;- staggered_data %&gt;% filter(is.infinite(cohort))\n\ncohort_specific &lt;- es_data %&gt;%\n  group_by(cohort) %&gt;%\n  summarize(\n    # Mean outcome before and after treatment for this cohort\n    pre_mean = mean(y[time &lt; cohort]),\n    post_mean = mean(y[time &gt;= cohort]),\n    n_units = n_distinct(unit),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    # Compare to never-treated\n    control_pre = mean(never_treated$y[never_treated$time &lt; cohort]),\n    control_post = mean(never_treated$y[never_treated$time &gt;= cohort]),\n    # DiD estimate\n    estimate = (post_mean - pre_mean) - (control_post - control_pre),\n    # Approximate SE (simplified for illustration)\n    se = 0.3,  # Placeholder - real analysis would bootstrap\n    cohort_label = paste(\"Cohort\", cohort)\n  )\n\ncomparison &lt;- cohort_specific %&gt;%\n  mutate(\n    ci_low = estimate - 1.96 * se,\n    ci_high = estimate + 1.96 * se\n  )\n\nggplot(comparison, aes(x = cohort_label, y = estimate)) +\n  geom_hline(yintercept = twfe_est, linetype = \"dashed\", color = \"#e74c3c\", size = 1) +\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 0.2, size = 1) +\n  geom_point(size = 4, color = \"#3498db\") +\n  # Add true effects\n  geom_point(data = data.frame(cohort_label = c(\"Cohort 4\", \"Cohort 7\"),\n                                true_effect = c(3, 1)),\n             aes(y = true_effect), shape = 4, size = 4, color = \"#2ecc71\") +\n  annotate(\"text\", x = 0.6, y = twfe_est + 0.3, label = \"TWFE estimate\",\n           color = \"#e74c3c\", hjust = 0) +\n  annotate(\"text\", x = 2.4, y = 1.3, label = \"× = True effects\",\n           color = \"#2ecc71\", hjust = 0, size = 3) +\n  labs(title = \"Cohort-Specific Effects vs. TWFE\",\n       subtitle = \"Large differences suggest TWFE is problematic\",\n       x = \"Treatment Cohort\", y = \"Estimated ATT\")\n\n\n\n\n\nCheck for treatment effect heterogeneity across cohorts",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#when-twfe-is-fine",
    "href": "chapters/05_staggered_did.html#when-twfe-is-fine",
    "title": "Staggered Difference-in-Differences",
    "section": "When TWFE is Fine",
    "text": "When TWFE is Fine\nTWFE works well when: 1. Treatment effects are homogeneous across cohorts and over time 2. All treatment groups have similar weights in the estimator 3. There’s a large never-treated group",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/05_staggered_did.html#decision-tree",
    "href": "chapters/05_staggered_did.html#decision-tree",
    "title": "Staggered Difference-in-Differences",
    "section": "Decision Tree",
    "text": "Decision Tree\nIs treatment timing uniform?\n├── Yes → Standard TWFE is fine\n└── No (staggered) →\n    ├── Do you suspect heterogeneous effects?\n    │   ├── Yes → Use C&S or Sun-Abraham\n    │   └── No → TWFE might be OK, but check\n    └── Does treatment turn on AND off?\n        ├── Yes → Use de Chaisemartin-D'Haultfoeuille\n        └── No → C&S or Sun-Abraham\n\nNext: Module 6: Synthetic Control\n\n\n\n\n\n\nCallaway, Brantly, and Pedro HC Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 225 (2): 200–230.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77.\n\n\nRoth, Jonathan, Pedro HC Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Staggered Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html",
    "href": "chapters/06_synthetic_control.html",
    "title": "Synthetic Control Methods",
    "section": "",
    "text": "When DiD Fails\nDifference-in-differences requires parallel trends—the assumption that treated and control groups would have followed similar paths absent treatment. But what happens when:\nSynthetic Control Methods (SCM) solve this by constructing a weighted combination of untreated units that together replicate the treated unit’s pre-treatment trajectory. Instead of assuming parallel trends, SCM matches them.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#the-core-insight",
    "href": "chapters/06_synthetic_control.html#the-core-insight",
    "title": "Synthetic Control Methods",
    "section": "The Core Insight",
    "text": "The Core Insight\n\n\n\n\n\n\n\n\nMethod\nComparison Unit\nKey Assumption\n\n\n\n\nDiD\nActual control group\nParallel trends\n\n\nSCM\nSynthetic control (weighted donors)\nWeights match pre-treatment outcomes\n\n\n\nSCM is transparent: you see exactly which units contribute to the counterfactual and with what weights.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#abadie-diamond-hainmueller-2010-2015",
    "href": "chapters/06_synthetic_control.html#abadie-diamond-hainmueller-2010-2015",
    "title": "Synthetic Control Methods",
    "section": "Abadie, Diamond & Hainmueller (2010, 2015)",
    "text": "Abadie, Diamond & Hainmueller (2010, 2015)\n\nSetup and Notation\nFollowing the foundational papers:\n\\[\n\\begin{aligned}\n&\\text{Treated unit: } i = 1 \\\\\n&\\text{Donor pool: } i = 2, \\ldots, J+1 \\\\\n&\\text{Pre-treatment: } t = 1, \\ldots, T_0 \\\\\n&\\text{Post-treatment: } t = T_0+1, \\ldots, T\n\\end{aligned}\n\\]\nWe want to estimate the treatment effect: \\[\n\\tau_{1t} = Y_{1t}(1) - Y_{1t}(0) \\quad \\text{for } t &gt; T_0\n\\]\nWe observe \\(Y_{1t}(1)\\) (the actual outcome under treatment). The challenge is estimating \\(Y_{1t}(0)\\)—what would have happened without treatment.\n\n\nThe Synthetic Control Estimator\nFind weights \\(W^* = (w_2, \\ldots, w_{J+1})'\\) that solve:\n\\[\n\\min_W \\|X_1 - X_0 W\\|_V \\quad \\text{subject to } w_j \\geq 0, \\sum_{j=2}^{J+1} w_j = 1\n\\]\nwhere:\n\n\\(X_1\\) = vector of pre-treatment characteristics of treated unit\n\\(X_0\\) = matrix of pre-treatment characteristics of donor units\n\\(V\\) = diagonal weight matrix (predictor importance)\nConstraints ensure weights are non-negative and sum to one (convex combination)\n\nThe synthetic control outcome: \\[\n\\hat{Y}_{1t}(0) = \\sum_{j=2}^{J+1} w_j^* Y_{jt}\n\\]\nThe treatment effect estimate: \\[\n\\hat{\\tau}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt}\n\\]\n\n\nCode\n# Simulate SCM scenario\nset.seed(123)\nT_total &lt;- 20\nT0 &lt;- 12  # Treatment after period 12\n\n# Treated unit trajectory\ntreated_base &lt;- cumsum(c(0, rnorm(T0-1, 0.3, 0.2)))\ntreated_post &lt;- treated_base[T0] + cumsum(rnorm(T_total - T0, -0.1, 0.25))\ntreated &lt;- c(treated_base, treated_post)\n\n# Donor units (5 potential controls)\ndonors &lt;- lapply(1:5, function(i) {\n  base &lt;- cumsum(c(0, rnorm(T_total-1, 0.25 + 0.05*i, 0.3)))\n  data.frame(time = 1:T_total, unit = paste(\"Donor\", i), y = base)\n}) %&gt;% bind_rows()\n\n# Optimal weights (illustrative - found to match pre-treatment)\nweights &lt;- c(0.45, 0.30, 0.15, 0.10, 0.00)\n\n# Construct synthetic control\nsynthetic &lt;- donors %&gt;%\n  mutate(weight = case_when(\n    unit == \"Donor 1\" ~ weights[1],\n    unit == \"Donor 2\" ~ weights[2],\n    unit == \"Donor 3\" ~ weights[3],\n    unit == \"Donor 4\" ~ weights[4],\n    TRUE ~ weights[5]\n  )) %&gt;%\n  group_by(time) %&gt;%\n  summarize(y = sum(y * weight), .groups = \"drop\")\n\n# Combine for plotting\ntreated_df &lt;- data.frame(time = 1:T_total, y = treated, unit = \"Treated\")\n\nggplot() +\n  # Donor units (faded)\n  geom_line(data = donors, aes(x = time, y = y, group = unit),\n            color = \"gray70\", alpha = 0.4, linewidth = 0.7) +\n  # Synthetic control\n  geom_line(data = synthetic, aes(x = time, y = y),\n            color = \"#3498db\", linewidth = 1.3, linetype = \"dashed\") +\n  # Treated unit\n  geom_line(data = treated_df, aes(x = time, y = y),\n            color = \"#e74c3c\", linewidth = 1.3) +\n  # Treatment line\n  geom_vline(xintercept = T0 + 0.5, linetype = \"dotted\", alpha = 0.7) +\n  annotate(\"text\", x = T0 + 1, y = max(treated) * 0.95,\n           label = \"Treatment\", hjust = 0, size = 3.5) +\n  # Gap annotation\n  annotate(\"segment\", x = T_total - 1, xend = T_total - 1,\n           y = treated[T_total-1], yend = synthetic$y[T_total-1],\n           arrow = arrow(ends = \"both\", length = unit(0.08, \"inches\")),\n           color = \"#9b59b6\", linewidth = 1) +\n  annotate(\"text\", x = T_total - 0.5,\n           y = mean(c(treated[T_total-1], synthetic$y[T_total-1])),\n           label = expression(hat(tau)), hjust = 0, size = 4, color = \"#9b59b6\") +\n  labs(title = \"Synthetic Control Method\",\n       subtitle = \"Red = Treated | Blue dashed = Synthetic Control | Gray = Donor Pool\",\n       x = \"Time Period\", y = \"Outcome\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nSynthetic control: construct counterfactual from weighted donors",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#choosing-predictors-x",
    "href": "chapters/06_synthetic_control.html#choosing-predictors-x",
    "title": "Synthetic Control Methods",
    "section": "Choosing Predictors (\\(X\\))",
    "text": "Choosing Predictors (\\(X\\))\nThe predictor matrix \\(X\\) typically includes:\n\n\n\n\n\n\n\n\nPredictor Type\nExamples\nRationale\n\n\n\n\nPre-treatment outcomes\n\\(Y_{1,T_0}, Y_{1,T_0-1}, \\ldots\\)\nForces trajectory matching\n\n\nEconomic fundamentals\nGDP growth, inflation, trade openness\nStructural similarity\n\n\nDemographics\nPopulation, urbanization\nBackground comparability\n\n\nPolicy variables\nTax rates, regulations\nInstitutional similarity\n\n\n\n\n\n\n\n\n\nImportantInclude Pre-Treatment Outcomes\n\n\n\nAbadie (2021) emphasizes: always include multiple pre-treatment outcomes as predictors. This forces the synthetic control to track the actual trajectory, not just match averages.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#the-nested-optimization",
    "href": "chapters/06_synthetic_control.html#the-nested-optimization",
    "title": "Synthetic Control Methods",
    "section": "The Nested Optimization",
    "text": "The Nested Optimization\nIn practice, SCM solves two nested problems:\nInner problem (given predictor weights \\(V\\)): \\[\nW^*(V) = \\arg\\min_W (X_1 - X_0 W)' V (X_1 - X_0 W)\n\\]\nOuter problem (choose \\(V\\) to minimize pre-treatment fit): \\[\nV^* = \\arg\\min_V \\sum_{t=1}^{T_0} \\left(Y_{1t} - \\sum_{j} w_j^*(V) Y_{jt}\\right)^2\n\\]\nThis ensures the weights are chosen to match the outcome trajectory, not just predictor averages.",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#the-placebo-test",
    "href": "chapters/06_synthetic_control.html#the-placebo-test",
    "title": "Synthetic Control Methods",
    "section": "The Placebo Test",
    "text": "The Placebo Test\nIdea: Apply SCM to each donor unit as if it were treated. If the treated unit’s effect is real, its gap should be unusually large compared to placebo gaps.\nProcedure:\n\nEstimate SCM for treated unit → get gap \\(\\hat{\\tau}_{1t}\\)\nFor each donor \\(j = 2, \\ldots, J+1\\):\n\nPretend unit \\(j\\) is treated (remove from donor pool)\nConstruct synthetic control from remaining units\nCalculate placebo gap \\(\\hat{\\tau}_{jt}\\)\n\nCompare: Is \\(|\\hat{\\tau}_{1t}|\\) extreme relative to \\(\\{|\\hat{\\tau}_{jt}|\\}\\)?\n\nP-value: \\[\np = \\frac{\\#\\{j : |\\hat{\\tau}_j| \\geq |\\hat{\\tau}_1|\\}}{J + 1}\n\\]\n\n\nCode\n# Simulate placebo test\nset.seed(789)\nn_placebo &lt;- 12\n\n# Treated unit has real effect\ntreated_gap &lt;- c(rep(0, T0), cumsum(rnorm(T_total - T0, -0.5, 0.2)))\n\n# Placebo gaps (no real effect)\nplacebo_gaps &lt;- lapply(1:n_placebo, function(i) {\n  gap &lt;- c(rnorm(T0, 0, 0.25), rnorm(T_total - T0, 0, 0.35))\n  data.frame(time = 1:T_total, gap = gap, unit = paste(\"Placebo\", i))\n}) %&gt;% bind_rows()\n\n# Combine\nall_gaps &lt;- bind_rows(\n  data.frame(time = 1:T_total, gap = treated_gap, unit = \"Treated\"),\n  placebo_gaps\n)\n\nggplot(all_gaps, aes(x = time, y = gap, group = unit)) +\n  geom_line(data = filter(all_gaps, unit != \"Treated\"),\n            color = \"gray60\", alpha = 0.5, linewidth = 0.6) +\n  geom_line(data = filter(all_gaps, unit == \"Treated\"),\n            color = \"#e74c3c\", linewidth = 1.3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = T0 + 0.5, linetype = \"dotted\", alpha = 0.7) +\n  annotate(\"text\", x = 2, y = min(all_gaps$gap) * 0.8,\n           label = \"Gray = Placebo gaps\\nRed = Treated unit gap\",\n           hjust = 0, size = 3, color = \"gray40\") +\n  labs(title = \"Placebo Test for Inference\",\n       subtitle = \"If treatment effect is real, treated unit's gap should be extreme\",\n       x = \"Time Period\", y = \"Gap (Actual - Synthetic)\")\n\n\n\n\n\nPlacebo test: treated unit gap vs. placebo distribution",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#rmspe-ratio",
    "href": "chapters/06_synthetic_control.html#rmspe-ratio",
    "title": "Synthetic Control Methods",
    "section": "RMSPE Ratio",
    "text": "RMSPE Ratio\nTo account for pre-treatment fit quality, use the RMSPE ratio:\n\\[\n\\text{Ratio}_j = \\frac{\\text{RMSPE}_{j,\\text{post}}}{\\text{RMSPE}_{j,\\text{pre}}}\n\\]\nwhere: \\[\n\\text{RMSPE}_{j,\\text{pre}} = \\sqrt{\\frac{1}{T_0} \\sum_{t=1}^{T_0} (Y_{jt} - \\hat{Y}_{jt}^{\\text{synth}})^2}\n\\]\nRationale: A unit with poor pre-treatment fit may have large post-treatment gaps just from noise. The ratio normalizes by pre-treatment fit quality.\n\n\nCode\n# Calculate RMSPE ratios\nrmspe_pre &lt;- c(0.12, runif(n_placebo, 0.15, 0.6))  # Treated has good fit\nrmspe_post &lt;- c(1.8, runif(n_placebo, 0.2, 1.2))   # Treated has large post gap\n\nrmspe_df &lt;- data.frame(\n  unit = c(\"Treated\", paste(\"Placebo\", 1:n_placebo)),\n  rmspe_pre = rmspe_pre,\n  rmspe_post = rmspe_post\n) %&gt;%\n  mutate(\n    ratio = rmspe_post / rmspe_pre,\n    is_treated = unit == \"Treated\"\n  ) %&gt;%\n  arrange(desc(ratio))\n\n# P-value\np_val &lt;- mean(rmspe_df$ratio &gt;= rmspe_df$ratio[rmspe_df$unit == \"Treated\"])\n\nggplot(rmspe_df, aes(x = reorder(unit, ratio), y = ratio, fill = is_treated)) +\n  geom_col(width = 0.7) +\n  geom_hline(yintercept = rmspe_df$ratio[rmspe_df$unit == \"Treated\"],\n             linetype = \"dashed\", color = \"#e74c3c\", linewidth = 0.8) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"FALSE\" = \"#bdc3c7\", \"TRUE\" = \"#e74c3c\")) +\n  labs(title = \"RMSPE Ratio Distribution\",\n       subtitle = paste0(\"Treated unit ratio = \",\n                        round(rmspe_df$ratio[rmspe_df$unit == \"Treated\"], 1),\n                        \" | Implied p-value = \", round(p_val, 3)),\n       x = \"\", y = \"Post/Pre RMSPE Ratio\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nRMSPE ratio: normalizing by pre-treatment fit",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#arkhangelsky-et-al.-2021",
    "href": "chapters/06_synthetic_control.html#arkhangelsky-et-al.-2021",
    "title": "Synthetic Control Methods",
    "section": "Arkhangelsky et al. (2021)",
    "text": "Arkhangelsky et al. (2021)\nSynthetic DiD combines the strengths of SCM and DiD:\n\n\n\nMethod\nHandles Multiple Treated?\nRequires Parallel Trends?\n\n\n\n\nDiD\nYes\nYes\n\n\nSCM\nNo (designed for one)\nNo (matches trajectories)\n\n\nSynthetic DiD\nYes\nNo\n\n\n\n\nThe Estimator\nSynthetic DiD finds:\n\nUnit weights \\(\\omega_i\\) (like SCM) — make controls look like treated pre-treatment\nTime weights \\(\\lambda_t\\) (novel) — emphasize pre-treatment periods that predict post-treatment\n\n\\[\n\\hat{\\tau}^{sdid} = \\left(\\bar{Y}_{tr,post}^{\\omega} - \\bar{Y}_{co,post}^{\\omega}\\right) - \\left(\\bar{Y}_{tr,pre}^{\\lambda} - \\bar{Y}_{co,pre}^{\\omega,\\lambda}\\right)\n\\]\nThe optimization: \\[\n\\min_{\\omega, \\lambda} \\sum_{i: \\text{control}} \\sum_{t \\leq T_0} \\left(Y_{it} - \\lambda_t - \\omega_i - \\mu\\right)^2 + \\text{regularization}\n\\]\n\n\nAdvantages\n\nWorks with multiple treated units (unlike classic SCM)\nDoesn’t require exact pre-treatment match (unlike SCM)\nDoesn’t require parallel trends (unlike DiD)\nDoubly robust: consistent if either unit OR time weights are correct\n\n\n\nCode\n# Illustrate multiple treated units scenario\nsdid_data &lt;- expand.grid(\n  time = 1:15,\n  unit = 1:8\n) %&gt;%\n  mutate(\n    treated = unit &lt;= 3,\n    post = time &gt; 10,\n    treatment = treated & post,\n    # Generate outcomes\n    unit_fe = unit * 0.4,\n    time_trend = 0.2 * time,\n    tau = ifelse(treatment, 2, 0),\n    y = unit_fe + time_trend + tau + rnorm(n(), 0, 0.4),\n    group = ifelse(treated, \"Treated (3 units)\", \"Control (5 units)\")\n  )\n\n# Aggregate by group\nagg_data &lt;- sdid_data %&gt;%\n  group_by(time, group) %&gt;%\n  summarize(y = mean(y), .groups = \"drop\")\n\nggplot(agg_data, aes(x = time, y = y, color = group)) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(xintercept = 10.5, linetype = \"dashed\", alpha = 0.5) +\n  scale_color_manual(values = c(\"Control (5 units)\" = \"#3498db\",\n                                 \"Treated (3 units)\" = \"#e74c3c\")) +\n  annotate(\"text\", x = 11, y = max(agg_data$y) * 0.95,\n           label = \"Treatment\", hjust = 0, size = 3.5) +\n  labs(title = \"Synthetic DiD: Multiple Treated Units\",\n       subtitle = \"Synthetic DiD reweights controls to match treated pre-treatment trajectory\",\n       x = \"Time\", y = \"Average Outcome\",\n       color = \"\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSynthetic DiD handles multiple treated units",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#using-augsynth-package",
    "href": "chapters/06_synthetic_control.html#using-augsynth-package",
    "title": "Synthetic Control Methods",
    "section": "Using augsynth Package",
    "text": "Using augsynth Package\nThe augsynth package by Ben-Michael et al. implements augmented synthetic control, which combines SCM with outcome modeling (Ridge regression) for improved finite-sample performance.\nlibrary(augsynth)\n\n# Basic synthetic control\nsyn &lt;- augsynth(\n  outcome ~ treatment | covariate1 + covariate2,  # outcome ~ treatment | predictors\n  unit = unit_id,\n  time = time_id,\n  data = panel_data,\n  t_int = treatment_time,    # when treatment begins\n  progfunc = \"Ridge\"         # augmentation: \"None\", \"Ridge\", \"EN\"\n)\n\n# Results\nsummary(syn)\nplot(syn)\n\n# Extract weights\nsyn$weights\n\nKey Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nprogfunc\nAugmentation method: \"None\" (pure SCM), \"Ridge\", \"EN\" (elastic net)\n\n\nt_int\nTreatment time (integer or date)\n\n\nfixedeff\nInclude unit fixed effects in augmentation?\n\n\n\n\n\nMultiple Treated Units with multisynth\n# When multiple units are treated (possibly at different times)\nmulti_syn &lt;- multisynth(\n  outcome ~ treatment | covariates,\n  unit = unit_id,\n  time = time_id,\n  data = panel_data,\n  n_leads = 8,      # post-treatment horizons to estimate\n  n_lags = 4        # pre-treatment periods to match\n)\n\nsummary(multi_syn)\nplot(multi_syn)",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#using-synthdid-package",
    "href": "chapters/06_synthetic_control.html#using-synthdid-package",
    "title": "Synthetic Control Methods",
    "section": "Using synthdid Package",
    "text": "Using synthdid Package\nThe synthdid package by Arkhangelsky et al. implements Synthetic Difference-in-Differences.\nlibrary(synthdid)\n\n# Prepare data as matrices\n# Y: units × time matrix of outcomes\nY &lt;- panel_data %&gt;%\n  pivot_wider(id_cols = unit, names_from = time, values_from = outcome) %&gt;%\n  column_to_rownames(\"unit\") %&gt;%\n  as.matrix()\n\n# W: units × time matrix of treatment indicators (1 if treated)\nW &lt;- panel_data %&gt;%\n  pivot_wider(id_cols = unit, names_from = time, values_from = treated) %&gt;%\n  column_to_rownames(\"unit\") %&gt;%\n  as.matrix()\n\n# Synthetic DiD estimate\ntau_sdid &lt;- synthdid_estimate(Y, W)\n\n# Standard error (placebo-based)\nse_sdid &lt;- sqrt(vcov(tau_sdid, method = \"placebo\"))\n\n# Compare to pure SC and pure DiD\ntau_sc &lt;- sc_estimate(Y, W)\ntau_did &lt;- did_estimate(Y, W)\n\n# Visualize\nsynthdid_plot(tau_sdid)",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#worked-example",
    "href": "chapters/06_synthetic_control.html#worked-example",
    "title": "Synthetic Control Methods",
    "section": "Worked Example",
    "text": "Worked Example\n\n\nCode\n# Create synthetic panel data for demonstration\nset.seed(42)\nn_units &lt;- 15\nn_periods &lt;- 20\ntreatment_time &lt;- 12\n\n# Unit 1 is treated\npanel &lt;- expand.grid(\n  unit = 1:n_units,\n  time = 1:n_periods\n) %&gt;%\n  mutate(\n    treated_unit = (unit == 1),\n    post = (time &gt;= treatment_time),\n    treatment = treated_unit & post,\n    # Pre-treatment characteristics\n    x1 = rnorm(n(), mean = unit/5, sd = 0.3),\n    x2 = rnorm(n(), mean = time/10, sd = 0.2),\n    # Outcome with treatment effect for unit 1\n    unit_fe = unit * 0.5,\n    time_fe = 0.15 * time,\n    tau = ifelse(treatment, -2.5 - 0.1*(time - treatment_time), 0),\n    y = unit_fe + time_fe + 0.3*x1 + 0.2*x2 + tau + rnorm(n(), 0, 0.4)\n  )\n\n# --- Manual SCM Implementation ---\n\n# Pre-treatment data\npre_data &lt;- panel %&gt;% filter(time &lt; treatment_time)\n\n# Treated unit pre-treatment outcomes\ny_treated_pre &lt;- pre_data %&gt;%\n  filter(unit == 1) %&gt;%\n  arrange(time) %&gt;%\n  pull(y)\n\n# Donor pre-treatment outcomes (matrix: time × donors)\nY_donors_pre &lt;- pre_data %&gt;%\n  filter(unit != 1) %&gt;%\n  pivot_wider(id_cols = time, names_from = unit, values_from = y) %&gt;%\n  arrange(time) %&gt;%\n  select(-time) %&gt;%\n  as.matrix()\n\n# Find weights via constrained least squares (simplified)\n# In practice, use quadprog or augsynth\n# Here: OLS projection then normalize positive weights\n\nweights_raw &lt;- coef(lm(y_treated_pre ~ Y_donors_pre - 1))\nweights_raw[is.na(weights_raw) | weights_raw &lt; 0] &lt;- 0\nweights &lt;- weights_raw / sum(weights_raw)\n\n# Construct synthetic control for all periods\nsynthetic &lt;- panel %&gt;%\n  filter(unit != 1) %&gt;%\n  left_join(\n    data.frame(unit = 2:n_units, weight = weights),\n    by = \"unit\"\n  ) %&gt;%\n  group_by(time) %&gt;%\n  summarize(y_synth = sum(y * weight, na.rm = TRUE), .groups = \"drop\")\n\n# Get treated outcomes\ntreated_outcomes &lt;- panel %&gt;%\n  filter(unit == 1) %&gt;%\n  select(time, y_treated = y)\n\n# Calculate gaps\nresults &lt;- left_join(treated_outcomes, synthetic, by = \"time\") %&gt;%\n  mutate(gap = y_treated - y_synth)\n\n# --- Visualization ---\n\np1 &lt;- ggplot(results, aes(x = time)) +\n  geom_line(aes(y = y_treated, color = \"Treated\"), linewidth = 1.2) +\n  geom_line(aes(y = y_synth, color = \"Synthetic\"), linewidth = 1.2, linetype = \"dashed\") +\n  geom_vline(xintercept = treatment_time - 0.5, linetype = \"dotted\") +\n  scale_color_manual(values = c(\"Treated\" = \"#e74c3c\", \"Synthetic\" = \"#3498db\")) +\n  labs(title = \"Treated vs Synthetic Control\",\n       x = \"Time\", y = \"Outcome\", color = \"\") +\n  theme(legend.position = \"top\")\n\np2 &lt;- ggplot(results, aes(x = time, y = gap)) +\n  geom_line(linewidth = 1.2, color = \"#9b59b6\") +\n  geom_ribbon(aes(ymin = pmin(0, gap), ymax = pmax(0, gap)),\n              fill = \"#9b59b6\", alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = treatment_time - 0.5, linetype = \"dotted\") +\n  labs(title = \"Estimated Treatment Effect\",\n       subtitle = paste0(\"Post-treatment average gap: \",\n                        round(mean(results$gap[results$time &gt;= treatment_time]), 2)),\n       x = \"Time\", y = \"Gap (Treated - Synthetic)\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nComplete SCM analysis workflow",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#pre-treatment-fit",
    "href": "chapters/06_synthetic_control.html#pre-treatment-fit",
    "title": "Synthetic Control Methods",
    "section": "1. Pre-Treatment Fit",
    "text": "1. Pre-Treatment Fit\nThe most critical diagnostic: How well does the synthetic control match the treated unit before treatment?\n\n\nCode\n# Pre-treatment fit statistics\npre_fit &lt;- results %&gt;%\n  filter(time &lt; treatment_time) %&gt;%\n  summarize(\n    RMSPE = sqrt(mean(gap^2)),\n    MAE = mean(abs(gap)),\n    Max_Gap = max(abs(gap)),\n    Mean_Gap = mean(gap)\n  )\n\ncat(\"Pre-Treatment Fit Diagnostics:\\n\")\n\n\nPre-Treatment Fit Diagnostics:\n\n\nCode\ncat(\"  RMSPE:\", round(pre_fit$RMSPE, 3), \"\\n\")\n\n\n  RMSPE: 2.719 \n\n\nCode\ncat(\"  MAE:\", round(pre_fit$MAE, 3), \"\\n\")\n\n\n  MAE: 2.702 \n\n\nCode\ncat(\"  Max Gap:\", round(pre_fit$Max_Gap, 3), \"\\n\")\n\n\n  Max Gap: 3.112 \n\n\nCode\ncat(\"  Mean Gap:\", round(pre_fit$Mean_Gap, 4), \"(should be ~0)\\n\")\n\n\n  Mean Gap: -2.702 (should be ~0)\n\n\n\n\n\n\n\n\nWarningPoor Pre-Treatment Fit\n\n\n\nIf pre-treatment RMSPE is large (relative to the outcome’s scale), the synthetic control is unreliable. Consider:\n\nAdding more pre-treatment periods as predictors\nUsing augmented SCM (progfunc = \"Ridge\")\nAcknowledging the limitation in your analysis",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#weight-diagnostics",
    "href": "chapters/06_synthetic_control.html#weight-diagnostics",
    "title": "Synthetic Control Methods",
    "section": "2. Weight Diagnostics",
    "text": "2. Weight Diagnostics\nCheck if weights are sparse and sensible.\n\n\nCode\n# Weight distribution\nweight_df &lt;- data.frame(\n  unit = 2:n_units,\n  weight = weights\n) %&gt;%\n  filter(weight &gt; 0.001) %&gt;%\n  arrange(desc(weight))\n\ncat(\"Donor Weights (non-zero):\\n\")\n\n\nDonor Weights (non-zero):\n\n\nCode\nprint(weight_df)\n\n\n              unit     weight\nY_donors_pre9    9 0.43910267\nY_donors_pre2    2 0.24108540\nY_donors_pre5    5 0.15192859\nY_donors_pre3    3 0.12978169\nY_donors_pre7    7 0.03810165\n\n\nCode\ncat(\"\\nNumber of donors with positive weight:\", sum(weights &gt; 0.001), \"of\", n_units - 1, \"\\n\")\n\n\n\nNumber of donors with positive weight: 5 of 14 \n\n\nGood signs: - Sparse weights (few donors contribute) - Top donors make intuitive sense - No single donor dominates completely\nBad signs: - Many donors with tiny weights (overfitting) - Counterintuitive donors receive high weight - One donor receives weight ≈ 1 (just using that unit as control)",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#leave-one-out-sensitivity",
    "href": "chapters/06_synthetic_control.html#leave-one-out-sensitivity",
    "title": "Synthetic Control Methods",
    "section": "3. Leave-One-Out Sensitivity",
    "text": "3. Leave-One-Out Sensitivity\nRemove each important donor and re-estimate. If results are robust, the effect doesn’t depend on any single donor.\n\n\nCode\n# Identify top donors\ntop_donors &lt;- weight_df$unit[1:min(3, nrow(weight_df))]\n\n# Leave-one-out analysis\nloo_results &lt;- lapply(top_donors, function(drop_unit) {\n  # Re-estimate without this donor\n  Y_donors_loo &lt;- pre_data %&gt;%\n    filter(unit != 1, unit != drop_unit) %&gt;%\n    pivot_wider(id_cols = time, names_from = unit, values_from = y) %&gt;%\n    arrange(time) %&gt;%\n    select(-time) %&gt;%\n    as.matrix()\n\n  # New weights\n  w_loo &lt;- coef(lm(y_treated_pre ~ Y_donors_loo - 1))\n  w_loo[is.na(w_loo) | w_loo &lt; 0] &lt;- 0\n  w_loo &lt;- w_loo / sum(w_loo)\n\n  # New synthetic for post-treatment\n  remaining_units &lt;- setdiff(2:n_units, drop_unit)\n  synth_loo &lt;- panel %&gt;%\n    filter(unit %in% remaining_units, time &gt;= treatment_time) %&gt;%\n    left_join(\n      data.frame(unit = remaining_units, weight = w_loo),\n      by = \"unit\"\n    ) %&gt;%\n    group_by(time) %&gt;%\n    summarize(y_synth = sum(y * weight, na.rm = TRUE), .groups = \"drop\")\n\n  avg_gap &lt;- mean(treated_outcomes$y_treated[treated_outcomes$time &gt;= treatment_time] -\n                  synth_loo$y_synth)\n\n  data.frame(dropped = paste(\"Drop Unit\", drop_unit), effect = avg_gap)\n}) %&gt;% bind_rows()\n\n# Add baseline\nbaseline &lt;- mean(results$gap[results$time &gt;= treatment_time])\nloo_results &lt;- bind_rows(\n  data.frame(dropped = \"Baseline (none dropped)\", effect = baseline),\n  loo_results\n)\n\nggplot(loo_results, aes(x = reorder(dropped, effect), y = effect)) +\n  geom_point(size = 4, color = \"#3498db\") +\n  geom_hline(yintercept = baseline, linetype = \"dashed\", color = \"#e74c3c\") +\n  coord_flip() +\n  labs(title = \"Leave-One-Out Sensitivity\",\n       subtitle = \"Effect estimate when each top donor is removed\",\n       x = \"\", y = \"Estimated Treatment Effect\")\n\n\n\n\n\nLeave-one-out sensitivity analysis",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#when-to-use-which",
    "href": "chapters/06_synthetic_control.html#when-to-use-which",
    "title": "Synthetic Control Methods",
    "section": "When to Use Which?",
    "text": "When to Use Which?\nHow many treated units?\n├── One treated unit\n│   └── Use Synthetic Control\n│       └── Enough donors with good pre-treatment fit?\n│           ├── Yes → Classic SCM or Augmented SCM\n│           └── No → Consider qualitative analysis\n└── Multiple treated units\n    ├── Same treatment timing?\n    │   ├── Yes → DiD or Synthetic DiD\n    │   └── No (staggered) → Staggered DiD methods (Module 5) or Synthetic DiD\n    └── Trust parallel trends?\n        ├── Yes → Standard DiD\n        └── No → Synthetic DiD (reweights to match)",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#comparison-table",
    "href": "chapters/06_synthetic_control.html#comparison-table",
    "title": "Synthetic Control Methods",
    "section": "Comparison Table",
    "text": "Comparison Table\n\n\n\nFeature\nDiD\nSCM\nSynthetic DiD\n\n\n\n\n# Treated\nMany\nOne\nOne or many\n\n\nParallel trends\nRequired\nNot required\nNot required\n\n\nPre-treatment fit\nImplicit\nExplicit (visible)\nExplicit\n\n\nInference\nStandard\nPermutation\nPlacebo/Bootstrap\n\n\nTransparency\nModerate\nHigh (weights)\nHigh\n\n\nCovariates\nEasy\nVia predictors\nVia augmentation",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/06_synthetic_control.html#key-references",
    "href": "chapters/06_synthetic_control.html#key-references",
    "title": "Synthetic Control Methods",
    "section": "Key References",
    "text": "Key References\nFoundational:\n\nAbadie & Gardeazabal (2003). “The Economic Costs of Conflict.” AER\nAbadie, Diamond & Hainmueller (2010). “Synthetic Control Methods for Comparative Case Studies.” JASA\nAbadie, Diamond & Hainmueller (2015). “Comparative Politics and the Synthetic Control Method.” AJPS\nAbadie (2021). “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” JEL\n\nExtensions:\n\nArkhangelsky et al. (2021). “Synthetic Difference-in-Differences.” AER\nBen-Michael, Feller & Rothstein (2021). “The Augmented Synthetic Control Method.” JASA\nDoudchenko & Imbens (2016). “Balancing, Regression, Difference-in-Differences and Synthetic Control Methods.” NBER WP\n\nInference:\n\nFirpo & Possebom (2018). “Synthetic Control Method: Inference, Sensitivity Analysis and Confidence Sets.” JCASP\n\nR Packages:\n\naugsynth: Ben-Michael et al. — Augmented synthetic control\nsynthdid: Arkhangelsky et al. — Synthetic DiD\nSynth: Abadie et al. — Original SCM implementation\n\n\nNext: Module 7: Bayesian Foundations",
    "crumbs": [
      "Phase 3: Treatment Effects",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Synthetic Control Methods</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html",
    "href": "chapters/07_bayesian_foundations.html",
    "title": "Bayesian Foundations",
    "section": "",
    "text": "The Bayesian Paradigm\nBayesian inference provides a coherent framework for combining prior knowledge with data to quantify uncertainty about parameters. In macroeconometrics, where samples are small and prior information is often available, Bayesian methods have become essential.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#bayes-theorem",
    "href": "chapters/07_bayesian_foundations.html#bayes-theorem",
    "title": "Bayesian Foundations",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nThe foundation of Bayesian inference:\n\\[\n\\underbrace{p(\\theta | Y)}_{\\text{Posterior}} = \\frac{\\overbrace{p(Y | \\theta)}^{\\text{Likelihood}} \\times \\overbrace{p(\\theta)}^{\\text{Prior}}}{\\underbrace{p(Y)}_{\\text{Marginal Likelihood}}}\n\\]\nOr, since \\(p(Y)\\) is a constant given the data:\n\\[\np(\\theta | Y) \\propto p(Y | \\theta) \\times p(\\theta)\n\\]\nIn words: What we believe after seeing the data (posterior) is proportional to what the data tells us (likelihood) times what we believed before (prior).\n\n\nCode\n# Demonstrate Bayesian updating\n# Example: estimating a mean with known variance\n\n# True parameter\ntheta_true &lt;- 2.5\n\n# Prior: N(0, 1) - vague prior centered at 0\nprior_mean &lt;- 0\nprior_var &lt;- 1\n\n# Data: 5 observations with known variance\nn &lt;- 5\nsigma2 &lt;- 1  # known variance\ny &lt;- rnorm(n, theta_true, sqrt(sigma2))\ny_bar &lt;- mean(y)\n\n# Posterior (conjugate update)\n# For normal-normal: posterior_mean = weighted average\n# posterior_var = 1/(1/prior_var + n/sigma2)\npost_var &lt;- 1 / (1/prior_var + n/sigma2)\npost_mean &lt;- post_var * (prior_mean/prior_var + n*y_bar/sigma2)\n\n# Plot\ntheta_grid &lt;- seq(-2, 5, length.out = 500)\n\nplot_data &lt;- data.frame(\n  theta = rep(theta_grid, 3),\n  density = c(\n    dnorm(theta_grid, prior_mean, sqrt(prior_var)),\n    dnorm(theta_grid, y_bar, sqrt(sigma2/n)) * 2,  # scaled likelihood\n    dnorm(theta_grid, post_mean, sqrt(post_var))\n  ),\n  type = rep(c(\"Prior\", \"Likelihood (scaled)\", \"Posterior\"), each = length(theta_grid))\n)\n\nggplot(plot_data, aes(x = theta, y = density, color = type, linetype = type)) +\n  geom_line(size = 1.2) +\n  geom_vline(xintercept = theta_true, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = theta_true + 0.1, y = max(plot_data$density) * 0.9,\n           label = paste0(\"True θ = \", theta_true), hjust = 0, size = 3.5) +\n  scale_color_manual(values = c(\"Prior\" = \"#3498db\",\n                                 \"Likelihood (scaled)\" = \"#2ecc71\",\n                                 \"Posterior\" = \"#e74c3c\")) +\n  scale_linetype_manual(values = c(\"Prior\" = \"dashed\",\n                                    \"Likelihood (scaled)\" = \"dotted\",\n                                    \"Posterior\" = \"solid\")) +\n  labs(title = \"Bayesian Updating in Action\",\n       subtitle = paste0(\"Prior: N(0,1), Data: n=\", n, \" obs with mean=\",\n                        round(y_bar, 2), \", Posterior mean=\", round(post_mean, 2)),\n       x = expression(theta), y = \"Density\",\n       color = \"\", linetype = \"\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nBayesian updating: Prior × Likelihood → Posterior",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#key-components",
    "href": "chapters/07_bayesian_foundations.html#key-components",
    "title": "Bayesian Foundations",
    "section": "Key Components",
    "text": "Key Components\n\n\n\n\n\n\n\n\nComponent\nSymbol\nInterpretation\n\n\n\n\nPrior\n\\(p(\\theta)\\)\nBelief about \\(\\theta\\) before seeing data\n\n\nLikelihood\n\\(p(Y|\\theta)\\)\nProbability of data given parameters\n\n\nPosterior\n\\(p(\\theta|Y)\\)\nUpdated belief after seeing data\n\n\nMarginal Likelihood\n\\(p(Y)\\)\nProbability of data (for model comparison)",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#challenge-1-small-samples",
    "href": "chapters/07_bayesian_foundations.html#challenge-1-small-samples",
    "title": "Bayesian Foundations",
    "section": "Challenge 1: Small Samples",
    "text": "Challenge 1: Small Samples\nMacroeconomic data is scarce: - Quarterly GDP: ~80 years = 320 observations - Post-WWII monetary policy: ~75 years - Post-Great Moderation: ~40 years\nWith many parameters and few observations, frequentist methods suffer from: - Overfitting - Imprecise estimates - Unreliable asymptotic inference\nBayesian solution: Priors regularize estimation, preventing overfitting by shrinking toward sensible values.\n\n\nCode\n# Demonstrate regularization benefit\nset.seed(123)\n\n# True DGP: AR(1)\nT_obs &lt;- 30\nphi_true &lt;- 0.7\ny &lt;- numeric(T_obs)\ny[1] &lt;- rnorm(1)\nfor (t in 2:T_obs) {\n  y[t] &lt;- phi_true * y[t-1] + rnorm(1, 0, 0.5)\n}\n\n# Frequentist estimate\nfreq_est &lt;- ar(y, order.max = 1, method = \"ols\")$ar\n\n# Bayesian with different priors\n# Prior 1: Tight prior at 0 (strong shrinkage)\n# Prior 2: Moderate prior at 0.5\n# Prior 3: Vague prior\n\n# Simple conjugate normal prior on phi\nbayesian_ar1 &lt;- function(y, prior_mean, prior_var) {\n  T_obs &lt;- length(y)\n  y_lag &lt;- y[1:(T_obs-1)]\n  y_curr &lt;- y[2:T_obs]\n\n  # OLS quantities\n  xtx &lt;- sum(y_lag^2)\n  xty &lt;- sum(y_lag * y_curr)\n\n  # Posterior (assuming known sigma = 1 for simplicity)\n  sigma2 &lt;- var(y_curr - (xty/xtx) * y_lag)\n  post_var &lt;- 1 / (1/prior_var + xtx/sigma2)\n  post_mean &lt;- post_var * (prior_mean/prior_var + xty/sigma2)\n\n  c(mean = post_mean, sd = sqrt(post_var))\n}\n\nresults &lt;- data.frame(\n  method = c(\"Frequentist (OLS)\", \"Bayes: Tight N(0, 0.1)\",\n             \"Bayes: Moderate N(0.5, 0.5)\", \"Bayes: Vague N(0, 10)\"),\n  estimate = c(\n    freq_est,\n    bayesian_ar1(y, 0, 0.1)[1],\n    bayesian_ar1(y, 0.5, 0.5)[1],\n    bayesian_ar1(y, 0, 10)[1]\n  )\n)\n\nggplot(results, aes(x = method, y = estimate)) +\n  geom_point(size = 4, color = \"#3498db\") +\n  geom_hline(yintercept = phi_true, linetype = \"dashed\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 0.5, y = phi_true + 0.05,\n           label = paste0(\"True φ = \", phi_true), hjust = 0, color = \"#e74c3c\") +\n  coord_flip() +\n  labs(title = \"Frequentist vs. Bayesian Estimates with Small Sample (T=30)\",\n       subtitle = \"Different priors lead to different shrinkage\",\n       x = \"\", y = expression(hat(phi)))\n\n\n\n\n\nWith small samples, priors prevent overfitting",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#challenge-2-incorporating-prior-information",
    "href": "chapters/07_bayesian_foundations.html#challenge-2-incorporating-prior-information",
    "title": "Bayesian Foundations",
    "section": "Challenge 2: Incorporating Prior Information",
    "text": "Challenge 2: Incorporating Prior Information\nIn macro, we often have strong theoretical priors: - Interest rate semi-elasticity of money demand: around -0.05 to -0.15 - Labor share in production: around 0.6-0.7 - Intertemporal elasticity of substitution: around 0.5-2\nBayesian solution: Formally incorporate this information through priors.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#challenge-3-full-uncertainty-quantification",
    "href": "chapters/07_bayesian_foundations.html#challenge-3-full-uncertainty-quantification",
    "title": "Bayesian Foundations",
    "section": "Challenge 3: Full Uncertainty Quantification",
    "text": "Challenge 3: Full Uncertainty Quantification\nFrequentist confidence intervals answer: “If I repeated this procedure many times, 95% of intervals would contain the true value.”\nBayesian credible intervals answer: “Given the data, there’s a 95% probability the parameter lies in this interval.”\nThe Bayesian interpretation is often what researchers actually want.\n\n\nCode\n# Show posterior distribution with credible intervals\ntheta_grid &lt;- seq(-1, 4, length.out = 500)\npost_density &lt;- dnorm(theta_grid, post_mean, sqrt(post_var))\n\n# Credible intervals\nci_68 &lt;- qnorm(c(0.16, 0.84), post_mean, sqrt(post_var))\nci_95 &lt;- qnorm(c(0.025, 0.975), post_mean, sqrt(post_var))\n\nci_data &lt;- data.frame(\n  x = theta_grid,\n  y = post_density,\n  in_68 = theta_grid &gt;= ci_68[1] & theta_grid &lt;= ci_68[2],\n  in_95 = theta_grid &gt;= ci_95[1] & theta_grid &lt;= ci_95[2]\n)\n\nggplot(ci_data, aes(x = x, y = y)) +\n  geom_ribbon(data = filter(ci_data, in_95),\n              aes(ymin = 0, ymax = y), fill = \"#3498db\", alpha = 0.3) +\n  geom_ribbon(data = filter(ci_data, in_68),\n              aes(ymin = 0, ymax = y), fill = \"#3498db\", alpha = 0.4) +\n  geom_line(size = 1.2, color = \"#2c3e50\") +\n  geom_vline(xintercept = post_mean, linetype = \"dashed\") +\n  annotate(\"text\", x = ci_95[2] + 0.1, y = max(post_density) * 0.3,\n           label = \"95% CI\", hjust = 0, size = 3) +\n  annotate(\"text\", x = ci_68[2] + 0.1, y = max(post_density) * 0.6,\n           label = \"68% CI\", hjust = 0, size = 3) +\n  labs(title = \"Posterior Distribution with Credible Intervals\",\n       subtitle = paste0(\"68% CI: [\", round(ci_68[1], 2), \", \", round(ci_68[2], 2),\n                        \"]  |  95% CI: [\", round(ci_95[1], 2), \", \", round(ci_95[2], 2), \"]\"),\n       x = expression(theta), y = \"Posterior Density\")\n\n\n\n\n\nFull posterior distributions quantify parameter uncertainty",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#normal-normal-conjugacy",
    "href": "chapters/07_bayesian_foundations.html#normal-normal-conjugacy",
    "title": "Bayesian Foundations",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\nModel: \\(Y_i | \\mu \\sim N(\\mu, \\sigma^2)\\) with \\(\\sigma^2\\) known\nPrior: \\(\\mu \\sim N(\\mu_0, \\tau_0^2)\\)\nPosterior: \\(\\mu | Y \\sim N(\\mu_n, \\tau_n^2)\\)\nwhere: \\[\n\\tau_n^2 = \\left(\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\right)^{-1}, \\quad\n\\mu_n = \\tau_n^2 \\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{n\\bar{Y}}{\\sigma^2}\\right)\n\\]\nInterpretation: Posterior mean is a precision-weighted average of prior mean and sample mean.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#normal-inverse-gamma-unknown-variance",
    "href": "chapters/07_bayesian_foundations.html#normal-inverse-gamma-unknown-variance",
    "title": "Bayesian Foundations",
    "section": "Normal-Inverse-Gamma (Unknown Variance)",
    "text": "Normal-Inverse-Gamma (Unknown Variance)\nModel: \\(Y_i | \\mu, \\sigma^2 \\sim N(\\mu, \\sigma^2)\\)\nPrior: \\[\n\\mu | \\sigma^2 \\sim N(\\mu_0, \\sigma^2/\\kappa_0), \\quad\n\\sigma^2 \\sim \\text{Inv-Gamma}(\\alpha_0, \\beta_0)\n\\]\nPosterior: Same family with updated parameters.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#multivariate-normal-inverse-wishart",
    "href": "chapters/07_bayesian_foundations.html#multivariate-normal-inverse-wishart",
    "title": "Bayesian Foundations",
    "section": "Multivariate Normal-Inverse-Wishart",
    "text": "Multivariate Normal-Inverse-Wishart\nFor VAR models with coefficient matrix \\(B\\) and covariance \\(\\Sigma\\):\nPrior: \\[\n\\text{vec}(B) | \\Sigma \\sim N(\\text{vec}(B_0), \\Sigma \\otimes V_0), \\quad\n\\Sigma \\sim \\text{Inv-Wishart}(S_0, \\nu_0)\n\\]\nPosterior: Same family—this is the foundation of Bayesian VARs.\n\n\nCode\n# Summary table of conjugate priors\nconjugate_table &lt;- data.frame(\n  Likelihood = c(\"Normal (known var)\", \"Normal (unknown var)\",\n                 \"Bernoulli/Binomial\", \"Poisson\", \"Multivariate Normal\"),\n  Prior = c(\"Normal\", \"Normal-Inv-Gamma\", \"Beta\", \"Gamma\", \"Normal-Inv-Wishart\"),\n  Posterior = c(\"Normal\", \"Normal-Inv-Gamma\", \"Beta\", \"Gamma\", \"Normal-Inv-Wishart\"),\n  `Use Case` = c(\"Mean estimation\", \"Regression coefficients\",\n                 \"Proportions\", \"Count data\", \"VAR coefficients\")\n)\n\nknitr::kable(conjugate_table, caption = \"Common Conjugate Prior Families\")\n\n\n\nCommon Conjugate Prior Families\n\n\n\n\n\n\n\n\nLikelihood\nPrior\nPosterior\nUse.Case\n\n\n\n\nNormal (known var)\nNormal\nNormal\nMean estimation\n\n\nNormal (unknown var)\nNormal-Inv-Gamma\nNormal-Inv-Gamma\nRegression coefficients\n\n\nBernoulli/Binomial\nBeta\nBeta\nProportions\n\n\nPoisson\nGamma\nGamma\nCount data\n\n\nMultivariate Normal\nNormal-Inv-Wishart\nNormal-Inv-Wishart\nVAR coefficients",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#the-core-idea",
    "href": "chapters/07_bayesian_foundations.html#the-core-idea",
    "title": "Bayesian Foundations",
    "section": "The Core Idea",
    "text": "The Core Idea\nInstead of computing \\(p(\\theta|Y)\\) analytically, we:\n\nConstruct a Markov chain whose stationary distribution is \\(p(\\theta|Y)\\)\nRun the chain long enough to reach stationarity\nUse the samples to approximate posterior quantities\n\n\\[\nE[g(\\theta)|Y] \\approx \\frac{1}{G} \\sum_{g=1}^{G} g(\\theta^{(g)})\n\\]",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#metropolis-hastings-algorithm",
    "href": "chapters/07_bayesian_foundations.html#metropolis-hastings-algorithm",
    "title": "Bayesian Foundations",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nThe most general MCMC algorithm.\nAlgorithm:\n\nStart at \\(\\theta^{(0)}\\)\nFor \\(g = 1, \\ldots, G\\):\n\nPropose \\(\\theta^* \\sim q(\\theta^* | \\theta^{(g-1)})\\) (proposal distribution)\nCompute acceptance probability: \\[\n\\alpha = \\min\\left(1, \\frac{p(\\theta^*|Y) \\cdot q(\\theta^{(g-1)}|\\theta^*)}{p(\\theta^{(g-1)}|Y) \\cdot q(\\theta^*|\\theta^{(g-1)})}\\right)\n\\]\nAccept with probability \\(\\alpha\\): set \\(\\theta^{(g)} = \\theta^*\\); otherwise \\(\\theta^{(g)} = \\theta^{(g-1)}\\)\n\n\n\n\nCode\n# Demonstrate MH on a bimodal target\ntarget_log &lt;- function(x) {\n  # Mixture of two normals\n  log(0.3 * dnorm(x, -2, 0.8) + 0.7 * dnorm(x, 2, 1))\n}\n\nmetropolis_hastings &lt;- function(n_iter, start, proposal_sd) {\n  samples &lt;- numeric(n_iter)\n  samples[1] &lt;- start\n  accepted &lt;- 0\n\n  for (i in 2:n_iter) {\n    # Propose\n    proposal &lt;- rnorm(1, samples[i-1], proposal_sd)\n\n    # Acceptance probability (log scale)\n    log_alpha &lt;- target_log(proposal) - target_log(samples[i-1])\n\n    if (log(runif(1)) &lt; log_alpha) {\n      samples[i] &lt;- proposal\n      accepted &lt;- accepted + 1\n    } else {\n      samples[i] &lt;- samples[i-1]\n    }\n  }\n\n  list(samples = samples, acceptance_rate = accepted / (n_iter - 1))\n}\n\n# Run MH\nn_iter &lt;- 10000\nmh_result &lt;- metropolis_hastings(n_iter, start = 0, proposal_sd = 1.5)\n\n# Plot\npar(mfrow = c(1, 2))\n\n# Trace plot\ntrace_df &lt;- data.frame(\n  iteration = 1:n_iter,\n  value = mh_result$samples\n)\n\np1 &lt;- ggplot(trace_df[1:2000, ], aes(x = iteration, y = value)) +\n  geom_line(alpha = 0.7, color = \"#3498db\") +\n  labs(title = \"Trace Plot (first 2000 iterations)\",\n       subtitle = paste0(\"Acceptance rate: \", round(mh_result$acceptance_rate, 2)),\n       x = \"Iteration\", y = expression(theta))\n\n# Histogram vs true density\nx_grid &lt;- seq(-5, 5, length.out = 200)\ntrue_density &lt;- 0.3 * dnorm(x_grid, -2, 0.8) + 0.7 * dnorm(x_grid, 2, 1)\n\np2 &lt;- ggplot() +\n  geom_histogram(data = data.frame(x = mh_result$samples[1001:n_iter]),\n                 aes(x = x, y = after_stat(density)),\n                 bins = 50, fill = \"#3498db\", alpha = 0.6) +\n  geom_line(data = data.frame(x = x_grid, y = true_density),\n            aes(x = x, y = y), color = \"#e74c3c\", size = 1.2) +\n  labs(title = \"Posterior Samples vs True Density\",\n       subtitle = \"Red line = target distribution\",\n       x = expression(theta), y = \"Density\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nMetropolis-Hastings sampling from a bimodal posterior",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#gibbs-sampling",
    "href": "chapters/07_bayesian_foundations.html#gibbs-sampling",
    "title": "Bayesian Foundations",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\nA special case of MH where we sample each parameter conditionally on the others.\nAlgorithm (for \\(\\theta = (\\theta_1, \\theta_2)\\)):\n\nStart at \\((\\theta_1^{(0)}, \\theta_2^{(0)})\\)\nFor \\(g = 1, \\ldots, G\\):\n\nDraw \\(\\theta_1^{(g)} \\sim p(\\theta_1 | \\theta_2^{(g-1)}, Y)\\)\nDraw \\(\\theta_2^{(g)} \\sim p(\\theta_2 | \\theta_1^{(g)}, Y)\\)\n\n\nAdvantage: No tuning required (acceptance rate = 100%)\nRequirement: Must know conditional distributions\n\n\nCode\n# Gibbs sampler for bivariate normal\ngibbs_bivariate_normal &lt;- function(n_iter, mu, Sigma) {\n  # Target: N(mu, Sigma)\n  rho &lt;- Sigma[1,2] / sqrt(Sigma[1,1] * Sigma[2,2])\n  sd1 &lt;- sqrt(Sigma[1,1])\n  sd2 &lt;- sqrt(Sigma[2,2])\n\n  samples &lt;- matrix(0, n_iter, 2)\n  samples[1, ] &lt;- c(0, 0)  # start\n\n  for (i in 2:n_iter) {\n    # Draw x1 | x2\n    cond_mean1 &lt;- mu[1] + rho * sd1/sd2 * (samples[i-1, 2] - mu[2])\n    cond_sd1 &lt;- sd1 * sqrt(1 - rho^2)\n    samples[i, 1] &lt;- rnorm(1, cond_mean1, cond_sd1)\n\n    # Draw x2 | x1\n    cond_mean2 &lt;- mu[2] + rho * sd2/sd1 * (samples[i, 1] - mu[1])\n    cond_sd2 &lt;- sd2 * sqrt(1 - rho^2)\n    samples[i, 2] &lt;- rnorm(1, cond_mean2, cond_sd2)\n  }\n\n  samples\n}\n\n# Run Gibbs\nmu &lt;- c(1, 2)\nSigma &lt;- matrix(c(1, 0.8, 0.8, 1), 2, 2)\ngibbs_samples &lt;- gibbs_bivariate_normal(5000, mu, Sigma)\n\n# Plot trajectory and samples\ngibbs_df &lt;- data.frame(\n  x1 = gibbs_samples[, 1],\n  x2 = gibbs_samples[, 2],\n  iteration = 1:nrow(gibbs_samples)\n)\n\n# First 100 iterations showing path\np1 &lt;- ggplot(gibbs_df[1:100, ], aes(x = x1, y = x2)) +\n  geom_path(alpha = 0.5, color = \"#3498db\") +\n  geom_point(size = 0.5) +\n  labs(title = \"Gibbs Sampler Path (first 100 iterations)\",\n       x = expression(theta[1]), y = expression(theta[2]))\n\n# All samples\np2 &lt;- ggplot(gibbs_df[501:5000, ], aes(x = x1, y = x2)) +\n  geom_point(alpha = 0.2, size = 0.5, color = \"#3498db\") +\n  stat_ellipse(level = 0.95, color = \"#e74c3c\", size = 1) +\n  geom_point(aes(x = mu[1], y = mu[2]), color = \"#e74c3c\", size = 3, shape = 4) +\n  labs(title = \"Posterior Samples (after burn-in)\",\n       subtitle = \"Red ellipse = 95% contour, X = true mean\",\n       x = expression(theta[1]), y = expression(theta[2]))\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nGibbs sampling from a bivariate normal",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#gibbs-for-bayesian-regression",
    "href": "chapters/07_bayesian_foundations.html#gibbs-for-bayesian-regression",
    "title": "Bayesian Foundations",
    "section": "Gibbs for Bayesian Regression",
    "text": "Gibbs for Bayesian Regression\nFor the normal linear model \\(Y = X\\beta + \\varepsilon\\) with \\(\\varepsilon \\sim N(0, \\sigma^2 I)\\):\nBlock 1: Draw \\(\\beta | \\sigma^2, Y\\) \\[\n\\beta | \\sigma^2, Y \\sim N\\left(V_n X'Y, \\sigma^2 V_n\\right), \\quad V_n = (V_0^{-1} + X'X)^{-1}\n\\]\nBlock 2: Draw \\(\\sigma^2 | \\beta, Y\\) \\[\n\\sigma^2 | \\beta, Y \\sim \\text{Inv-Gamma}\\left(\\frac{\\nu_0 + n}{2}, \\frac{\\nu_0 s_0^2 + (Y-X\\beta)'(Y-X\\beta)}{2}\\right)\n\\]",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#hamiltonian-monte-carlo-hmc",
    "href": "chapters/07_bayesian_foundations.html#hamiltonian-monte-carlo-hmc",
    "title": "Bayesian Foundations",
    "section": "Hamiltonian Monte Carlo (HMC)",
    "text": "Hamiltonian Monte Carlo (HMC)\nModern MCMC that uses gradient information for efficient exploration.\nKey idea: Treat the parameter space as a physical system. The negative log-posterior is “potential energy.” Add “momentum” variables and simulate Hamiltonian dynamics.\nAdvantages: - Much more efficient for high-dimensional problems - Fewer iterations needed for convergence - Better exploration of complex posteriors\nImplementation: Use Stan (via rstan or cmdstanr) or PyMC\n\n\nCode\n# Comparison table of MCMC methods\nmcmc_table &lt;- data.frame(\n  Method = c(\"Metropolis-Hastings\", \"Gibbs Sampling\", \"HMC/NUTS\"),\n  Tuning = c(\"Proposal variance\", \"None (if conjugate)\", \"Step size, path length\"),\n  Efficiency = c(\"Low-Medium\", \"Medium\", \"High\"),\n  `Best For` = c(\"Simple models, low dimensions\",\n                 \"Conjugate models, block structure\",\n                 \"High dimensions, complex posteriors\"),\n  Software = c(\"Manual, MCMCpack\", \"Manual, JAGS\", \"Stan, PyMC\")\n)\n\nknitr::kable(mcmc_table, caption = \"Comparison of MCMC Methods\")\n\n\n\nComparison of MCMC Methods\n\n\n\n\n\n\n\n\n\nMethod\nTuning\nEfficiency\nBest.For\nSoftware\n\n\n\n\nMetropolis-Hastings\nProposal variance\nLow-Medium\nSimple models, low dimensions\nManual, MCMCpack\n\n\nGibbs Sampling\nNone (if conjugate)\nMedium\nConjugate models, block structure\nManual, JAGS\n\n\nHMC/NUTS\nStep size, path length\nHigh\nHigh dimensions, complex posteriors\nStan, PyMC",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#visual-diagnostics",
    "href": "chapters/07_bayesian_foundations.html#visual-diagnostics",
    "title": "Bayesian Foundations",
    "section": "Visual Diagnostics",
    "text": "Visual Diagnostics\n\nTrace Plots\nThe chain should look like a “fuzzy caterpillar”—no trends, quick mixing.\n\n\nCode\n# Good chain\ngood_chain &lt;- cumsum(rnorm(2000, 0, 1)) * 0.02 + rnorm(2000, 2, 0.5)\n\n# Bad chain (slow mixing)\nbad_chain &lt;- numeric(2000)\nbad_chain[1] &lt;- -3\nfor (i in 2:2000) {\n  bad_chain[i] &lt;- 0.995 * bad_chain[i-1] + rnorm(1, 0.01, 0.1)\n}\n\ntrace_comparison &lt;- data.frame(\n  iteration = rep(1:2000, 2),\n  value = c(good_chain, bad_chain),\n  chain = rep(c(\"Good: Fast Mixing\", \"Bad: Slow Mixing\"), each = 2000)\n)\n\nggplot(trace_comparison, aes(x = iteration, y = value)) +\n  geom_line(alpha = 0.7, color = \"#3498db\") +\n  facet_wrap(~chain, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Trace Plot Diagnostics\",\n       subtitle = \"Good chains look like 'fuzzy caterpillars'; bad chains show trends or slow drift\",\n       x = \"Iteration\", y = expression(theta))\n\n\n\n\n\nGood vs. bad trace plots\n\n\n\n\n\n\nAutocorrelation Plots\nSamples should be approximately independent. High autocorrelation = inefficient sampling.\n\n\nCode\n# Calculate ACF\nacf_good &lt;- acf(good_chain, lag.max = 50, plot = FALSE)\nacf_bad &lt;- acf(bad_chain, lag.max = 50, plot = FALSE)\n\nacf_df &lt;- data.frame(\n  lag = rep(0:50, 2),\n  acf = c(acf_good$acf, acf_bad$acf),\n  chain = rep(c(\"Good Chain\", \"Bad Chain\"), each = 51)\n)\n\nggplot(acf_df, aes(x = lag, y = acf)) +\n  geom_col(fill = \"#3498db\", width = 0.7) +\n  geom_hline(yintercept = c(-0.1, 0.1), linetype = \"dashed\", color = \"#e74c3c\") +\n  facet_wrap(~chain, ncol = 2) +\n  labs(title = \"Autocorrelation Function\",\n       subtitle = \"Good chains: ACF drops quickly; Bad chains: ACF stays high\",\n       x = \"Lag\", y = \"Autocorrelation\")\n\n\n\n\n\nAutocorrelation should decay quickly",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#quantitative-diagnostics",
    "href": "chapters/07_bayesian_foundations.html#quantitative-diagnostics",
    "title": "Bayesian Foundations",
    "section": "Quantitative Diagnostics",
    "text": "Quantitative Diagnostics\n\nGelman-Rubin \\(\\hat{R}\\) Statistic\nRun multiple chains from different starting points. Compare within-chain variance to between-chain variance.\n\\[\n\\hat{R} = \\sqrt{\\frac{\\text{Var}(\\theta | Y)_{\\text{estimate}}}{\\text{Within-chain variance}}}\n\\]\nRule: \\(\\hat{R} &lt; 1.1\\) (ideally \\(&lt; 1.05\\))\n\n\nEffective Sample Size (ESS)\nAccounts for autocorrelation. With \\(G\\) draws and autocorrelation \\(\\rho_k\\) at lag \\(k\\):\n\\[\n\\text{ESS} = \\frac{G}{1 + 2\\sum_{k=1}^{\\infty} \\rho_k}\n\\]\nRule: ESS \\(&gt; 400\\) for reliable posterior summaries\n\n\nCode\n# Calculate ESS (simplified)\ncalculate_ess &lt;- function(chain) {\n  n &lt;- length(chain)\n  acf_vals &lt;- acf(chain, lag.max = min(n-1, 100), plot = FALSE)$acf[-1]\n\n  # Sum until first negative autocorrelation\n  sum_rho &lt;- 0\n  for (rho in acf_vals) {\n    if (rho &lt; 0) break\n    sum_rho &lt;- sum_rho + rho\n  }\n\n  n / (1 + 2 * sum_rho)\n}\n\ncat(\"Good chain ESS:\", round(calculate_ess(good_chain)), \"out of\", length(good_chain), \"\\n\")\n\n\nGood chain ESS: 35 out of 2000 \n\n\nCode\ncat(\"Bad chain ESS:\", round(calculate_ess(bad_chain)), \"out of\", length(bad_chain), \"\\n\")\n\n\nBad chain ESS: 14 out of 2000 \n\n\n\n\nDiagnostic Summary Table\n\n\nCode\ndiagnostic_table &lt;- data.frame(\n  Diagnostic = c(\"Trace plot\", \"R-hat\", \"ESS\", \"Geweke\", \"Autocorrelation\"),\n  Target = c(\"No trends, good mixing\", \"&lt; 1.1 (ideally &lt; 1.05)\",\n             \"&gt; 400\", \"p &gt; 0.05\", \"Quick decay\"),\n  `What It Catches` = c(\"Non-stationarity\", \"Between-chain disagreement\",\n                        \"High autocorrelation\", \"Drift in mean\",\n                        \"Inefficient sampling\")\n)\n\nknitr::kable(diagnostic_table, caption = \"MCMC Convergence Diagnostics\")\n\n\n\nMCMC Convergence Diagnostics\n\n\nDiagnostic\nTarget\nWhat.It.Catches\n\n\n\n\nTrace plot\nNo trends, good mixing\nNon-stationarity\n\n\nR-hat\n&lt; 1.1 (ideally &lt; 1.05)\nBetween-chain disagreement\n\n\nESS\n&gt; 400\nHigh autocorrelation\n\n\nGeweke\np &gt; 0.05\nDrift in mean\n\n\nAutocorrelation\nQuick decay\nInefficient sampling",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#marginal-likelihood",
    "href": "chapters/07_bayesian_foundations.html#marginal-likelihood",
    "title": "Bayesian Foundations",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\nThe probability of the data under the model:\n\\[\np(Y | M) = \\int p(Y | \\theta, M) p(\\theta | M) d\\theta\n\\]\nThis is the denominator in Bayes’ theorem—it averages the likelihood over the prior.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#bayes-factors",
    "href": "chapters/07_bayesian_foundations.html#bayes-factors",
    "title": "Bayesian Foundations",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nCompare two models:\n\\[\nBF_{12} = \\frac{p(Y | M_1)}{p(Y | M_2)}\n\\]\n\n\n\nlog(BF)\nInterpretation\n\n\n\n\n0-1\nNot worth mentioning\n\n\n1-3\nPositive evidence\n\n\n3-5\nStrong evidence\n\n\n&gt; 5\nVery strong evidence",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#computing-marginal-likelihood",
    "href": "chapters/07_bayesian_foundations.html#computing-marginal-likelihood",
    "title": "Bayesian Foundations",
    "section": "Computing Marginal Likelihood",
    "text": "Computing Marginal Likelihood\nChallenge: The integral is often intractable.\nMethods:\n\nAnalytical (for conjugate models like NIW-BVAR)\nHarmonic Mean (easy but unreliable—avoid!)\nChib’s Method (for Gibbs samplers)\nBridge Sampling (general, reliable)\n\n\n\nCode\n# Simple example: comparing models with different priors\n# Model 1: Tight prior (N(0, 0.5))\n# Model 2: Vague prior (N(0, 5))\n\n# For normal-normal model, marginal likelihood is available analytically\n# p(Y | prior) = N(Y | prior_mean, prior_var + data_var/n)\n\ny_obs &lt;- c(1.2, 1.5, 0.8, 1.1, 1.4)  # sample data\nn &lt;- length(y_obs)\ny_bar &lt;- mean(y_obs)\nsigma2 &lt;- 1  # known variance\n\n# Marginal likelihood under each prior\nlog_ml &lt;- function(prior_mean, prior_var, y_bar, n, sigma2) {\n  marginal_var &lt;- prior_var + sigma2/n\n  dnorm(y_bar, prior_mean, sqrt(marginal_var), log = TRUE)\n}\n\nmodels &lt;- data.frame(\n  model = c(\"M1: Tight prior N(0, 0.5)\", \"M2: Vague prior N(0, 5)\",\n            \"M3: Centered prior N(1, 1)\"),\n  prior_mean = c(0, 0, 1),\n  prior_var = c(0.5, 5, 1)\n)\n\nmodels$log_ml &lt;- mapply(log_ml, models$prior_mean, models$prior_var,\n                        MoreArgs = list(y_bar = y_bar, n = n, sigma2 = sigma2))\nmodels$posterior_prob &lt;- exp(models$log_ml) / sum(exp(models$log_ml))\n\n# Bayes factors relative to M1\nmodels$BF_vs_M1 &lt;- exp(models$log_ml - models$log_ml[1])\n\ncat(\"Data: y_bar =\", round(y_bar, 2), \"\\n\\n\")\n\n\nData: y_bar = 1.2 \n\n\nCode\nprint(models[, c(\"model\", \"log_ml\", \"BF_vs_M1\", \"posterior_prob\")])\n\n\n                       model    log_ml  BF_vs_M1 posterior_prob\n1  M1: Tight prior N(0, 0.5) -1.769172 1.0000000      0.2503422\n2    M2: Vague prior N(0, 5) -1.881729 0.8935465      0.2236924\n3 M3: Centered prior N(1, 1) -1.026766 2.1009855      0.5259654",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#information-criteria",
    "href": "chapters/07_bayesian_foundations.html#information-criteria",
    "title": "Bayesian Foundations",
    "section": "Information Criteria",
    "text": "Information Criteria\nDIC (Deviance Information Criterion): \\[\nDIC = \\bar{D}(\\theta) + p_D\n\\] where \\(\\bar{D}\\) is average deviance and \\(p_D\\) is effective number of parameters.\nWAIC (Widely Applicable IC): - Fully Bayesian - Asymptotically equivalent to leave-one-out cross-validation - More reliable than DIC\nRule: Lower is better (like AIC/BIC)",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#r-using-coda-for-diagnostics",
    "href": "chapters/07_bayesian_foundations.html#r-using-coda-for-diagnostics",
    "title": "Bayesian Foundations",
    "section": "R: Using coda for Diagnostics",
    "text": "R: Using coda for Diagnostics\n\n\nCode\nlibrary(coda)\n\n# Convert MCMC output to coda object\nmcmc_chain &lt;- as.mcmc(posterior_draws)\n\n# Diagnostics\nplot(mcmc_chain)                    # Trace and density\nautocorr.plot(mcmc_chain)           # ACF\neffectiveSize(mcmc_chain)           # ESS\ngeweke.diag(mcmc_chain)             # Geweke test\n\n# For multiple chains\nmcmc_list &lt;- mcmc.list(chain1, chain2, chain3)\ngelman.diag(mcmc_list)              # R-hat\ngelman.plot(mcmc_list)              # R-hat over iterations",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#r-using-bvar-package",
    "href": "chapters/07_bayesian_foundations.html#r-using-bvar-package",
    "title": "Bayesian Foundations",
    "section": "R: Using BVAR Package",
    "text": "R: Using BVAR Package\n\n\nCode\nlibrary(BVAR)\n\n# Estimate Bayesian VAR with Minnesota prior\nbvar_model &lt;- bvar(\n  data = macro_data,\n  lags = 4,\n  n_draw = 20000,\n  n_burn = 5000,\n  priors = bv_minnesota(\n    lambda = bv_lambda(mode = 0.2, sd = 0.4, min = 0.0001, max = 5),\n    alpha = bv_alpha(mode = 2)\n  )\n)\n\n# Check convergence\nplot(bvar_model)  # trace plots\n\n# Posterior summaries\nsummary(bvar_model)\n\n# IRFs with credible bands\nirf_results &lt;- irf(bvar_model, horizon = 20)\nplot(irf_results)",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#python-using-pymc",
    "href": "chapters/07_bayesian_foundations.html#python-using-pymc",
    "title": "Bayesian Foundations",
    "section": "Python: Using PyMC",
    "text": "Python: Using PyMC\nimport pymc as pm\nimport numpy as np\nimport arviz as az\n\n# Example: Bayesian linear regression\nwith pm.Model() as model:\n    # Priors\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n    beta = pm.Normal(\"beta\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n\n    # Likelihood\n    mu = alpha + beta * X\n    y_obs = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n\n    # Sample\n    trace = pm.sample(2000, tune=1000, cores=2)\n\n# Diagnostics\naz.plot_trace(trace)\naz.summary(trace)\naz.plot_posterior(trace)\n\n# Model comparison\naz.waic(trace)\naz.loo(trace)",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/07_bayesian_foundations.html#key-references",
    "href": "chapters/07_bayesian_foundations.html#key-references",
    "title": "Bayesian Foundations",
    "section": "Key References",
    "text": "Key References\nFoundational:\n\nKoop (2003). Bayesian Econometrics\nGreenberg (2008). Introduction to Bayesian Econometrics\nGelman et al. (2013). Bayesian Data Analysis (3rd ed.)\n\nMCMC Methods:\n\nChib & Greenberg (1995). “Understanding the Metropolis-Hastings Algorithm.” The American Statistician\nGelfand & Smith (1990). “Sampling-Based Approaches to Calculating Marginal Densities.” JASA\n\nDiagnostics:\n\nGelman & Rubin (1992). “Inference from Iterative Simulation Using Multiple Sequences.” Statistical Science\nVehtari et al. (2021). “Rank-Normalization, Folding, and Localization: An Improved R-hat.” Bayesian Analysis\n\nModel Comparison:\n\nKass & Raftery (1995). “Bayes Factors.” JASA\nChib (1995). “Marginal Likelihood from the Gibbs Output.” JASA\n\nSoftware:\n\nStan Development Team. Stan User’s Guide\nPyMC Developers. PyMC Documentation\n\n\nNext: Module 8: Bayesian VARs",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html",
    "href": "chapters/08_bayesian_var.html",
    "title": "Bayesian Vector Autoregressions",
    "section": "",
    "text": "Why Bayesian VARs?\nVector Autoregressions are powerful tools for macroeconomic analysis, but they suffer from a fundamental problem: too many parameters, too little data.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#the-curse-of-dimensionality",
    "href": "chapters/08_bayesian_var.html#the-curse-of-dimensionality",
    "title": "Bayesian Vector Autoregressions",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nA VAR(p) with K variables has: \\[\n\\text{Parameters} = K^2 p + K = K(Kp + 1)\n\\]\n\n\n\nVariables (K)\nLags (p)\nParameters\nTypical Macro Sample (T=200)\n\n\n\n\n3\n4\n39\nOK\n\n\n5\n4\n105\nMarginal\n\n\n7\n4\n203\nProblematic\n\n\n10\n4\n410\nUnreliable\n\n\n\nWith quarterly data since 1960, you have ~250 observations. A 7-variable VAR(4) estimates 203 parameters—almost one parameter per observation!\n\n\nCode\nparam_count &lt;- expand.grid(K = 2:12, p = 1:8) %&gt;%\n  mutate(params = K * (K * p + 1))\n\nggplot(param_count, aes(x = K, y = params, color = factor(p))) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  geom_hline(yintercept = c(100, 200), linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 11, y = 110, label = \"T = 100\", hjust = 0, size = 3) +\n  annotate(\"text\", x = 11, y = 210, label = \"T = 200\", hjust = 0, size = 3) +\n  scale_color_viridis_d(option = \"plasma\") +\n  labs(title = \"VAR Parameters Explode with System Size\",\n       subtitle = \"Dashed lines show typical macro sample sizes\",\n       x = \"Number of Variables (K)\", y = \"Number of Parameters\",\n       color = \"Lags (p)\")\n\n\n\n\n\nThe curse of dimensionality in VARs",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#the-ols-problem",
    "href": "chapters/08_bayesian_var.html#the-ols-problem",
    "title": "Bayesian Vector Autoregressions",
    "section": "The OLS Problem",
    "text": "The OLS Problem\nWith many parameters and few observations: - Overfitting: In-sample fit is excellent, out-of-sample forecasts are poor - Imprecision: Huge standard errors, wide confidence intervals - Instability: Small changes in data lead to large changes in estimates",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#the-bayesian-solution-shrinkage",
    "href": "chapters/08_bayesian_var.html#the-bayesian-solution-shrinkage",
    "title": "Bayesian Vector Autoregressions",
    "section": "The Bayesian Solution: Shrinkage",
    "text": "The Bayesian Solution: Shrinkage\nKey insight: We have prior beliefs about macroeconomic dynamics. Use them!\nReasonable priors for macro VARs: 1. Variables are persistent (close to random walks) 2. Own lags matter more than other variables’ lags 3. Recent lags matter more than distant lags 4. Cross-variable effects are typically small\nBayesian estimation shrinks estimates toward these beliefs, reducing variance at the cost of some bias. With many parameters, this variance-bias tradeoff strongly favors shrinkage.\n\n\nCode\n# Demonstrate shrinkage benefit\nset.seed(123)\nn_sim &lt;- 1000\ntrue_beta &lt;- 0.3\nn_obs &lt;- 30\n\n# OLS estimates (high variance)\nols_estimates &lt;- replicate(n_sim, {\n  x &lt;- rnorm(n_obs)\n  y &lt;- true_beta * x + rnorm(n_obs, 0, 2)\n  coef(lm(y ~ x - 1))\n})\n\n# Bayesian estimates with shrinkage prior toward 0\n# Prior: beta ~ N(0, 0.5^2)\nprior_var &lt;- 0.5^2\nbayes_estimates &lt;- replicate(n_sim, {\n  x &lt;- rnorm(n_obs)\n  y &lt;- true_beta * x + rnorm(n_obs, 0, 2)\n\n  # Posterior with known sigma = 2\n  sigma2 &lt;- 4\n  post_var &lt;- 1 / (1/prior_var + sum(x^2)/sigma2)\n  post_mean &lt;- post_var * (sum(x*y)/sigma2)\n  post_mean\n})\n\nestimates_df &lt;- data.frame(\n  estimate = c(ols_estimates, bayes_estimates),\n  method = rep(c(\"OLS (no shrinkage)\", \"Bayesian (shrinkage)\"), each = n_sim)\n)\n\nggplot(estimates_df, aes(x = estimate, fill = method)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = true_beta, linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = true_beta + 0.1, y = 1.5,\n           label = paste0(\"True β = \", true_beta), hjust = 0) +\n  scale_fill_manual(values = c(\"OLS (no shrinkage)\" = \"#e74c3c\",\n                                \"Bayesian (shrinkage)\" = \"#3498db\")) +\n  labs(title = \"Shrinkage Reduces Estimation Variance\",\n       subtitle = \"Bayesian estimates cluster closer to truth despite small bias\",\n       x = expression(hat(beta)), y = \"Density\",\n       fill = \"\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nShrinkage reduces variance at the cost of small bias",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#litterman-1986-and-doan-litterman-sims-1984",
    "href": "chapters/08_bayesian_var.html#litterman-1986-and-doan-litterman-sims-1984",
    "title": "Bayesian Vector Autoregressions",
    "section": "Litterman (1986) and Doan-Litterman-Sims (1984)",
    "text": "Litterman (1986) and Doan-Litterman-Sims (1984)\nThe Minnesota prior is the workhorse prior for macroeconomic BVARs. It encodes the belief that macro variables behave approximately like random walks.\n\nPrior Mean\nFor variables in levels (GDP, prices, etc.):\n\\[\nE[A_1] = I_K, \\quad E[A_j] = 0 \\text{ for } j &gt; 1\n\\]\nEach variable’s first own lag coefficient is centered at 1 (random walk), all other coefficients at 0.\nFor stationary variables (growth rates, spreads):\n\\[\nE[A_j] = 0 \\text{ for all } j\n\\]\n\n\nPrior Variance (The Shrinkage Structure)\nThe key innovation is the structured prior variance:\nOwn lag \\(l\\) of variable \\(i\\): \\[\n\\text{Var}(a_{ii,l}) = \\left(\\frac{\\lambda_1}{l^d}\\right)^2\n\\]\nCross lag \\(l\\) of variable \\(j\\) on equation \\(i\\): \\[\n\\text{Var}(a_{ij,l}) = \\left(\\frac{\\lambda_1 \\cdot \\lambda_2 \\cdot \\sigma_i}{l^d \\cdot \\sigma_j}\\right)^2\n\\]\nConstant term: \\[\n\\text{Var}(c_i) = (\\lambda_1 \\cdot \\lambda_3)^2\n\\]\nwhere: - \\(\\sigma_i\\) = residual std from univariate AR(p) of variable \\(i\\) - \\(\\lambda_1\\) = overall tightness (controls shrinkage strength) - \\(\\lambda_2\\) = cross-variable shrinkage (typically &lt; 1) - \\(\\lambda_3\\) = constant tightness (typically large, ~100) - \\(d\\) = lag decay (typically 1 or 2)\n\n\nHyperparameter Interpretation\n\n\n\nParameter\nTypical Range\nEffect\n\n\n\n\n\\(\\lambda_1\\)\n0.1 - 0.5\nSmaller → more shrinkage toward prior\n\n\n\\(\\lambda_2\\)\n0.3 - 0.5\nSmaller → own lags dominate cross effects\n\n\n\\(\\lambda_3\\)\n100\nLarge → loose prior on constant\n\n\n\\(d\\)\n1 or 2\nLarger → faster decay for distant lags\n\n\n\n\n\nCode\n# Illustrate Minnesota prior variance structure\nK &lt;- 3\np &lt;- 4\nlambda1 &lt;- 0.2\nlambda2 &lt;- 0.5\nd &lt;- 1\n\n# Prior variance for different coefficient types\nlags &lt;- 1:p\nown_var &lt;- (lambda1 / lags^d)^2\ncross_var &lt;- (lambda1 * lambda2 / lags^d)^2\n\nvar_df &lt;- data.frame(\n  lag = rep(lags, 2),\n  variance = c(own_var, cross_var),\n  type = rep(c(\"Own lag\", \"Cross lag\"), each = p)\n)\n\nggplot(var_df, aes(x = lag, y = sqrt(variance), color = type)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"Own lag\" = \"#3498db\", \"Cross lag\" = \"#e74c3c\")) +\n  labs(title = \"Minnesota Prior: Standard Deviation by Lag\",\n       subtitle = expression(paste(lambda[1], \" = 0.2, \", lambda[2], \" = 0.5, d = 1\")),\n       x = \"Lag\", y = \"Prior Standard Deviation\",\n       color = \"Coefficient Type\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\nMinnesota prior variance structure",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#the-normal-inverse-wishart-prior",
    "href": "chapters/08_bayesian_var.html#the-normal-inverse-wishart-prior",
    "title": "Bayesian Vector Autoregressions",
    "section": "The Normal-Inverse-Wishart Prior",
    "text": "The Normal-Inverse-Wishart Prior\nThe Minnesota prior is typically implemented as a Normal-Inverse-Wishart (NIW) prior:\n\\[\n\\begin{aligned}\n\\text{vec}(B) | \\Sigma &\\sim N(\\text{vec}(B_0), \\Sigma \\otimes V_0) \\\\\n\\Sigma &\\sim \\text{Inverse-Wishart}(S_0, \\nu_0)\n\\end{aligned}\n\\]\nThis is conjugate: the posterior has the same form with updated parameters.\nPosterior: \\[\n\\begin{aligned}\nV_{\\text{post}} &= (V_0^{-1} + X'X)^{-1} \\\\\nB_{\\text{post}} &= V_{\\text{post}}(V_0^{-1} B_0 + X'Y) \\\\\n\\nu_{\\text{post}} &= \\nu_0 + T \\\\\nS_{\\text{post}} &= S_0 + (Y - XB_{\\text{post}})'(Y - XB_{\\text{post}}) + (B_{\\text{post}} - B_0)'V_0^{-1}(B_{\\text{post}} - B_0)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#giannone-lenza-primiceri-2015",
    "href": "chapters/08_bayesian_var.html#giannone-lenza-primiceri-2015",
    "title": "Bayesian Vector Autoregressions",
    "section": "Giannone, Lenza & Primiceri (2015)",
    "text": "Giannone, Lenza & Primiceri (2015)\nHow do we choose \\(\\lambda_1, \\lambda_2\\)? The traditional approach: researcher judgment or cross-validation.\nGLP innovation: Choose hyperparameters by maximizing the marginal likelihood:\n\\[\np(Y | \\lambda) = \\int p(Y | B, \\Sigma) \\cdot p(B, \\Sigma | \\lambda) \\, dB \\, d\\Sigma\n\\]\nFor the NIW prior, this integral has a closed-form solution!\n\nThe Algorithm\n\nDefine a grid over \\(\\lambda = (\\lambda_1, \\lambda_2, \\ldots)\\)\nFor each \\(\\lambda\\), compute \\(p(Y | \\lambda)\\) analytically\nChoose \\(\\lambda^* = \\arg\\max_\\lambda p(Y | \\lambda)\\)\nEstimate BVAR with optimal hyperparameters\n\n\n\nWhy This Matters\n\nData-driven: No arbitrary hyperparameter choices\nAutomatic: Adapts to different datasets\nForecast accuracy: GLP BVARs consistently outperform OLS VARs\n\n\n\nCode\n# Illustrate marginal likelihood as function of lambda\n# (Simulated for illustration)\nlambda_grid &lt;- seq(0.01, 1, length.out = 50)\n\n# Simulated marginal likelihood (realistic shape)\nset.seed(42)\nml_values &lt;- -50 - 20*(lambda_grid - 0.25)^2 + rnorm(50, 0, 2)\n\nml_df &lt;- data.frame(lambda = lambda_grid, log_ml = ml_values)\noptimal_lambda &lt;- lambda_grid[which.max(ml_values)]\n\nggplot(ml_df, aes(x = lambda, y = log_ml)) +\n  geom_line(linewidth = 1, color = \"#3498db\") +\n  geom_vline(xintercept = optimal_lambda, linetype = \"dashed\", color = \"#e74c3c\") +\n  annotate(\"point\", x = optimal_lambda, y = max(ml_values),\n           size = 4, color = \"#e74c3c\") +\n  annotate(\"text\", x = optimal_lambda + 0.05, y = max(ml_values) - 5,\n           label = paste0(\"λ* = \", round(optimal_lambda, 2)), hjust = 0,\n           color = \"#e74c3c\") +\n  labs(title = \"GLP: Marginal Likelihood Optimization\",\n       subtitle = \"Optimal shrinkage is data-determined\",\n       x = expression(lambda[1] ~ \"(Overall Tightness)\"),\n       y = \"Log Marginal Likelihood\")\n\n\n\n\n\nGLP marginal likelihood optimization",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#additional-priors-sum-of-coefficients-and-no-cointegration",
    "href": "chapters/08_bayesian_var.html#additional-priors-sum-of-coefficients-and-no-cointegration",
    "title": "Bayesian Vector Autoregressions",
    "section": "Additional Priors: Sum-of-Coefficients and No-Cointegration",
    "text": "Additional Priors: Sum-of-Coefficients and No-Cointegration\nGLP (2015) also optimizes over additional priors that improve forecasting:\n\nSum-of-Coefficients (Doan-Litterman-Sims)\nBelief: If all variables equal their sample means, the forecast should also be the mean.\nImplemented by adding dummy observations. Controlled by hyperparameter \\(\\mu\\).\n\n\nNo-Cointegration (Sims-Zha)\nBelief: Variables have unit roots but are not cointegrated.\nAlso implemented via dummy observations. Controlled by hyperparameter \\(\\delta\\).\nGLP jointly optimizes \\((\\lambda_1, \\lambda_2, \\mu, \\delta)\\) via marginal likelihood.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#the-gibbs-sampler",
    "href": "chapters/08_bayesian_var.html#the-gibbs-sampler",
    "title": "Bayesian Vector Autoregressions",
    "section": "The Gibbs Sampler",
    "text": "The Gibbs Sampler\nBlock 1: Draw \\(B | \\Sigma, Y\\) \\[\n\\text{vec}(B) | \\Sigma, Y \\sim N(\\text{vec}(B_{\\text{post}}), \\Sigma \\otimes V_{\\text{post}})\n\\]\nBlock 2: Draw \\(\\Sigma | B, Y\\) \\[\n\\Sigma | B, Y \\sim \\text{Inverse-Wishart}(S_{\\text{post}}, \\nu_{\\text{post}})\n\\]\nIterate for \\(G\\) draws, discarding the first \\(B_{\\text{burn}}\\) as burn-in.\n\n\nCode\n# Simplified BVAR Gibbs sampler demonstration\n# Generate synthetic VAR(1) data\nT_obs &lt;- 100\nK &lt;- 2\n\n# True parameters\nA_true &lt;- matrix(c(0.8, 0.1, 0.05, 0.7), K, K)\nSigma_true &lt;- matrix(c(1, 0.3, 0.3, 1), K, K)\n\n# Generate data\nY &lt;- matrix(0, T_obs, K)\nY[1, ] &lt;- rnorm(K)\nfor (t in 2:T_obs) {\n  Y[t, ] &lt;- Y[t-1, ] %*% t(A_true) + rmvnorm(1, sigma = Sigma_true)\n}\n\n# Build matrices\ny &lt;- Y[2:T_obs, ]\nX &lt;- cbind(1, Y[1:(T_obs-1), ])\nT_eff &lt;- nrow(y)\nM &lt;- ncol(X)\n\n# Minnesota-style prior\nB0 &lt;- matrix(0, M, K)\nB0[2:3, ] &lt;- diag(K) * 0.9  # Prior: close to random walk\n\n# Prior precision (diagonal, tight) - must be (M*K) x (M*K) for vec(B)\nlambda1 &lt;- 0.2\n# Build prior variance for each coefficient in vec(B)\n# Structure: [const_eq1, y1_eq1, y2_eq1, const_eq2, y1_eq2, y2_eq2]\nv0_diag &lt;- numeric(M * K)\nfor (eq in 1:K) {\n  idx_const &lt;- (eq - 1) * M + 1\n  v0_diag[idx_const] &lt;- 100  # Loose prior on constant\n  for (var in 1:K) {\n    idx_var &lt;- (eq - 1) * M + 1 + var\n    v0_diag[idx_var] &lt;- lambda1^2  # Tight prior on AR coefficients\n  }\n}\nV0_inv &lt;- diag(1 / v0_diag)\n\n# Prior for Sigma\nnu0 &lt;- K + 2\nS0 &lt;- diag(K)\n\n# Gibbs sampler\nn_draw &lt;- 2000\nn_burn &lt;- 500\nB_draws &lt;- array(0, dim = c(M, K, n_draw))\nSigma_draws &lt;- array(0, dim = c(K, K, n_draw))\n\n# Initialize\nB_curr &lt;- solve(crossprod(X)) %*% crossprod(X, y)\nSigma_curr &lt;- crossprod(y - X %*% B_curr) / T_eff\n\nfor (g in 1:(n_draw + n_burn)) {\n  # Draw B | Sigma using vectorized form\n  # vec(B) | Sigma ~ N(b_post, V_post) where V_post = (V0_inv + Sigma^{-1} ⊗ X'X)^{-1}\n  Sigma_inv &lt;- solve(Sigma_curr)\n  V_post_inv &lt;- V0_inv + kronecker(Sigma_inv, crossprod(X))\n  V_post &lt;- solve(V_post_inv)\n  b_post &lt;- V_post %*% (V0_inv %*% as.vector(B0) +\n                         kronecker(Sigma_inv, t(X)) %*% as.vector(y))\n  B_curr &lt;- matrix(rmvnorm(1, b_post, V_post), M, K)\n\n  # Draw Sigma | B\n  U &lt;- y - X %*% B_curr\n  S_post &lt;- S0 + crossprod(U)\n  Sigma_curr &lt;- solve(rWishart(1, nu0 + T_eff, solve(S_post))[,,1])\n\n  # Store\n  if (g &gt; n_burn) {\n    B_draws[,,g - n_burn] &lt;- B_curr\n    Sigma_draws[,,g - n_burn] &lt;- Sigma_curr\n  }\n}\n\n# Trace plot for A[1,1]\ntrace_df &lt;- data.frame(\n  iteration = 1:n_draw,\n  value = B_draws[2, 1, ]  # A[1,1] coefficient\n)\n\nggplot(trace_df, aes(x = iteration, y = value)) +\n  geom_line(alpha = 0.7, color = \"#3498db\") +\n  geom_hline(yintercept = A_true[1,1], linetype = \"dashed\", color = \"#e74c3c\") +\n  geom_hline(yintercept = mean(trace_df$value), linetype = \"dotted\", color = \"#2ecc71\") +\n  annotate(\"text\", x = n_draw * 0.8, y = A_true[1,1] + 0.05,\n           label = paste0(\"True = \", A_true[1,1]), color = \"#e74c3c\") +\n  labs(title = \"Gibbs Sampler: Trace Plot for VAR Coefficient\",\n       subtitle = \"Green = posterior mean, Red = true value\",\n       x = \"Iteration (post burn-in)\", y = expression(A[11]))\n\n\n\n\n\nGibbs sampler trace plots for BVAR coefficient",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#cholesky-identification",
    "href": "chapters/08_bayesian_var.html#cholesky-identification",
    "title": "Bayesian Vector Autoregressions",
    "section": "Cholesky Identification",
    "text": "Cholesky Identification\nThe simplest approach: impose a recursive structure.\n\\[\n\\Sigma = PP', \\quad P = \\text{chol}(\\Sigma, \\text{lower})\n\\]\nFor each posterior draw \\(\\Sigma^{(g)}\\), compute \\(P^{(g)} = \\text{chol}(\\Sigma^{(g)})\\).\nThe ordering matters: variable ordered first is not affected contemporaneously by others.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#sign-restrictions-in-bvar",
    "href": "chapters/08_bayesian_var.html#sign-restrictions-in-bvar",
    "title": "Bayesian Vector Autoregressions",
    "section": "Sign Restrictions in BVAR",
    "text": "Sign Restrictions in BVAR\nBayesian framework naturally handles set-identification:\n\nFor each posterior draw \\((B^{(g)}, \\Sigma^{(g)})\\):\n\nCompute \\(P^{(g)} = \\text{chol}(\\Sigma^{(g)})\\)\nDraw orthogonal rotation \\(Q\\) uniformly\nCandidate impact: \\(\\tilde{P}^{(g)} = P^{(g)} Q\\)\nCheck sign restrictions on IRFs\nIf satisfied: keep. Otherwise: redraw \\(Q\\)\n\nReport distribution across accepted draws\n\n\n\nCode\n# Sign restriction: monetary tightening\n# - Raises interest rate (positive)\n# - Lowers output (negative)\n# - Lowers inflation (negative)\n\nrestrictions &lt;- data.frame(\n  Variable = c(\"Interest Rate\", \"Output\", \"Inflation\"),\n  Sign = c(\"+\", \"-\", \"-\"),\n  Interpretation = c(\"Policy instrument\", \"Demand falls\", \"Prices fall\")\n)\n\nknitr::kable(restrictions,\n             caption = \"Example Sign Restrictions for Monetary Policy Shock\")\n\n\n\nExample Sign Restrictions for Monetary Policy Shock\n\n\nVariable\nSign\nInterpretation\n\n\n\n\nInterest Rate\n+\nPolicy instrument\n\n\nOutput\n-\nDemand falls\n\n\nInflation\n-\nPrices fall",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#computing-posterior-irfs",
    "href": "chapters/08_bayesian_var.html#computing-posterior-irfs",
    "title": "Bayesian Vector Autoregressions",
    "section": "Computing Posterior IRFs",
    "text": "Computing Posterior IRFs\nFor each posterior draw \\((B^{(g)}, \\Sigma^{(g)})\\):\n\nExtract coefficient matrices \\(A_1^{(g)}, \\ldots, A_p^{(g)}\\) from \\(B^{(g)}\\)\nCompute structural impact \\(P^{(g)}\\) (Cholesky or sign-restricted)\nCompute IRF at each horizon using MA representation: \\[\n\\Phi_h = \\sum_{j=1}^{\\min(h,p)} \\Phi_{h-j} A_j, \\quad \\Phi_0 = I\n\\]\nStructural IRF: \\(\\text{IRF}_h = \\Phi_h P\\)\n\nReport posterior median and credible bands (68%, 95%).\n\n\nCode\n# Compute IRFs from Gibbs draws\nH &lt;- 20  # horizons\nK &lt;- 2\nn_draws_use &lt;- 500  # use subset for speed\n\nIRF_draws &lt;- array(0, dim = c(K, K, H+1, n_draws_use))\n\nfor (g in 1:n_draws_use) {\n  # Extract A matrix (coefficients on lagged Y)\n  A1 &lt;- t(B_draws[2:3, , g])\n\n  # Cholesky impact\n  P &lt;- t(chol(Sigma_draws[,,g]))\n\n  # Compute IRFs\n  Phi &lt;- diag(K)\n  IRF_draws[,,1,g] &lt;- P\n\n  for (h in 1:H) {\n    Phi &lt;- Phi %*% A1\n    IRF_draws[,,h+1,g] &lt;- Phi %*% P\n  }\n}\n\n# Extract IRF of variable 1 to shock 1\nirf_11 &lt;- IRF_draws[1, 1, , ]\n\n# Posterior summaries\nirf_median &lt;- apply(irf_11, 1, median)\nirf_lower &lt;- apply(irf_11, 1, quantile, 0.16)\nirf_upper &lt;- apply(irf_11, 1, quantile, 0.84)\nirf_lower95 &lt;- apply(irf_11, 1, quantile, 0.025)\nirf_upper95 &lt;- apply(irf_11, 1, quantile, 0.975)\n\nirf_df &lt;- data.frame(\n  horizon = 0:H,\n  median = irf_median,\n  lower68 = irf_lower,\n  upper68 = irf_upper,\n  lower95 = irf_lower95,\n  upper95 = irf_upper95\n)\n\nggplot(irf_df, aes(x = horizon)) +\n  geom_ribbon(aes(ymin = lower95, ymax = upper95), fill = \"#3498db\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = lower68, ymax = upper68), fill = \"#3498db\", alpha = 0.4) +\n  geom_line(aes(y = median), linewidth = 1.2, color = \"#2c3e50\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bayesian VAR: Impulse Response Function\",\n       subtitle = \"Posterior median with 68% and 95% credible bands\",\n       x = \"Horizon\", y = \"Response of Y1 to Shock 1\")\n\n\n\n\n\nImpulse responses with posterior credible bands",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#forecast-error-variance-decomposition",
    "href": "chapters/08_bayesian_var.html#forecast-error-variance-decomposition",
    "title": "Bayesian Vector Autoregressions",
    "section": "Forecast Error Variance Decomposition",
    "text": "Forecast Error Variance Decomposition\nFEVD at horizon \\(h\\): \\[\n\\text{FEVD}_{i,j}(h) = \\frac{\\sum_{s=0}^{h} (\\text{IRF}_{i,j,s})^2}{\\sum_{s=0}^{h} \\sum_{k=1}^{K} (\\text{IRF}_{i,k,s})^2}\n\\]\nCompute for each posterior draw, report median and bands.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#using-the-bvar-package",
    "href": "chapters/08_bayesian_var.html#using-the-bvar-package",
    "title": "Bayesian Vector Autoregressions",
    "section": "Using the BVAR Package",
    "text": "Using the BVAR Package\nThe BVAR package implements Minnesota priors with GLP-style optimization.\nlibrary(BVAR)\n\n# Prepare data (T × K matrix)\ndata_mat &lt;- as.matrix(macro_data[, c(\"gdp_growth\", \"inflation\", \"interest_rate\")])\n\n# Minnesota prior with GLP optimization\nmn_prior &lt;- bv_minnesota(\n  lambda = bv_lambda(mode = 0.2, sd = 0.4, min = 0.0001, max = 5),\n  alpha = bv_alpha(mode = 2),   # lag decay\n  var = 1e07                     # constant variance\n)\n\n# Estimate BVAR\nbvar_model &lt;- bvar(\n  data = data_mat,\n  lags = 4,\n  n_draw = 20000,\n  n_burn = 5000,\n  priors = mn_prior,\n  mh = bv_mh(scale_hess = 0.01, adjust_acc = TRUE)\n)\n\n# Diagnostics\nsummary(bvar_model)\nplot(bvar_model)  # trace plots\n\n# IRFs (Cholesky identification)\nirf_results &lt;- irf(bvar_model, horizon = 20, identification = TRUE)\nplot(irf_results)\n\n# FEVD\nfevd_results &lt;- fevd(bvar_model, horizon = 20)\nplot(fevd_results)\n\n# Forecasting\nforecasts &lt;- predict(bvar_model, horizon = 8)\nplot(forecasts)",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#using-the-bvartools-package",
    "href": "chapters/08_bayesian_var.html#using-the-bvartools-package",
    "title": "Bayesian Vector Autoregressions",
    "section": "Using the bvartools Package",
    "text": "Using the bvartools Package\nMore flexible, allows custom prior specification.\nlibrary(bvartools)\n\n# Generate VAR data matrices\nbvar_data &lt;- gen_var(data_ts, p = 4, deterministic = \"const\")\ny &lt;- t(bvar_data$data$Y)\nx &lt;- t(bvar_data$data$Z)\n\n# Set up Minnesota prior manually\nK &lt;- ncol(data_mat)\nM &lt;- K * 4 + 1\n\n# Prior mean: random walk on first lag\na_mu_prior &lt;- matrix(0, M, K)\na_mu_prior[2:(K+1), ] &lt;- diag(K)\n\n# Prior precision (Minnesota structure)\na_v_i_prior &lt;- diag(1, M * K) * 0.01\n\n# Prior for Sigma\nu_sigma_df_prior &lt;- K + 2\nu_sigma_scale_prior &lt;- diag(1, K)\n\n# Estimate\nbvar_est &lt;- bvar(\n  y = y, x = x,\n  A = list(mu = a_mu_prior, V_i = a_v_i_prior),\n  Sigma = list(df = u_sigma_df_prior, scale = u_sigma_scale_prior),\n  iterations = 20000, burnin = 5000\n)\n\n# IRFs\nbvar_irf &lt;- irf(bvar_est,\n                impulse = \"interest_rate\",\n                response = \"inflation\",\n                n.ahead = 20,\n                type = \"feir\",  # forecast error IRF\n                ci = 0.95)\nplot(bvar_irf)",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#manual-implementation",
    "href": "chapters/08_bayesian_var.html#manual-implementation",
    "title": "Bayesian Vector Autoregressions",
    "section": "Manual Implementation",
    "text": "Manual Implementation\nFor full control and understanding:\nbvar_gibbs &lt;- function(Y, p, n_draw = 10000, n_burn = 2000,\n                        lambda1 = 0.2, lambda2 = 0.5, lambda3 = 100) {\n  T_full &lt;- nrow(Y)\n  K &lt;- ncol(Y)\n  M &lt;- K * p + 1\n\n  # Build data matrices\n  y &lt;- Y[(p + 1):T_full, ]\n  T_eff &lt;- nrow(y)\n  X &lt;- cbind(1)\n  for (j in 1:p) {\n    X &lt;- cbind(X, Y[(p + 1 - j):(T_full - j), ])\n  }\n\n  # Minnesota prior mean\n  B0 &lt;- matrix(0, M, K)\n  B0[2:(K + 1), ] &lt;- diag(K)  # first lag = identity\n\n  # Scaling from AR residuals\n  sigma_i &lt;- numeric(K)\n  for (i in 1:K) {\n    ar_fit &lt;- ar(Y[, i], order.max = p, method = \"ols\")\n    sigma_i[i] &lt;- sqrt(ar_fit$var.pred)\n  }\n\n  # Prior variance (Minnesota structure)\n  V0_diag &lt;- numeric(M * K)\n  for (eq in 1:K) {\n    idx &lt;- (eq - 1) * M + 1\n    V0_diag[idx] &lt;- (lambda1 * lambda3)^2  # constant\n\n    for (lag in 1:p) {\n      for (var in 1:K) {\n        idx &lt;- (eq - 1) * M + 1 + (lag - 1) * K + var\n        if (var == eq) {\n          V0_diag[idx] &lt;- (lambda1 / lag)^2\n        } else {\n          V0_diag[idx] &lt;- (lambda1 * lambda2 * sigma_i[eq] / (lag * sigma_i[var]))^2\n        }\n      }\n    }\n  }\n  V0_inv &lt;- diag(1 / V0_diag)\n\n  # Inverse Wishart prior\n  v0 &lt;- K + 2\n  S0 &lt;- diag(sigma_i^2)\n\n  # Storage and initialization\n  B_draws &lt;- array(0, dim = c(M, K, n_draw))\n  Sigma_draws &lt;- array(0, dim = c(K, K, n_draw))\n  B_curr &lt;- solve(crossprod(X)) %*% crossprod(X, y)\n  Sigma_curr &lt;- crossprod(y - X %*% B_curr) / T_eff\n\n  # Gibbs sampler\n  for (g in 1:(n_draw + n_burn)) {\n    # Block 1: B | Sigma\n    Sigma_inv &lt;- solve(Sigma_curr)\n    V_post_inv &lt;- V0_inv + kronecker(Sigma_inv, crossprod(X))\n    V_post &lt;- solve(V_post_inv)\n    b_post &lt;- V_post %*% (V0_inv %*% as.vector(B0) +\n                           kronecker(Sigma_inv, t(X)) %*% as.vector(y))\n    B_curr &lt;- matrix(mvtnorm::rmvnorm(1, b_post, V_post), M, K)\n\n    # Block 2: Sigma | B\n    U &lt;- y - X %*% B_curr\n    S_post &lt;- S0 + crossprod(U)\n    v_post &lt;- v0 + T_eff\n    Sigma_curr &lt;- solve(rWishart(1, v_post, solve(S_post))[,,1])\n\n    if (g &gt; n_burn) {\n      B_draws[,,g - n_burn] &lt;- B_curr\n      Sigma_draws[,,g - n_burn] &lt;- Sigma_curr\n    }\n  }\n\n  list(B = B_draws, Sigma = Sigma_draws, K = K, p = p)\n}",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#using-numpyscipy-manual",
    "href": "chapters/08_bayesian_var.html#using-numpyscipy-manual",
    "title": "Bayesian Vector Autoregressions",
    "section": "Using NumPy/SciPy (Manual)",
    "text": "Using NumPy/SciPy (Manual)\nimport numpy as np\nfrom scipy import stats\nfrom scipy.linalg import cholesky, solve\n\ndef bvar_gibbs(Y, p, n_draw=10000, n_burn=2000,\n               lambda1=0.2, lambda2=0.5, lambda3=100):\n    \"\"\"\n    Bayesian VAR with Minnesota prior via Gibbs sampling.\n\n    Parameters\n    ----------\n    Y : ndarray (T, K)\n        Data matrix\n    p : int\n        Lag order\n    n_draw : int\n        Number of posterior draws\n    n_burn : int\n        Burn-in period\n    lambda1, lambda2, lambda3 : float\n        Minnesota prior hyperparameters\n\n    Returns\n    -------\n    dict with posterior draws of B and Sigma\n    \"\"\"\n    T_full, K = Y.shape\n    M = K * p + 1\n\n    # Build data matrices\n    y = Y[p:]\n    T_eff = len(y)\n\n    X = np.ones((T_eff, 1))\n    for j in range(1, p + 1):\n        X = np.hstack([X, Y[p-j:T_full-j]])\n\n    # Minnesota prior mean (random walk on first lag)\n    B0 = np.zeros((M, K))\n    B0[1:K+1, :] = np.eye(K)\n\n    # AR residual scaling\n    sigma_i = np.zeros(K)\n    for i in range(K):\n        ar_resid = Y[p:, i] - Y[p-1:-1, i] * 0.9  # approximate\n        sigma_i[i] = np.std(ar_resid)\n\n    # Prior variance (Minnesota structure)\n    V0_diag = np.zeros(M * K)\n    for eq in range(K):\n        idx = eq * M\n        V0_diag[idx] = (lambda1 * lambda3)**2\n\n        for lag in range(1, p + 1):\n            for var in range(K):\n                idx = eq * M + 1 + (lag - 1) * K + var\n                if var == eq:\n                    V0_diag[idx] = (lambda1 / lag)**2\n                else:\n                    V0_diag[idx] = (lambda1 * lambda2 * sigma_i[eq] /\n                                    (lag * sigma_i[var]))**2\n\n    V0_inv = np.diag(1 / V0_diag)\n\n    # Inverse Wishart prior\n    v0 = K + 2\n    S0 = np.diag(sigma_i**2)\n\n    # Initialize\n    B_curr = np.linalg.lstsq(X, y, rcond=None)[0]\n    U = y - X @ B_curr\n    Sigma_curr = U.T @ U / T_eff\n\n    # Storage\n    B_draws = np.zeros((M, K, n_draw))\n    Sigma_draws = np.zeros((K, K, n_draw))\n\n    for g in range(n_draw + n_burn):\n        # Block 1: Draw B | Sigma\n        Sigma_inv = np.linalg.inv(Sigma_curr)\n        V_post_inv = V0_inv + np.kron(Sigma_inv, X.T @ X)\n        V_post = np.linalg.inv(V_post_inv)\n\n        b0_vec = B0.flatten('F')\n        xy_vec = (X.T @ y @ Sigma_inv).flatten('F')\n        b_post = V_post @ (V0_inv @ b0_vec + xy_vec)\n\n        B_curr = np.random.multivariate_normal(b_post, V_post).reshape((M, K), order='F')\n\n        # Block 2: Draw Sigma | B\n        U = y - X @ B_curr\n        S_post = S0 + U.T @ U\n        v_post = v0 + T_eff\n\n        # Inverse Wishart draw\n        Sigma_curr = stats.invwishart.rvs(df=v_post, scale=S_post)\n\n        if g &gt;= n_burn:\n            B_draws[:, :, g - n_burn] = B_curr\n            Sigma_draws[:, :, g - n_burn] = Sigma_curr\n\n    return {'B': B_draws, 'Sigma': Sigma_draws, 'K': K, 'p': p}\n\n\ndef compute_irfs(bvar_result, shock_idx, H=20):\n    \"\"\"\n    Compute IRFs from BVAR posterior draws.\n    \"\"\"\n    B_draws = bvar_result['B']\n    Sigma_draws = bvar_result['Sigma']\n    K = bvar_result['K']\n    p = bvar_result['p']\n    n_draw = B_draws.shape[2]\n\n    IRF_draws = np.zeros((K, H + 1, n_draw))\n\n    for g in range(n_draw):\n        # Extract A matrices\n        A = [B_draws[1 + j*K:1 + (j+1)*K, :, g].T for j in range(p)]\n\n        # Cholesky impact\n        P = cholesky(Sigma_draws[:, :, g], lower=True)\n\n        # Compute IRFs\n        Phi = np.eye(K)\n        IRF_draws[:, 0, g] = P[:, shock_idx]\n\n        for h in range(1, H + 1):\n            Phi = Phi @ A[0] if p &gt;= 1 else np.zeros((K, K))\n            IRF_draws[:, h, g] = Phi @ P[:, shock_idx]\n\n    return {\n        'median': np.median(IRF_draws, axis=2),\n        'lower': np.percentile(IRF_draws, 16, axis=2),\n        'upper': np.percentile(IRF_draws, 84, axis=2)\n    }",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#using-pymc-probabilistic-programming",
    "href": "chapters/08_bayesian_var.html#using-pymc-probabilistic-programming",
    "title": "Bayesian Vector Autoregressions",
    "section": "Using PyMC (Probabilistic Programming)",
    "text": "Using PyMC (Probabilistic Programming)\nimport pymc as pm\nimport numpy as np\nimport arviz as az\n\ndef bvar_pymc(Y, p, n_draw=2000, n_tune=1000):\n    \"\"\"\n    Bayesian VAR using PyMC.\n    \"\"\"\n    T_full, K = Y.shape\n\n    # Build data matrices\n    y = Y[p:]\n    T_eff = len(y)\n\n    X = np.ones((T_eff, 1))\n    for j in range(1, p + 1):\n        X = np.hstack([X, Y[p-j:T_full-j]])\n    M = X.shape[1]\n\n    with pm.Model() as bvar_model:\n        # Priors\n        # Coefficients: Normal with Minnesota-style shrinkage\n        B = pm.Normal('B', mu=0, sigma=0.5, shape=(M, K))\n\n        # Covariance: LKJ prior on correlation + half-normal on scales\n        chol, corr, stds = pm.LKJCholeskyCov(\n            'chol', n=K, eta=2, sd_dist=pm.HalfNormal.dist(sigma=1)\n        )\n        Sigma = pm.Deterministic('Sigma', chol @ chol.T)\n\n        # Likelihood\n        mu = pm.math.dot(X, B)\n        y_obs = pm.MvNormal('y', mu=mu, chol=chol, observed=y)\n\n        # Sample\n        trace = pm.sample(n_draw, tune=n_tune, cores=2,\n                          return_inferencedata=True)\n\n    return trace\n\n# Usage:\n# trace = bvar_pymc(Y, p=4)\n# az.plot_trace(trace, var_names=['B'])\n# az.summary(trace, var_names=['B', 'Sigma'])",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/08_bayesian_var.html#key-references",
    "href": "chapters/08_bayesian_var.html#key-references",
    "title": "Bayesian Vector Autoregressions",
    "section": "Key References",
    "text": "Key References\nFoundational:\n\nLitterman (1986). “Forecasting with Bayesian VARs.” JBES\nDoan, Litterman & Sims (1984). “Forecasting and Conditional Projection.”\nSims & Zha (1998). “Bayesian Methods for Dynamic Multivariate Models.” IER\n\nModern Methods:\n\nGiannone, Lenza & Primiceri (2015). “Prior Selection for VARs.” REStat\nBanbura, Giannone & Reichlin (2010). “Large Bayesian VARs.” JAE\nCarriero, Clark & Marcellino (2019). “Large BVARs with Stochastic Volatility.” JoE\n\nTextbooks:\n\nKoop (2003). Bayesian Econometrics\nBlake & Mumtaz (2012). Applied Bayesian Econometrics for Central Bankers\n\nR Packages:\n\nBVAR: Kuschnig & Vashold — Minnesota prior with GLP\nbvartools: Mohr — Flexible BVAR toolkit\nbvarsv: Krueger — TVP-VAR with stochastic volatility\n\n\nNext: Module 9: Bayesian Panel Methods",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Vector Autoregressions</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html",
    "href": "chapters/09_bayesian_panel.html",
    "title": "Bayesian Panel Methods",
    "section": "",
    "text": "The Panel Data Challenge\nCross-country macroeconomic panels present a fundamental tension:\nHeterogeneity: Countries differ in institutions, structures, and responses Limited Data: Each country has finite time series (T = 84 quarters) Efficiency: Ignoring cross-country information wastes data",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#three-approaches",
    "href": "chapters/09_bayesian_panel.html#three-approaches",
    "title": "Bayesian Panel Methods",
    "section": "Three Approaches",
    "text": "Three Approaches\n\n\n\n\n\n\n\n\nApproach\nAssumption\nProblem\n\n\n\n\nPooled\nAll countries identical\nIgnores heterogeneity\n\n\nCountry-by-country\nCountries unrelated\nInefficient, noisy\n\n\nHierarchical Bayes\nCountries related but different\nBest of both worlds\n\n\n\nWith N = 46 countries and T = 84 quarters, hierarchical Bayesian methods offer the natural solution.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#the-multilevel-structure",
    "href": "chapters/09_bayesian_panel.html#the-multilevel-structure",
    "title": "Bayesian Panel Methods",
    "section": "The Multilevel Structure",
    "text": "The Multilevel Structure\nConsider a simple panel regression: \\[\ny_{it} = x_{it}'\\beta_i + \\varepsilon_{it}, \\quad \\varepsilon_{it} \\sim N(0, \\sigma^2_i)\n\\]\nLevel 1 (Within-Country): \\[\n\\beta_i | \\mu, \\Omega \\sim N(\\mu, \\Omega)\n\\]\nLevel 2 (Across-Countries): \\[\n\\mu \\sim N(\\mu_0, V_0), \\quad \\Omega \\sim \\text{Inverse-Wishart}(\\Omega_0, \\nu_0)\n\\]\nThe country-specific coefficients \\(\\beta_i\\) are drawn from a common distribution with unknown mean \\(\\mu\\) and variance \\(\\Omega\\).",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#shrinkage-and-partial-pooling",
    "href": "chapters/09_bayesian_panel.html#shrinkage-and-partial-pooling",
    "title": "Bayesian Panel Methods",
    "section": "Shrinkage and Partial Pooling",
    "text": "Shrinkage and Partial Pooling\nThe posterior for \\(\\beta_i\\) combines:\n\nCountry-specific data (likelihood)\nCross-country information (hierarchical prior)\n\n\\[\n\\hat{\\beta}_i^{\\text{Bayes}} = \\lambda_i \\hat{\\beta}_i^{\\text{OLS}} + (1 - \\lambda_i) \\bar{\\beta}\n\\]\nwhere \\(\\lambda_i \\in [0,1]\\) depends on: - Precision of country data: More data → less shrinkage - Cross-country heterogeneity: More variance in \\(\\Omega\\) → less shrinkage\n\n\nCode\n# Simulate hierarchical data\nN &lt;- 20  # countries\nT_obs &lt;- 50  # periods per country\n\n# True hierarchical structure\nmu_true &lt;- 0.5\nomega_true &lt;- 0.3^2\nsigma_true &lt;- 1\n\n# Country-specific coefficients\nbeta_true &lt;- rnorm(N, mu_true, sqrt(omega_true))\n\n# Generate data\ndata_hier &lt;- map_dfr(1:N, function(i) {\n  x &lt;- rnorm(T_obs)\n  y &lt;- beta_true[i] * x + rnorm(T_obs, 0, sigma_true)\n  tibble(country = i, x = x, y = y)\n})\n\n# OLS estimates by country\nols_estimates &lt;- data_hier %&gt;%\n  group_by(country) %&gt;%\n  summarise(\n    beta_ols = coef(lm(y ~ x - 1))[1],\n    se_ols = summary(lm(y ~ x - 1))$coefficients[1, 2],\n    n = n()\n  )\n\n# Hierarchical shrinkage (empirical Bayes approximation)\ngrand_mean &lt;- mean(ols_estimates$beta_ols)\nbetween_var &lt;- var(ols_estimates$beta_ols)\nwithin_var &lt;- mean(ols_estimates$se_ols^2)\n\nshrinkage_factor &lt;- between_var / (between_var + within_var / T_obs)\n\nols_estimates &lt;- ols_estimates %&gt;%\n  mutate(\n    beta_hier = shrinkage_factor * beta_ols + (1 - shrinkage_factor) * grand_mean,\n    beta_true = beta_true\n  )\n\n# Plot\nggplot(ols_estimates) +\n  geom_segment(aes(x = beta_ols, xend = beta_hier, y = country, yend = country),\n               arrow = arrow(length = unit(0.1, \"cm\")), color = \"gray50\") +\n  geom_point(aes(x = beta_ols, y = country), color = \"#e74c3c\", size = 2) +\n  geom_point(aes(x = beta_hier, y = country), color = \"#3498db\", size = 2) +\n  geom_point(aes(x = beta_true, y = country), shape = 4, size = 2, color = \"#2ecc71\") +\n  geom_vline(xintercept = grand_mean, linetype = \"dashed\") +\n  annotate(\"text\", x = grand_mean + 0.1, y = N + 1, label = \"Global mean\") +\n  labs(title = \"Hierarchical Shrinkage in Action\",\n       subtitle = \"Red = OLS, Blue = Hierarchical, Green × = True\",\n       x = expression(beta[i]), y = \"Country\") +\n  theme_minimal()\n\n\n\n\n\nHierarchical shrinkage: country estimates pulled toward global mean",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#why-shrinkage-helps",
    "href": "chapters/09_bayesian_panel.html#why-shrinkage-helps",
    "title": "Bayesian Panel Methods",
    "section": "Why Shrinkage Helps",
    "text": "Why Shrinkage Helps\nKey Insight: Countries with less precise estimates get shrunk more toward the global mean.\n\n\n\nCountry Type\nData Quality\nShrinkage\nResult\n\n\n\n\nData-rich (US, Germany)\nHigh\nLow\nNear OLS\n\n\nData-poor (small EMs)\nLow\nHigh\nNear global mean\n\n\n\nThis is not bias — it’s optimal use of information under the hierarchical structure.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#the-algorithm",
    "href": "chapters/09_bayesian_panel.html#the-algorithm",
    "title": "Bayesian Panel Methods",
    "section": "The Algorithm",
    "text": "The Algorithm\nFor the hierarchical linear model:\nBlock 1: Draw \\(\\beta_i | y_i, \\mu, \\Omega, \\sigma^2_i\\) for each country \\[\n\\beta_i | \\cdot \\sim N\\left( V_i^{-1}(X_i'y_i/\\sigma^2_i + \\Omega^{-1}\\mu), V_i^{-1} \\right)\n\\] where \\(V_i = X_i'X_i/\\sigma^2_i + \\Omega^{-1}\\)\nBlock 2: Draw \\(\\sigma^2_i | y_i, \\beta_i\\) \\[\n\\sigma^2_i | \\cdot \\sim \\text{Inverse-Gamma}\\left( \\frac{T_i}{2}, \\frac{(y_i - X_i\\beta_i)'(y_i - X_i\\beta_i)}{2} \\right)\n\\]\nBlock 3: Draw \\(\\mu | \\beta_1, \\ldots, \\beta_N, \\Omega\\) \\[\n\\mu | \\cdot \\sim N\\left( (N\\Omega^{-1} + V_0^{-1})^{-1}(N\\Omega^{-1}\\bar{\\beta} + V_0^{-1}\\mu_0), (N\\Omega^{-1} + V_0^{-1})^{-1} \\right)\n\\]\nBlock 4: Draw \\(\\Omega | \\beta_1, \\ldots, \\beta_N, \\mu\\) \\[\n\\Omega | \\cdot \\sim \\text{Inverse-Wishart}\\left( \\Omega_0 + \\sum_{i=1}^N (\\beta_i - \\mu)(\\beta_i - \\mu)', \\nu_0 + N \\right)\n\\]\n\n\nCode\n# Hierarchical Gibbs sampler for panel regression\nhierarchical_panel_gibbs &lt;- function(data, n_draw = 5000, n_burn = 1000) {\n\n  countries &lt;- unique(data$country)\n  N &lt;- length(countries)\n  K &lt;- 1  # number of regressors (simplified)\n\n  # Organize data by country\n  country_data &lt;- map(countries, function(c) {\n    d &lt;- filter(data, country == c)\n    list(y = d$y, X = matrix(d$x, ncol = 1), T_i = nrow(d))\n  })\n\n  # Priors\n  mu0 &lt;- 0\n  V0 &lt;- 10\n  omega0 &lt;- 1\n  nu0 &lt;- 3\n  a0 &lt;- 0.01; b0 &lt;- 0.01  # for sigma^2\n\n  # Storage\n  beta_draws &lt;- array(0, dim = c(N, K, n_draw))\n  mu_draws &lt;- matrix(0, n_draw, K)\n  omega_draws &lt;- numeric(n_draw)\n  sigma2_draws &lt;- matrix(0, n_draw, N)\n\n  # Initialize\n  beta_curr &lt;- rep(0, N)\n  sigma2_curr &lt;- rep(1, N)\n  mu_curr &lt;- 0\n  omega_curr &lt;- 1\n\n  for (g in 1:(n_draw + n_burn)) {\n\n    # Block 1: Draw beta_i | rest\n    for (i in 1:N) {\n      X_i &lt;- country_data[[i]]$X\n      y_i &lt;- country_data[[i]]$y\n      T_i &lt;- country_data[[i]]$T_i\n\n      V_i &lt;- as.numeric(crossprod(X_i) / sigma2_curr[i] + 1/omega_curr)\n      m_i &lt;- (crossprod(X_i, y_i) / sigma2_curr[i] + mu_curr/omega_curr) / V_i\n      beta_curr[i] &lt;- rnorm(1, m_i, sqrt(1/V_i))\n    }\n\n    # Block 2: Draw sigma2_i | rest\n    for (i in 1:N) {\n      X_i &lt;- country_data[[i]]$X\n      y_i &lt;- country_data[[i]]$y\n      T_i &lt;- country_data[[i]]$T_i\n\n      resid &lt;- y_i - X_i * beta_curr[i]\n      a_post &lt;- a0 + T_i/2\n      b_post &lt;- b0 + sum(resid^2)/2\n      sigma2_curr[i] &lt;- 1/rgamma(1, a_post, b_post)\n    }\n\n    # Block 3: Draw mu | rest\n    V_mu &lt;- 1 / (N/omega_curr + 1/V0)\n    m_mu &lt;- V_mu * (sum(beta_curr)/omega_curr + mu0/V0)\n    mu_curr &lt;- rnorm(1, m_mu, sqrt(V_mu))\n\n    # Block 4: Draw omega | rest\n    nu_post &lt;- nu0 + N\n    S_post &lt;- omega0 + sum((beta_curr - mu_curr)^2)\n    omega_curr &lt;- 1/rgamma(1, nu_post/2, S_post/2)\n\n    # Store\n    if (g &gt; n_burn) {\n      idx &lt;- g - n_burn\n      beta_draws[, , idx] &lt;- beta_curr\n      mu_draws[idx, ] &lt;- mu_curr\n      omega_draws[idx] &lt;- omega_curr\n      sigma2_draws[idx, ] &lt;- sigma2_curr\n    }\n  }\n\n  list(beta = beta_draws, mu = mu_draws, omega = omega_draws, sigma2 = sigma2_draws)\n}\n\n# Run on simulated data\nhier_result &lt;- hierarchical_panel_gibbs(data_hier, n_draw = 3000, n_burn = 1000)\n\n# Posterior summary\nbeta_post_mean &lt;- apply(hier_result$beta, 1, mean)\nbeta_post_sd &lt;- apply(hier_result$beta, 1, sd)\n\n# Compare to truth\ncomparison &lt;- tibble(\n  country = 1:N,\n  true = beta_true,\n  ols = ols_estimates$beta_ols,\n  bayes_mean = beta_post_mean,\n  bayes_sd = beta_post_sd\n)\n\n# Plot posterior distributions for selected countries\nselected &lt;- c(1, 5, 10, 15)\npost_df &lt;- map_dfr(selected, function(i) {\n  tibble(\n    country = paste(\"Country\", i),\n    value = hier_result$beta[i, 1, ],\n    true = beta_true[i],\n    ols = ols_estimates$beta_ols[i]\n  )\n})\n\nggplot(post_df, aes(x = value)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"#3498db\", alpha = 0.7) +\n  geom_vline(aes(xintercept = true), color = \"#2ecc71\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(aes(xintercept = ols), color = \"#e74c3c\", linetype = \"dotted\", linewidth = 1) +\n  facet_wrap(~country, scales = \"free_y\") +\n  labs(title = \"Posterior Distributions of Country-Specific Coefficients\",\n       subtitle = \"Green dashed = True, Red dotted = OLS\",\n       x = expression(beta[i]), y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\nPosterior distributions from hierarchical panel model",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#the-model-structure",
    "href": "chapters/09_bayesian_panel.html#the-model-structure",
    "title": "Bayesian Panel Methods",
    "section": "The Model Structure",
    "text": "The Model Structure\nFor N countries, each with a K-variable VAR(p):\n\\[\ny_{it} = c_i + A_{i,1} y_{i,t-1} + \\cdots + A_{i,p} y_{i,t-p} + u_{it}\n\\]\nHierarchical Prior: \\[\n\\text{vec}(B_i) | \\mu_B, \\Omega_B \\sim N(\\mu_B, \\Omega_B)\n\\]\nwhere \\(B_i = [c_i, A_{i,1}, \\ldots, A_{i,p}]'\\) stacks all VAR coefficients for country \\(i\\).",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#why-panel-var",
    "href": "chapters/09_bayesian_panel.html#why-panel-var",
    "title": "Bayesian Panel Methods",
    "section": "Why Panel VAR?",
    "text": "Why Panel VAR?\n\n\n\nFeature\nSeparate VARs\nPooled VAR\nPanel VAR\n\n\n\n\nHeterogeneity\nYes\nNo\nYes\n\n\nSample size\nT per country\nN×T\nN×T (shared info)\n\n\nCurse of dimensionality\nSevere\nModerate\nMitigated\n\n\nInterpretation\nCountry-specific\nGlobal average\nBoth\n\n\n\nWith K = 3 variables, p = 2 lags, and T = 84: - Parameters per country: 3 × (1 + 3×2) = 21 - Observations per country: 84 - 2 = 82 - Degrees of freedom: Tight!\nHierarchical pooling provides effective sample size boost without forcing homogeneity.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#mean-group-prior-structure",
    "href": "chapters/09_bayesian_panel.html#mean-group-prior-structure",
    "title": "Bayesian Panel Methods",
    "section": "Mean-Group Prior Structure",
    "text": "Mean-Group Prior Structure\nFollowing Canova & Ciccarelli (2009), we can decompose:\n\\[\nB_i = \\underbrace{\\bar{B}}_{\\text{common}} + \\underbrace{\\tilde{B}_i}_{\\text{country-specific}} + \\underbrace{Z_i \\gamma}_{\\text{observed heterogeneity}}\n\\]\nwhere: - \\(\\bar{B}\\): Common component (global dynamics) - \\(\\tilde{B}_i\\): Idiosyncratic deviation - \\(Z_i\\): Country characteristics (GDP per capita, openness, institutions)\n\n\nCode\n# Illustrate panel VAR structure\npanel_var_diagram &lt;- tibble(\n  level = c(\"Data\", \"Data\", \"Country VAR\", \"Country VAR\", \"Hierarchical\", \"Hyperprior\"),\n  component = c(\"y_it (Country 1)\", \"y_it (Country N)\",\n                \"B_1 ~ N(mu_B, Omega)\", \"B_N ~ N(mu_B, Omega)\",\n                \"mu_B, Omega\", \"mu_0, V_0, Omega_0, nu_0\"),\n  x = c(1, 3, 1, 3, 2, 2),\n  y = c(1, 1, 2, 2, 3, 4)\n)\n\nggplot(panel_var_diagram, aes(x = x, y = y, label = component)) +\n  geom_point(size = 10, color = \"#3498db\", alpha = 0.3) +\n  geom_text(size = 3) +\n  geom_segment(aes(x = 1, xend = 1, y = 1.2, yend = 1.8),\n               arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_segment(aes(x = 3, xend = 3, y = 1.2, yend = 1.8),\n               arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_segment(aes(x = 1, xend = 1.8, y = 2.2, yend = 2.8),\n               arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_segment(aes(x = 3, xend = 2.2, y = 2.2, yend = 2.8),\n               arrow = arrow(length = unit(0.2, \"cm\"))) +\n  geom_segment(aes(x = 2, xend = 2, y = 3.2, yend = 3.8),\n               arrow = arrow(length = unit(0.2, \"cm\"))) +\n  xlim(0, 4) + ylim(0.5, 4.5) +\n  labs(title = \"Hierarchical Panel VAR Structure\",\n       subtitle = \"Information flows up (estimation) and down (shrinkage)\") +\n  theme_void()\n\n\n\n\n\nPanel VAR structure for cross-country macro analysis",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#gibbs-sampler-for-panel-var",
    "href": "chapters/09_bayesian_panel.html#gibbs-sampler-for-panel-var",
    "title": "Bayesian Panel Methods",
    "section": "Gibbs Sampler for Panel VAR",
    "text": "Gibbs Sampler for Panel VAR\n\n\nCode\n# Simplified Panel VAR Gibbs sampler\n# For K=2 variables, p=1 lag, demonstration purposes\n\npanel_var_gibbs &lt;- function(Y_list, p = 1, n_draw = 3000, n_burn = 1000,\n                             lambda1 = 0.2) {\n\n  N &lt;- length(Y_list)  # number of countries\n  K &lt;- ncol(Y_list[[1]])  # number of variables\n  M &lt;- K * p + 1  # regressors per equation\n\n  # Build data matrices for each country\n  data_list &lt;- map(Y_list, function(Y) {\n    T_full &lt;- nrow(Y)\n    y &lt;- Y[(p+1):T_full, , drop = FALSE]\n    X &lt;- matrix(1, nrow = T_full - p, ncol = 1)\n    for (j in 1:p) {\n      X &lt;- cbind(X, Y[(p+1-j):(T_full-j), , drop = FALSE])\n    }\n    list(y = y, X = X, T_eff = nrow(y))\n  })\n\n  # Hierarchical priors\n  # Level 2: mu_B ~ N(0, 10*I), Omega ~ IW(I, K+2)\n  mu_B_prior &lt;- rep(0, M * K)\n  V_mu_prior &lt;- diag(10, M * K)\n  Omega_prior &lt;- diag(1, M * K)\n  nu_Omega_prior &lt;- M * K + 2\n\n  # Storage\n  B_draws &lt;- array(0, dim = c(M, K, N, n_draw))\n  mu_B_draws &lt;- matrix(0, n_draw, M * K)\n\n  # Initialize\n  B_curr &lt;- array(0, dim = c(M, K, N))\n  for (i in 1:N) {\n    B_curr[,,i] &lt;- solve(crossprod(data_list[[i]]$X)) %*%\n                   crossprod(data_list[[i]]$X, data_list[[i]]$y)\n  }\n  Sigma_curr &lt;- lapply(1:N, function(i) {\n    U &lt;- data_list[[i]]$y - data_list[[i]]$X %*% B_curr[,,i]\n    crossprod(U) / data_list[[i]]$T_eff\n  })\n  mu_B_curr &lt;- apply(B_curr, c(1,2), mean) %&gt;% as.vector()\n  Omega_curr &lt;- diag(lambda1^2, M * K)\n\n  for (g in 1:(n_draw + n_burn)) {\n\n    # Block 1: Draw B_i | Sigma_i, mu_B, Omega for each country\n    for (i in 1:N) {\n      X_i &lt;- data_list[[i]]$X\n      y_i &lt;- data_list[[i]]$y\n      Sigma_inv &lt;- solve(Sigma_curr[[i]])\n      Omega_inv &lt;- solve(Omega_curr)\n\n      V_post_inv &lt;- Omega_inv + kronecker(Sigma_inv, crossprod(X_i))\n      V_post &lt;- solve(V_post_inv)\n      b_post &lt;- V_post %*% (Omega_inv %*% mu_B_curr +\n                            kronecker(Sigma_inv, t(X_i)) %*% as.vector(y_i))\n      B_curr[,,i] &lt;- matrix(rmvnorm(1, b_post, V_post), M, K)\n    }\n\n    # Block 2: Draw Sigma_i | B_i for each country\n    for (i in 1:N) {\n      U &lt;- data_list[[i]]$y - data_list[[i]]$X %*% B_curr[,,i]\n      S_post &lt;- diag(K) + crossprod(U)\n      Sigma_curr[[i]] &lt;- solve(rWishart(1, K + 2 + data_list[[i]]$T_eff,\n                                         solve(S_post))[,,1])\n    }\n\n    # Block 3: Draw mu_B | B_1,...,B_N, Omega\n    B_vec &lt;- sapply(1:N, function(i) as.vector(B_curr[,,i]))\n    B_mean &lt;- rowMeans(B_vec)\n    Omega_inv &lt;- solve(Omega_curr)\n    V_mu_inv &lt;- solve(V_mu_prior)\n    V_mu_post &lt;- solve(N * Omega_inv + V_mu_inv)\n    m_mu_post &lt;- V_mu_post %*% (N * Omega_inv %*% B_mean + V_mu_inv %*% mu_B_prior)\n    mu_B_curr &lt;- as.vector(rmvnorm(1, m_mu_post, V_mu_post))\n\n    # Block 4: Draw Omega | B_1,...,B_N, mu_B\n    S_Omega &lt;- Omega_prior\n    for (i in 1:N) {\n      b_i &lt;- as.vector(B_curr[,,i])\n      S_Omega &lt;- S_Omega + outer(b_i - mu_B_curr, b_i - mu_B_curr)\n    }\n    Omega_curr &lt;- solve(rWishart(1, nu_Omega_prior + N, solve(S_Omega))[,,1])\n\n    # Store\n    if (g &gt; n_burn) {\n      idx &lt;- g - n_burn\n      B_draws[,,,idx] &lt;- B_curr\n      mu_B_draws[idx,] &lt;- mu_B_curr\n    }\n\n    if (g %% 500 == 0) cat(\"Draw\", g, \"of\", n_draw + n_burn, \"\\n\")\n  }\n\n  list(B = B_draws, mu_B = mu_B_draws, K = K, p = p, M = M, N = N)\n}\n\n# Generate synthetic panel VAR data\nset.seed(123)\nN_countries &lt;- 10\nT_obs &lt;- 60\nK &lt;- 2\n\n# Common VAR structure with heterogeneity\nA_common &lt;- matrix(c(0.7, 0.1, 0.05, 0.6), K, K)\nSigma_common &lt;- matrix(c(1, 0.3, 0.3, 1), K, K)\n\nY_list &lt;- map(1:N_countries, function(i) {\n  # Country-specific deviation\n  A_i &lt;- A_common + matrix(rnorm(K^2, 0, 0.1), K, K)\n\n  Y &lt;- matrix(0, T_obs, K)\n  Y[1, ] &lt;- rnorm(K)\n  for (t in 2:T_obs) {\n    Y[t, ] &lt;- Y[t-1, ] %*% t(A_i) + rmvnorm(1, sigma = Sigma_common)\n  }\n  Y\n})\n\n# Estimate panel VAR\ncat(\"Estimating Panel VAR...\\n\")\n\n\nEstimating Panel VAR...\n\n\nCode\npvar_result &lt;- panel_var_gibbs(Y_list, p = 1, n_draw = 2000, n_burn = 500)\n\n\nDraw 500 of 2500 \nDraw 1000 of 2500 \nDraw 1500 of 2500 \nDraw 2000 of 2500 \nDraw 2500 of 2500 \n\n\n\n\nCode\n# Extract A[1,1] coefficient for each country\nA11_draws &lt;- pvar_result$B[2, 1, , ]  # position [2,1] is A[1,1] after constant\n\n# Posterior summaries\nA11_summary &lt;- tibble(\n  country = 1:N_countries,\n  mean = apply(A11_draws, 1, mean),\n  lower = apply(A11_draws, 1, quantile, 0.16),\n  upper = apply(A11_draws, 1, quantile, 0.84)\n)\n\n# Add global mean\nmu_A11 &lt;- mean(pvar_result$mu_B[, 2])  # position 2 in vec(B) is A[1,1]\n\nggplot(A11_summary, aes(x = factor(country), y = mean)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), color = \"#3498db\") +\n  geom_hline(yintercept = mu_A11, linetype = \"dashed\", color = \"#e74c3c\") +\n  geom_hline(yintercept = A_common[1,1], linetype = \"dotted\", color = \"#2ecc71\") +\n  annotate(\"text\", x = 9, y = mu_A11 + 0.05, label = \"Hierarchical mean\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 9, y = A_common[1,1] - 0.05, label = \"True common\", color = \"#2ecc71\") +\n  labs(title = \"Country-Specific VAR Coefficients with Hierarchical Shrinkage\",\n       subtitle = expression(paste(\"Posterior of \", A[11], \" across countries\")),\n       x = \"Country\", y = expression(A[11])) +\n  theme_minimal()\n\n\n\n\n\nPanel VAR coefficient posteriors across countries",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#swamy-1970-random-coefficients",
    "href": "chapters/09_bayesian_panel.html#swamy-1970-random-coefficients",
    "title": "Bayesian Panel Methods",
    "section": "Swamy (1970) Random Coefficients",
    "text": "Swamy (1970) Random Coefficients\nThe classic frequentist approach:\n\\[\ny_{it} = x_{it}'\\beta_i + \\varepsilon_{it}\n\\] \\[\n\\beta_i = \\bar{\\beta} + v_i, \\quad v_i \\sim N(0, \\Omega)\n\\]\nBayesian version adds priors on \\((\\bar{\\beta}, \\Omega)\\) and uses Gibbs sampling.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#correlated-random-coefficients",
    "href": "chapters/09_bayesian_panel.html#correlated-random-coefficients",
    "title": "Bayesian Panel Methods",
    "section": "Correlated Random Coefficients",
    "text": "Correlated Random Coefficients\nAllow \\(\\beta_i\\) to correlate with observables:\n\\[\n\\beta_i = \\bar{\\beta} + Z_i'\\gamma + v_i\n\\]\nwhere \\(Z_i\\) are country characteristics (GDP per capita, openness, institutions).\nApplication: Countries with higher institutional quality might have different Taylor rule coefficients.\n\n\nCode\n# Simulate correlated random coefficients\nN &lt;- 30\nT_obs &lt;- 40\n\n# Country characteristic\nz_i &lt;- rnorm(N)  # e.g., log GDP per capita\n\n# Coefficient depends on characteristic\ngamma &lt;- 0.3\nbeta_i &lt;- 0.5 + gamma * z_i + rnorm(N, 0, 0.2)\n\n# Generate panel\nrc_data &lt;- map_dfr(1:N, function(i) {\n  x &lt;- rnorm(T_obs)\n  y &lt;- beta_i[i] * x + rnorm(T_obs, 0, 1)\n  tibble(country = i, x = x, y = y, z = z_i[i])\n})\n\n# OLS by country\nols_rc &lt;- rc_data %&gt;%\n  group_by(country) %&gt;%\n  summarise(beta_ols = coef(lm(y ~ x - 1))[1], z = first(z))\n\n# Plot relationship\nggplot(ols_rc, aes(x = z, y = beta_ols)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#3498db\") +\n  geom_abline(intercept = 0.5, slope = gamma, linetype = \"dashed\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 1.5, y = 0.5 + gamma * 1.5 + 0.1,\n           label = \"True relationship\", color = \"#e74c3c\") +\n  labs(title = \"Correlated Random Coefficients\",\n       subtitle = \"Country characteristic explains coefficient heterogeneity\",\n       x = \"Country characteristic (z)\", y = expression(hat(beta)[i])) +\n  theme_minimal()\n\n\n\n\n\nRandom coefficients with observable heterogeneity",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#the-problem",
    "href": "chapters/09_bayesian_panel.html#the-problem",
    "title": "Bayesian Panel Methods",
    "section": "The Problem",
    "text": "The Problem\nStandard panel methods assume: \\[\nE[\\varepsilon_{it} \\varepsilon_{js}] = 0 \\quad \\text{for } i \\neq j\n\\]\nBut in macro panels, global shocks (oil prices, US monetary policy, COVID) create cross-sectional dependence.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#factor-structure-approach",
    "href": "chapters/09_bayesian_panel.html#factor-structure-approach",
    "title": "Bayesian Panel Methods",
    "section": "Factor Structure Approach",
    "text": "Factor Structure Approach\nModel the error as: \\[\n\\varepsilon_{it} = \\lambda_i' f_t + e_{it}\n\\]\nwhere: - \\(f_t\\): Common factors (K_f × 1) - \\(\\lambda_i\\): Country-specific factor loadings - \\(e_{it}\\): Idiosyncratic error\n\nInteractive Fixed Effects (Bai 2009)\n\\[\ny_{it} = x_{it}'\\beta + \\lambda_i' f_t + e_{it}\n\\]\nJointly estimates \\(\\beta\\), \\(\\Lambda = [\\lambda_1, \\ldots, \\lambda_N]'\\), and \\(F = [f_1, \\ldots, f_T]'\\).\n\n\nBayesian Factor Model\nPrior structure: \\[\nf_t | \\Sigma_f \\sim N(0, \\Sigma_f), \\quad \\lambda_i \\sim N(0, \\Omega_\\lambda)\n\\]\nIdentification: Impose lower-triangular structure on loadings (first K_f countries).\n\n\nCode\n# Simulate panel with common factor\nN &lt;- 20\nT_obs &lt;- 50\nK_f &lt;- 1  # one common factor\n\n# Common factor (e.g., global financial cycle)\nf_t &lt;- arima.sim(model = list(ar = 0.8), n = T_obs) %&gt;% as.vector()\n\n# Country loadings\nlambda_i &lt;- rnorm(N, mean = 0.5, sd = 0.3)\n\n# Generate correlated errors\nerrors &lt;- outer(f_t, lambda_i) + matrix(rnorm(T_obs * N, 0, 0.5), T_obs, N)\n\n# True coefficient\nbeta_true &lt;- 0.8\n\n# Generate data\nfactor_data &lt;- map_dfr(1:N, function(i) {\n  x &lt;- rnorm(T_obs)\n  y &lt;- beta_true * x + errors[, i]\n  tibble(country = i, t = 1:T_obs, x = x, y = y, factor = f_t)\n})\n\n# Show cross-correlation of residuals\nresids &lt;- factor_data %&gt;%\n  group_by(country) %&gt;%\n  mutate(resid = residuals(lm(y ~ x))) %&gt;%\n  select(country, t, resid) %&gt;%\n  pivot_wider(names_from = country, values_from = resid, names_prefix = \"c\")\n\n# Correlation matrix of residuals\nresid_mat &lt;- as.matrix(resids[, -1])\ncor_mat &lt;- cor(resid_mat)\n\n# Visualize\ncor_df &lt;- expand_grid(i = 1:N, j = 1:N) %&gt;%\n  mutate(correlation = as.vector(cor_mat))\n\nggplot(cor_df, aes(x = factor(i), y = factor(j), fill = correlation)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#3498db\", mid = \"white\", high = \"#e74c3c\", midpoint = 0) +\n  labs(title = \"Cross-Sectional Correlation of Residuals\",\n       subtitle = \"Common factor creates off-diagonal correlations\",\n       x = \"Country i\", y = \"Country j\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 8))\n\n\n\n\n\nCross-sectional dependence via common factor",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#bayesian-factor-augmented-panel-var",
    "href": "chapters/09_bayesian_panel.html#bayesian-factor-augmented-panel-var",
    "title": "Bayesian Panel Methods",
    "section": "Bayesian Factor-Augmented Panel VAR",
    "text": "Bayesian Factor-Augmented Panel VAR\nCombine hierarchical panel VAR with factor structure:\n\\[\ny_{it} = c_i + A_i y_{i,t-1} + \\Gamma_i f_t + u_{it}\n\\]\nwhere \\(f_t\\) is a vector of global factors (extracted or observed).\nApplication in macro: Include Global Financial Cycle (GFC) factor from Miranda-Agrippino & Rey.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#marginal-likelihood",
    "href": "chapters/09_bayesian_panel.html#marginal-likelihood",
    "title": "Bayesian Panel Methods",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\nFor hierarchical models, the marginal likelihood integrates over all parameters:\n\\[\np(Y | M) = \\int p(Y | \\theta) p(\\theta | M) d\\theta\n\\]\nFor linear hierarchical models: Often tractable analytically.\nFor complex models: Use bridge sampling or the Chib (1995) method.",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#dic-and-waic",
    "href": "chapters/09_bayesian_panel.html#dic-and-waic",
    "title": "Bayesian Panel Methods",
    "section": "DIC and WAIC",
    "text": "DIC and WAIC\n\n\n\n\n\n\n\n\nCriterion\nFormula\nUse\n\n\n\n\nDIC\n\\(\\bar{D} + p_D\\)\nEffective parameters penalty\n\n\nWAIC\nPointwise predictive density\nBetter for hierarchical models\n\n\nLOO-CV\nLeave-one-out cross-validation\nGold standard but expensive\n\n\n\n\n\nCode\n# DIC calculation for hierarchical model\ncalculate_dic &lt;- function(hier_result, data_hier) {\n\n  countries &lt;- unique(data_hier$country)\n  N &lt;- length(countries)\n  n_draw &lt;- dim(hier_result$beta)[3]\n\n  # Compute log-likelihood at each draw\n  log_lik_draws &lt;- numeric(n_draw)\n\n  for (g in 1:n_draw) {\n    ll &lt;- 0\n    for (i in 1:N) {\n      d &lt;- filter(data_hier, country == i)\n      beta_g &lt;- hier_result$beta[i, 1, g]\n      sigma2_g &lt;- hier_result$sigma2[g, i]\n\n      resid &lt;- d$y - d$x * beta_g\n      ll &lt;- ll + sum(dnorm(resid, 0, sqrt(sigma2_g), log = TRUE))\n    }\n    log_lik_draws[g] &lt;- ll\n  }\n\n  # DIC components\n  D_bar &lt;- -2 * mean(log_lik_draws)  # posterior mean deviance\n\n  # Deviance at posterior mean\n  ll_at_mean &lt;- 0\n  for (i in 1:N) {\n    d &lt;- filter(data_hier, country == i)\n    beta_mean &lt;- mean(hier_result$beta[i, 1, ])\n    sigma2_mean &lt;- mean(hier_result$sigma2[, i])\n    resid &lt;- d$y - d$x * beta_mean\n    ll_at_mean &lt;- ll_at_mean + sum(dnorm(resid, 0, sqrt(sigma2_mean), log = TRUE))\n  }\n  D_theta_bar &lt;- -2 * ll_at_mean\n\n  # Effective parameters\n  p_D &lt;- D_bar - D_theta_bar\n\n  # DIC\n  DIC &lt;- D_bar + p_D\n\n  list(DIC = DIC, D_bar = D_bar, p_D = p_D)\n}\n\ndic_result &lt;- calculate_dic(hier_result, data_hier)\ncat(\"DIC:\", round(dic_result$DIC, 1), \"\\n\")\n\n\nDIC: 2873.5 \n\n\nCode\ncat(\"Effective parameters (p_D):\", round(dic_result$p_D, 1), \"\\n\")\n\n\nEffective parameters (p_D): 36.9",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#when-to-use-hierarchical-vs.-standard-panel",
    "href": "chapters/09_bayesian_panel.html#when-to-use-hierarchical-vs.-standard-panel",
    "title": "Bayesian Panel Methods",
    "section": "When to Use Hierarchical vs. Standard Panel",
    "text": "When to Use Hierarchical vs. Standard Panel\n\n\n\nSituation\nRecommendation\n\n\n\n\nSmall T per country (&lt; 30)\nHierarchical (shrinkage helps)\n\n\nLarge N, moderate T\nHierarchical (efficient)\n\n\nFocus on average effect\nEither works\n\n\nFocus on heterogeneity\nHierarchical (full posterior)\n\n\nStrong prior beliefs\nBayesian hierarchical",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#implementation-checklist",
    "href": "chapters/09_bayesian_panel.html#implementation-checklist",
    "title": "Bayesian Panel Methods",
    "section": "Implementation Checklist",
    "text": "Implementation Checklist\n\nCheck MCMC convergence: Trace plots, R-hat, effective sample size\nAssess shrinkage: Compare hierarchical to OLS estimates\nTest homogeneity: Is \\(\\Omega\\) small relative to coefficient magnitudes?\nCross-sectional dependence: Test residuals, consider factors\nModel comparison: DIC/WAIC across specifications",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#common-pitfalls",
    "href": "chapters/09_bayesian_panel.html#common-pitfalls",
    "title": "Bayesian Panel Methods",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\n\n\n\n\n\nWarningWatch Out For\n\n\n\n\nOver-shrinkage: Very tight hyperpriors force all countries to look identical\nUnder-shrinkage: Vague hyperpriors fail to borrow strength\nIgnoring cross-sectional dependence: Biases standard errors downward\nToo many parameters: K-variable panel VAR with p lags has \\(N \\times K \\times (Kp + 1)\\) parameters\nImproper mixing: Hierarchical models can have slow mixing — increase draws",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#hierarchical-panel-models",
    "href": "chapters/09_bayesian_panel.html#hierarchical-panel-models",
    "title": "Bayesian Panel Methods",
    "section": "Hierarchical Panel Models",
    "text": "Hierarchical Panel Models\n\nGelman & Hill (2007) Data Analysis Using Regression and Multilevel/Hierarchical Models — Accessible introduction\nSwamy (1970) “Efficient Inference in a Random Coefficient Regression Model” Econometrica — Classic random coefficients\nHsiao (2014) Analysis of Panel Data — Comprehensive textbook",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#bayesian-panel-var-1",
    "href": "chapters/09_bayesian_panel.html#bayesian-panel-var-1",
    "title": "Bayesian Panel Methods",
    "section": "Bayesian Panel VAR",
    "text": "Bayesian Panel VAR\n\nCanova & Ciccarelli (2009) “Estimating Multicountry VAR Models” IER — Foundational paper\nCanova & Ciccarelli (2013) “Panel Vector Autoregressive Models: A Survey” — Survey chapter\nJarocinski (2010) “Responses to Monetary Policy Shocks in the East and the West of Europe” JAE — Application",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#cross-sectional-dependence-1",
    "href": "chapters/09_bayesian_panel.html#cross-sectional-dependence-1",
    "title": "Bayesian Panel Methods",
    "section": "Cross-Sectional Dependence",
    "text": "Cross-Sectional Dependence\n\nBai (2009) “Panel Data Models with Interactive Fixed Effects” Econometrica — Factor structure\nPesaran (2006) “Estimation and Inference in Large Heterogeneous Panels” Econometrica — CCE estimator\nChudik & Pesaran (2015) “Common Correlated Effects Estimation of Heterogeneous Dynamic Panel Data Models” JoE",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/09_bayesian_panel.html#software",
    "href": "chapters/09_bayesian_panel.html#software",
    "title": "Bayesian Panel Methods",
    "section": "Software",
    "text": "Software\n\nR: brms (Bürkner), lme4 (Bates et al.), custom Gibbs samplers\nPython: PyMC, NumPy/SciPy for manual MCMC\nStan: rstan/pystan for complex hierarchical models",
    "crumbs": [
      "Phase 4: Bayesian Econometrics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Panel Methods</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html",
    "href": "chapters/10_dsge_foundations.html",
    "title": "DSGE Foundations",
    "section": "",
    "text": "What is a DSGE Model?\nDynamic Stochastic General Equilibrium models are the workhorse of modern macroeconomics. They combine:",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#from-micro-to-macro",
    "href": "chapters/10_dsge_foundations.html#from-micro-to-macro",
    "title": "DSGE Foundations",
    "section": "From Micro to Macro",
    "text": "From Micro to Macro\n\n\n\n\n\nDSGE model structure",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#the-general-form",
    "href": "chapters/10_dsge_foundations.html#the-general-form",
    "title": "DSGE Foundations",
    "section": "The General Form",
    "text": "The General Form\nAll DSGE models can be written as:\n\\[\n\\mathbb{E}_t\\left[f(y_{t+1}, y_t, y_{t-1}, u_t)\\right] = 0\n\\]\nwhere:\n\n\\(y_t\\) = vector of endogenous variables (output, inflation, interest rate, capital, …)\n\\(u_t\\) = vector of exogenous shocks (technology, monetary, fiscal, …)\n\\(f\\) = system of equilibrium conditions (Euler equations, market clearing, …)\n\\(\\mathbb{E}_t\\) = expectation conditional on time-\\(t\\) information\n\nThe challenge: solving for \\(y_t\\) as a function of states and shocks.",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#household-problem",
    "href": "chapters/10_dsge_foundations.html#household-problem",
    "title": "DSGE Foundations",
    "section": "Household Problem",
    "text": "Household Problem\nA representative household maximizes expected lifetime utility:\n\\[\n\\max_{\\{C_t, N_t, K_t\\}} \\mathbb{E}_0 \\sum_{t=0}^{\\infty} \\beta^t U(C_t, N_t)\n\\]\nSubject to budget constraint: \\[\nC_t + K_t = W_t N_t + R_t^K K_{t-1} + \\Pi_t - T_t + (1-\\delta)K_{t-1}\n\\]\nwhere: - \\(C_t\\) = consumption - \\(N_t\\) = labor supply - \\(K_t\\) = end-of-period capital - \\(W_t\\) = real wage - \\(R_t^K\\) = rental rate on capital - \\(\\delta\\) = depreciation rate - \\(\\Pi_t\\) = firm profits - \\(T_t\\) = lump-sum taxes\n\nFirst-Order Conditions\nEuler equation (intertemporal consumption choice): \\[\nU_C(C_t, N_t) = \\beta \\mathbb{E}_t\\left[U_C(C_{t+1}, N_{t+1}) \\cdot (R_{t+1}^K + 1 - \\delta)\\right]\n\\]\nLabor supply (consumption-leisure trade-off): \\[\n\\frac{-U_N(C_t, N_t)}{U_C(C_t, N_t)} = W_t\n\\]\n\n\nExample: Log Utility\nWith \\(U(C, N) = \\log(C) - \\chi \\frac{N^{1+\\phi}}{1+\\phi}\\):\n\\[\n\\frac{1}{C_t} = \\beta \\mathbb{E}_t\\left[\\frac{1}{C_{t+1}} (R_{t+1}^K + 1 - \\delta)\\right]\n\\]\n\\[\n\\chi N_t^\\phi \\cdot C_t = W_t\n\\]\n\n\nCode\n# Illustrate consumption smoothing\nbeta &lt;- 0.99\nR_gross &lt;- 1.02  # gross interest rate\nsigma &lt;- 1  # CRRA coefficient\n\n# For sigma = 1 (log utility): C_t+1/C_t = (beta * R)\ngrowth_rate &lt;- beta * R_gross\n\n# Simulate consumption path with and without smoothing\nT_sim &lt;- 20\nincome &lt;- c(rep(1, 10), rep(1.2, 10))  # income jumps at t=10\n\n# Without smoothing (hand-to-mouth)\nc_htm &lt;- income\n\n# With smoothing (optimal)\n# Permanent income = average\nperm_inc &lt;- mean(income)\nc_smooth &lt;- rep(perm_inc, T_sim)\n\nconsumption_df &lt;- tibble(\n  t = rep(1:T_sim, 2),\n  consumption = c(c_htm, c_smooth),\n  type = rep(c(\"Hand-to-mouth\", \"Euler optimal\"), each = T_sim)\n)\n\nggplot(consumption_df, aes(x = t, y = consumption, color = type, linetype = type)) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(xintercept = 10, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 10.5, y = 1.15, label = \"Income jump\", hjust = 0) +\n  scale_color_manual(values = c(\"#e74c3c\", \"#3498db\")) +\n  labs(title = \"Consumption Smoothing via the Euler Equation\",\n       subtitle = \"Optimal: smooth consumption; Suboptimal: track income\",\n       x = \"Period\", y = \"Consumption\", color = NULL, linetype = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nEuler equation: optimal consumption smoothing",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#firm-problem",
    "href": "chapters/10_dsge_foundations.html#firm-problem",
    "title": "DSGE Foundations",
    "section": "Firm Problem",
    "text": "Firm Problem\nA representative firm maximizes profits with Cobb-Douglas production:\n\\[\nY_t = A_t K_{t-1}^\\alpha N_t^{1-\\alpha}\n\\]\nFactor demands (competitive markets, firms are price-takers): \\[\nW_t = (1-\\alpha) \\frac{Y_t}{N_t}\n\\] \\[\nR_t^K = \\alpha \\frac{Y_t}{K_{t-1}}\n\\]\nTechnology shock follows AR(1): \\[\n\\log(A_t) = \\rho_A \\log(A_{t-1}) + \\varepsilon_{A,t}, \\quad \\varepsilon_{A,t} \\sim N(0, \\sigma_A^2)\n\\]",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#market-clearing",
    "href": "chapters/10_dsge_foundations.html#market-clearing",
    "title": "DSGE Foundations",
    "section": "Market Clearing",
    "text": "Market Clearing\n\\[\nY_t = C_t + I_t\n\\]\nwhere investment \\(I_t = K_t - (1-\\delta)K_{t-1}\\).",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#the-complete-rbc-model",
    "href": "chapters/10_dsge_foundations.html#the-complete-rbc-model",
    "title": "DSGE Foundations",
    "section": "The Complete RBC Model",
    "text": "The Complete RBC Model\n\n\n\n\n\n\n\n\nEquation\nName\nVariables\n\n\n\n\n\\(\\frac{1}{C_t} = \\beta \\mathbb{E}_t\\left[\\frac{1}{C_{t+1}}(R_{t+1}^K + 1 - \\delta)\\right]\\)\nEuler\n\\(C, R^K\\)\n\n\n\\(\\chi N_t^\\phi C_t = W_t\\)\nLabor supply\n\\(N, C, W\\)\n\n\n\\(Y_t = A_t K_{t-1}^\\alpha N_t^{1-\\alpha}\\)\nProduction\n\\(Y, A, K, N\\)\n\n\n\\(W_t = (1-\\alpha) Y_t / N_t\\)\nLabor demand\n\\(W, Y, N\\)\n\n\n\\(R_t^K = \\alpha Y_t / K_{t-1}\\)\nCapital demand\n\\(R^K, Y, K\\)\n\n\n\\(Y_t = C_t + K_t - (1-\\delta)K_{t-1}\\)\nResource\n\\(Y, C, K\\)\n\n\n\\(\\log A_t = \\rho_A \\log A_{t-1} + \\varepsilon_A\\)\nTechnology\n\\(A\\)\n\n\n\n7 equations, 7 unknowns: \\((Y, C, N, K, W, R^K, A)\\)",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#the-3-equation-nk-model",
    "href": "chapters/10_dsge_foundations.html#the-3-equation-nk-model",
    "title": "DSGE Foundations",
    "section": "The 3-Equation NK Model",
    "text": "The 3-Equation NK Model\nThe canonical log-linearized NK model:\n\n1. IS Curve (Dynamic IS)\nFrom the household’s Euler equation: \\[\n\\hat{y}_t = \\mathbb{E}_t[\\hat{y}_{t+1}] - \\frac{1}{\\sigma}(i_t - \\mathbb{E}_t[\\pi_{t+1}] - r_t^n)\n\\]\nwhere: - \\(\\hat{y}_t\\) = output gap (deviation from flexible-price equilibrium) - \\(i_t\\) = nominal interest rate - \\(\\pi_t\\) = inflation - \\(r_t^n\\) = natural rate of interest - \\(\\sigma\\) = inverse elasticity of intertemporal substitution\nInterpretation: Higher real interest rate → postpone consumption → lower output today.\n\n\n2. Phillips Curve (NKPC)\nFrom Calvo pricing (fraction \\(1-\\theta\\) of firms adjust each period): \\[\n\\pi_t = \\beta \\mathbb{E}_t[\\pi_{t+1}] + \\kappa \\hat{y}_t\n\\]\nwhere: \\[\n\\kappa = \\frac{(1-\\theta)(1-\\beta\\theta)}{\\theta} \\cdot (\\sigma + \\phi)\n\\]\nInterpretation: Inflation today depends on expected future inflation plus current output gap (marginal cost pressure).\n\n\n3. Taylor Rule\nCentral bank sets interest rate: \\[\ni_t = \\rho_i i_{t-1} + (1-\\rho_i)\\left[r^* + \\phi_\\pi(\\pi_t - \\pi^*) + \\phi_y \\hat{y}_t\\right] + \\varepsilon_{m,t}\n\\]\nStandard calibration:\n\n\n\n\n\n\n\n\nParameter\nValue\nInterpretation\n\n\n\n\n\\(\\rho_i\\)\n0.8\nInterest rate smoothing\n\n\n\\(\\phi_\\pi\\)\n1.5\nResponse to inflation (Taylor principle: \\(\\phi_\\pi &gt; 1\\))\n\n\n\\(\\phi_y\\)\n0.5/4 = 0.125\nResponse to output gap (quarterly)\n\n\n\n\n\nCode\n# Solve simple 3-eq NK model\n# Parameters\nbeta &lt;- 0.99\nsigma &lt;- 1\nkappa &lt;- 0.1\nphi_pi &lt;- 1.5\nphi_y &lt;- 0.125\nrho_i &lt;- 0.8\n\n# Monetary policy shock simulation\nT_sim &lt;- 40\neps_m &lt;- c(0, 0.25, rep(0, T_sim - 2))  # 25bp shock at t=2\n\n# Initialize\ny_gap &lt;- pi &lt;- i_rate &lt;- numeric(T_sim)\ny_gap[1] &lt;- pi[1] &lt;- i_rate[1] &lt;- 0\n\n# Simple forward-looking solution (assuming expectations = 0 for simplicity)\n# More rigorous: use Blanchard-Kahn\nfor (t in 2:T_sim) {\n  # Taylor rule\n  i_rate[t] &lt;- rho_i * i_rate[t-1] + (1 - rho_i) * (phi_pi * pi[t-1] + phi_y * y_gap[t-1]) + eps_m[t]\n\n  # IS curve (simplified: E[y+1] = 0, E[pi+1] = 0)\n  y_gap[t] &lt;- -1/sigma * (i_rate[t] - 0 - 0) + 0.7 * y_gap[t-1]\n\n  # Phillips curve\n  pi[t] &lt;- beta * 0 + kappa * y_gap[t] + 0.5 * pi[t-1]\n}\n\nnk_df &lt;- tibble(\n  t = rep(1:T_sim, 3),\n  value = c(y_gap * 100, pi * 100, i_rate * 100),\n  variable = rep(c(\"Output Gap (%)\", \"Inflation (%)\", \"Interest Rate (%)\"), each = T_sim)\n)\n\nggplot(nk_df, aes(x = t, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  facet_wrap(~variable, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Response to 25bp Monetary Policy Shock\",\n       subtitle = \"Simplified 3-equation NK model\",\n       x = \"Quarters\", y = \"Percent deviation\") +\n  scale_color_manual(values = c(\"#3498db\", \"#e74c3c\", \"#2ecc71\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nThe 3-equation New Keynesian model",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#deriving-the-phillips-curve",
    "href": "chapters/10_dsge_foundations.html#deriving-the-phillips-curve",
    "title": "DSGE Foundations",
    "section": "Deriving the Phillips Curve",
    "text": "Deriving the Phillips Curve\n\nCalvo Pricing Setup\n\nEach period, fraction \\(1-\\theta\\) of firms can reset prices\nFraction \\(\\theta\\) are “stuck” with last period’s price\nWhen firms can adjust, they set price to maximize expected profits over the time they’ll be stuck\n\n\n\nOptimal Price Setting\nA firm that can adjust at time \\(t\\) chooses \\(P_t^*\\) to maximize: \\[\n\\mathbb{E}_t \\sum_{k=0}^{\\infty} (\\beta\\theta)^k \\left[ \\frac{P_t^*}{P_{t+k}} Y_{t+k|t} - MC_{t+k} Y_{t+k|t} \\right]\n\\]\nResult (after log-linearization): \\[\n\\hat{p}_t^* = (1-\\beta\\theta) \\sum_{k=0}^{\\infty} (\\beta\\theta)^k \\mathbb{E}_t[\\widehat{mc}_{t+k}]\n\\]\nFirms set prices as a weighted average of expected future marginal costs.\n\n\nAggregating to the Phillips Curve\nPrice index evolution: \\[\nP_t = \\left[\\theta P_{t-1}^{1-\\epsilon} + (1-\\theta)(P_t^*)^{1-\\epsilon}\\right]^{\\frac{1}{1-\\epsilon}}\n\\]\nLog-linearizing and combining: \\[\n\\pi_t = \\beta \\mathbb{E}_t[\\pi_{t+1}] + \\kappa \\cdot \\widehat{mc}_t\n\\]\nWith \\(\\widehat{mc}_t \\propto \\hat{y}_t\\) (output gap approximates marginal cost deviations).",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#the-technique",
    "href": "chapters/10_dsge_foundations.html#the-technique",
    "title": "DSGE Foundations",
    "section": "The Technique",
    "text": "The Technique\nFor any variable \\(X_t\\):\n\\[\n\\hat{x}_t \\equiv \\log(X_t) - \\log(\\bar{X}) = \\log\\left(\\frac{X_t}{\\bar{X}}\\right) \\approx \\frac{X_t - \\bar{X}}{\\bar{X}}\n\\]\nSo \\(\\hat{x}_t\\) is the percentage deviation from steady state.\n\nKey Approximation\nFor any function \\(f(X_t, Y_t)\\): \\[\nf(X_t, Y_t) \\approx f(\\bar{X}, \\bar{Y}) + f_X(\\bar{X}, \\bar{Y})(X_t - \\bar{X}) + f_Y(\\bar{X}, \\bar{Y})(Y_t - \\bar{Y})\n\\]\n\n\nExample: Euler Equation\nNonlinear Euler: \\[\n\\frac{1}{C_t} = \\beta \\mathbb{E}_t\\left[\\frac{1}{C_{t+1}} R_{t+1}\\right]\n\\]\nStep 1: Steady state \\[\n\\frac{1}{\\bar{C}} = \\beta \\frac{1}{\\bar{C}} \\bar{R} \\implies \\bar{R} = \\frac{1}{\\beta}\n\\]\nStep 2: Log-linearize. Let \\(C_t = \\bar{C} e^{\\hat{c}_t} \\approx \\bar{C}(1 + \\hat{c}_t)\\): \\[\n\\frac{1}{\\bar{C}(1 + \\hat{c}_t)} = \\beta \\mathbb{E}_t\\left[\\frac{1}{\\bar{C}(1 + \\hat{c}_{t+1})} \\bar{R}(1 + \\hat{r}_{t+1})\\right]\n\\]\nStep 3: First-order Taylor expansion (ignore second-order terms): \\[\n1 - \\hat{c}_t = \\beta \\bar{R} \\mathbb{E}_t\\left[(1 - \\hat{c}_{t+1})(1 + \\hat{r}_{t+1})\\right]\n\\] \\[\n1 - \\hat{c}_t \\approx \\mathbb{E}_t[1 - \\hat{c}_{t+1} + \\hat{r}_{t+1}]\n\\]\nResult: \\[\n\\hat{c}_t = \\mathbb{E}_t[\\hat{c}_{t+1}] - (\\hat{r}_{t+1} - 0)\n\\]\nThis is the log-linearized Euler equation (with \\(\\sigma = 1\\)).\n\n\nCode\n# Compare nonlinear vs log-linear Euler\nC_ss &lt;- 1\nbeta &lt;- 0.99\nR_ss &lt;- 1/beta\n\n# Deviations from steady state\nc_dev &lt;- seq(-0.2, 0.2, 0.01)\n\n# Nonlinear: U'(C) = 1/C\nuprime_nonlinear &lt;- 1 / (C_ss * (1 + c_dev))\n\n# Log-linear: U'(C) ≈ 1/C_ss * (1 - c_dev)\nuprime_linear &lt;- (1/C_ss) * (1 - c_dev)\n\napprox_df &lt;- tibble(\n  deviation = rep(c_dev * 100, 2),\n  marginal_utility = c(uprime_nonlinear, uprime_linear),\n  method = rep(c(\"Nonlinear\", \"Log-linear\"), each = length(c_dev))\n)\n\nggplot(approx_df, aes(x = deviation, y = marginal_utility, color = method)) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"Log-Linearization Accuracy\",\n       subtitle = \"Approximation works well for small deviations (±10%)\",\n       x = \"% Deviation from Steady State\",\n       y = \"Marginal Utility U'(C)\", color = NULL) +\n  scale_color_manual(values = c(\"#3498db\", \"#e74c3c\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nLog-linearization accuracy",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#partitioning-the-system",
    "href": "chapters/10_dsge_foundations.html#partitioning-the-system",
    "title": "DSGE Foundations",
    "section": "Partitioning the System",
    "text": "Partitioning the System\nSplit \\(y_t = \\begin{pmatrix} x_t \\\\ z_t \\end{pmatrix}\\) where:\n\n\\(x_t\\): Predetermined (state) variables — known at time \\(t\\) (e.g., capital \\(K_{t-1}\\))\n\\(z_t\\): Jump (forward-looking) variables — can adjust freely (e.g., consumption, inflation)",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#blanchard-kahn-conditions",
    "href": "chapters/10_dsge_foundations.html#blanchard-kahn-conditions",
    "title": "DSGE Foundations",
    "section": "Blanchard-Kahn Conditions",
    "text": "Blanchard-Kahn Conditions\nThe eigenvalue decomposition of the system matrix determines solvability (Blanchard and Kahn 1980).\n\n\n\n\n\n\nImportantBlanchard-Kahn (1980) Conditions\n\n\n\nFor a unique stable solution:\n\nNumber of eigenvalues outside unit circle = Number of jump variables\n\n\n\n\n\n\n\n\n\n\nEigenvalue Count\nDiagnosis\n\n\n\n\nEquals number of jump vars\nUnique solution (saddle-path stable)\n\n\nLess than number of jump vars\nIndeterminacy (multiple equilibria, sunspots)\n\n\nGreater than number of jump vars\nNo solution (explosive dynamics)",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#the-solution-form",
    "href": "chapters/10_dsge_foundations.html#the-solution-form",
    "title": "DSGE Foundations",
    "section": "The Solution Form",
    "text": "The Solution Form\nIf BK conditions hold, the solution is:\n\\[\nx_{t+1} = H_x x_t + H_u u_{t+1}\n\\] \\[\nz_t = G_x x_t\n\\]\nThe policy function \\(G_x\\) tells us how jump variables respond to states. The transition matrix \\(H_x\\) governs state evolution.\n\n\nCode\n# Illustrate saddle-path in a simple 2-variable system\n# x_t = state (capital), z_t = control (consumption)\n\n# Phase diagram for consumption-capital\nk_grid &lt;- seq(0.5, 1.5, 0.02)\nc_grid &lt;- seq(0.5, 1.5, 0.02)\n\n# Steady state (normalized)\nk_ss &lt;- c_ss &lt;- 1\n\n# Simplified dynamics for illustration\n# dk/dt = 0 locus: c = f(k) - delta*k\n# dc/dt = 0 locus: f'(k) = rho + delta\n\nalpha &lt;- 0.33\ndelta &lt;- 0.025\nrho &lt;- 0.01\n\n# Nullclines\nc_from_k_nullcline &lt;- function(k) k^alpha - delta * k\nk_from_c_nullcline &lt;- (alpha / (rho + delta))^(1/(1-alpha))\n\n# Create phase diagram\nphase_df &lt;- expand_grid(k = k_grid, c = c_grid) %&gt;%\n  mutate(\n    dk = k^alpha - c - delta * k,\n    dc = (alpha * k^(alpha-1) - delta - rho) * c / 2  # simplified\n  )\n\n# Saddle path (approximate)\nsaddle_path &lt;- tibble(\n  k = seq(0.7, 1.3, 0.01),\n  c = c_ss + 0.8 * (k - k_ss)  # slope of stable eigenvector\n)\n\nggplot() +\n  # Direction field\n  geom_segment(data = phase_df %&gt;% sample_n(200),\n               aes(x = k, y = c, xend = k + dk*0.05, yend = c + dc*0.05),\n               arrow = arrow(length = unit(0.1, \"cm\")), alpha = 0.3) +\n  # Nullclines\n  geom_function(fun = c_from_k_nullcline, color = \"#3498db\", linewidth = 1.2) +\n  geom_vline(xintercept = k_from_c_nullcline, color = \"#e74c3c\", linewidth = 1.2) +\n  # Saddle path\n  geom_line(data = saddle_path, aes(x = k, y = c),\n            color = \"#2ecc71\", linewidth = 1.5, linetype = \"dashed\") +\n  # Steady state\n  geom_point(aes(x = k_ss, y = c_ss), size = 4, color = \"black\") +\n  annotate(\"text\", x = k_ss + 0.05, y = c_ss + 0.05, label = \"Steady State\") +\n  annotate(\"text\", x = 1.3, y = c_from_k_nullcline(1.3) - 0.05,\n           label = \"dk = 0\", color = \"#3498db\") +\n  annotate(\"text\", x = k_from_c_nullcline + 0.05, y = 1.4,\n           label = \"dc = 0\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 1.2, y = c_ss + 0.8 * 0.2 + 0.05,\n           label = \"Saddle Path\", color = \"#2ecc71\") +\n  labs(title = \"Saddle-Path Stability in DSGE\",\n       subtitle = \"Jump variable (c) must be on saddle path for convergence\",\n       x = \"Capital (k)\", y = \"Consumption (c)\") +\n  coord_cartesian(xlim = c(0.6, 1.4), ylim = c(0.6, 1.4)) +\n  theme_minimal()\n\n\n\n\n\nBlanchard-Kahn: saddle-path stability",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#common-causes-of-bk-violations",
    "href": "chapters/10_dsge_foundations.html#common-causes-of-bk-violations",
    "title": "DSGE Foundations",
    "section": "Common Causes of BK Violations",
    "text": "Common Causes of BK Violations\n\n\n\n\n\n\n\n\nProblem\nSymptom\nCommon Fix\n\n\n\n\nTaylor principle violated\nToo few unstable roots\nRaise \\(\\phi_\\pi\\) above 1\n\n\nMissing expectations\nWrong root count\nCheck all \\(\\mathbb{E}_t\\) terms\n\n\nTiming error\nExplosive solutions\nDynare uses end-of-period capital: use \\(K(-1)\\)\n\n\nWrong variable classification\nIndeterminacy\nRe-check predetermined vs. jump",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#numerical-solution-qz-decomposition",
    "href": "chapters/10_dsge_foundations.html#numerical-solution-qz-decomposition",
    "title": "DSGE Foundations",
    "section": "Numerical Solution: QZ Decomposition",
    "text": "Numerical Solution: QZ Decomposition\nFor larger models, we use the generalized Schur (QZ) decomposition:\n\\[\nA = Q \\Lambda Z', \\quad B = Q \\Omega Z'\n\\]\nwhere \\(\\Lambda/\\Omega\\) gives generalized eigenvalues. Reorder to put unstable eigenvalues in the bottom block.\n# R code sketch for BK solution\nsolve_dsge_bk &lt;- function(A, B, n_state) {\n  # A * E[y_{t+1}] = B * y_t\n  # y = [x; z] where x = states, z = jumps\n\n  n_total &lt;- nrow(A)\n  n_jump &lt;- n_total - n_state\n\n  # QZ decomposition\n  qz &lt;- geigen::gqz(A, B)  # Generalized Schur\n\n  # Eigenvalues\n  eig &lt;- qz$alpha / qz$beta\n  n_unstable &lt;- sum(abs(eig) &gt; 1)\n\n  # Check BK conditions\n  if (n_unstable != n_jump) {\n    stop(paste(\"BK violation:\", n_unstable, \"unstable,\", n_jump, \"jump vars\"))\n  }\n\n  # Extract policy function (from stable block)\n  # ... (reorder and partition)\n\n  list(G = G_matrix, H = H_matrix, eigenvalues = eig)\n}",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#standard-rbcnk-calibration",
    "href": "chapters/10_dsge_foundations.html#standard-rbcnk-calibration",
    "title": "DSGE Foundations",
    "section": "Standard RBC/NK Calibration",
    "text": "Standard RBC/NK Calibration\n\n\n\nParameter\nSymbol\nValue\nSource\n\n\n\n\nDiscount factor\n\\(\\beta\\)\n0.99\nReal interest rate ≈ 4% annually\n\n\nCapital share\n\\(\\alpha\\)\n0.33\nNational accounts\n\n\nDepreciation\n\\(\\delta\\)\n0.025\n10% annual depreciation\n\n\nRisk aversion\n\\(\\sigma\\)\n1-2\nMicro studies\n\n\nFrisch elasticity\n\\(1/\\phi\\)\n0.5-2\nLabor supply literature\n\n\nCalvo parameter\n\\(\\theta\\)\n0.75\nPrices adjust every 4 quarters\n\n\nInflation weight\n\\(\\phi_\\pi\\)\n1.5\nTaylor (1993)\n\n\nOutput weight\n\\(\\phi_y\\)\n0.125\nTaylor (1993), quarterly",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#matching-moments",
    "href": "chapters/10_dsge_foundations.html#matching-moments",
    "title": "DSGE Foundations",
    "section": "Matching Moments",
    "text": "Matching Moments\nTarget: Model-implied moments ≈ Data moments\n\n\n\nMoment\nData (US)\nTypical RBC\n\n\n\n\nstd(Y)\n1.5-2%\nMatch by calibrating \\(\\sigma_A\\)\n\n\nstd(C)/std(Y)\n0.5-0.7\nEndogenous\n\n\nstd(I)/std(Y)\n2.5-3.5\nEndogenous\n\n\ncorr(C, Y)\n0.8-0.9\nEndogenous\n\n\ncorr(N, Y)\n0.8-0.9\nEndogenous\n\n\n\n\n\nCode\n# Simulate RBC model and compute moments\n# Simplified AR(1) approximation\n\n# Parameters\nrho_a &lt;- 0.95\nsigma_a &lt;- 0.007\nT_sim &lt;- 200\n\n# Technology shock\nset.seed(123)\nlog_a &lt;- numeric(T_sim)\nlog_a[1] &lt;- 0\nfor (t in 2:T_sim) {\n  log_a[t] &lt;- rho_a * log_a[t-1] + rnorm(1, 0, sigma_a)\n}\n\n# Simplified responses (from RBC solution)\n# Output: y_hat ≈ (1 + alpha/(1-alpha)) * a_hat\n# Consumption: c_hat ≈ 0.7 * y_hat (consumption smoother than output)\n# Investment: i_hat ≈ 3 * y_hat (investment more volatile)\n# Hours: n_hat ≈ 0.5 * y_hat\n\ny_hat &lt;- 1.5 * log_a\nc_hat &lt;- 0.7 * y_hat + rnorm(T_sim, 0, 0.002)\ni_hat &lt;- 3.0 * y_hat + rnorm(T_sim, 0, 0.005)\nn_hat &lt;- 0.5 * y_hat + rnorm(T_sim, 0, 0.003)\n\n# Compute moments\nmoments_model &lt;- tibble(\n  Variable = c(\"Output\", \"Consumption\", \"Investment\", \"Hours\"),\n  `Std Dev (%)` = c(sd(y_hat), sd(c_hat), sd(i_hat), sd(n_hat)) * 100,\n  `Rel to Y` = c(1, sd(c_hat)/sd(y_hat), sd(i_hat)/sd(y_hat), sd(n_hat)/sd(y_hat)),\n  `Corr with Y` = c(1, cor(c_hat, y_hat), cor(i_hat, y_hat), cor(n_hat, y_hat))\n)\n\n# Add data targets\nmoments_data &lt;- tibble(\n  Variable = c(\"Output\", \"Consumption\", \"Investment\", \"Hours\"),\n  `Std Dev (%)` = c(1.8, 1.0, 5.0, 1.5),\n  `Rel to Y` = c(1, 0.55, 2.8, 0.83),\n  `Corr with Y` = c(1, 0.85, 0.90, 0.85)\n)\n\n# Display\nknitr::kable(\n  bind_rows(\n    moments_model %&gt;% mutate(Source = \"Model\"),\n    moments_data %&gt;% mutate(Source = \"Data\")\n  ) %&gt;%\n    pivot_wider(names_from = Source, values_from = c(`Std Dev (%)`, `Rel to Y`, `Corr with Y`)),\n  caption = \"Business Cycle Moments: Model vs. Data\",\n  digits = 2\n)\n\n\n\nBusiness Cycle Moments: Model vs. Data\n\n\n\n\n\n\n\n\n\n\n\nVariable\nStd Dev (%)_Model\nStd Dev (%)_Data\nRel to Y_Model\nRel to Y_Data\nCorr with Y_Model\nCorr with Y_Data\n\n\n\n\nOutput\n2.26\n1.8\n1.00\n1.00\n1.00\n1.00\n\n\nConsumption\n1.60\n1.0\n0.71\n0.55\n0.99\n0.85\n\n\nInvestment\n6.77\n5.0\n2.99\n2.80\n1.00\n0.90\n\n\nHours\n1.14\n1.5\n0.50\n0.83\n0.96\n0.85\n\n\n\nCalibration: matching business cycle moments",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#steady-state-relationships",
    "href": "chapters/10_dsge_foundations.html#steady-state-relationships",
    "title": "DSGE Foundations",
    "section": "Steady-State Relationships",
    "text": "Steady-State Relationships\nMany parameters are pinned down by steady-state conditions:\nFrom Euler equation: \\[\n\\bar{R} = \\frac{1}{\\beta} \\implies \\beta = \\frac{1}{1 + r^*}\n\\] With \\(r^* = 4\\%\\) annually → \\(\\beta = 1/1.01 \\approx 0.99\\) quarterly.\nFrom capital demand: \\[\n\\bar{R}^K = \\alpha \\frac{\\bar{Y}}{\\bar{K}} = \\frac{1}{\\beta} - (1-\\delta)\n\\] Given \\(\\beta\\), \\(\\delta\\), and capital-output ratio, backs out \\(\\alpha\\).\nFrom labor supply: \\[\n\\chi \\bar{N}^\\phi \\bar{C} = \\bar{W} = (1-\\alpha)\\frac{\\bar{Y}}{\\bar{N}}\n\\] Given target \\(\\bar{N} = 1/3\\) (8 hours/day), backs out \\(\\chi\\).",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#a-simple-rbc-model-in-dynare",
    "href": "chapters/10_dsge_foundations.html#a-simple-rbc-model-in-dynare",
    "title": "DSGE Foundations",
    "section": "A Simple RBC Model in Dynare",
    "text": "A Simple RBC Model in Dynare\n%% RBC Model - rbc_simple.mod\n%% Preamble\nvar y c k n w r a;\nvarexo eps_a;\nparameters BETA ALPHA DELTA RHO_A SIGMA_A CHI PHI;\n\n%% Calibration\nBETA = 0.99;\nALPHA = 0.33;\nDELTA = 0.025;\nRHO_A = 0.95;\nSIGMA_A = 0.007;\nCHI = 1;       % labor disutility\nPHI = 1;       % inverse Frisch elasticity\n\n%% Model equations\nmodel;\n  % Euler equation\n  1/c = BETA * (1/c(+1)) * (r(+1) + 1 - DELTA);\n\n  % Labor supply\n  CHI * n^PHI * c = w;\n\n  % Production function\n  y = a * k(-1)^ALPHA * n^(1-ALPHA);\n\n  % Factor prices\n  w = (1-ALPHA) * y / n;\n  r = ALPHA * y / k(-1);\n\n  % Resource constraint\n  y = c + k - (1-DELTA)*k(-1);\n\n  % Technology shock\n  log(a) = RHO_A * log(a(-1)) + eps_a;\nend;\n\n%% Steady state\ninitval;\n  a = 1;\n  r = 1/BETA - 1 + DELTA;\n  k = (ALPHA/r)^(1/(1-ALPHA)) * 0.33;\n  n = 0.33;\n  y = a * k^ALPHA * n^(1-ALPHA);\n  c = y - DELTA*k;\n  w = (1-ALPHA) * y / n;\nend;\n\nsteady;\ncheck;\n\n%% Shocks\nshocks;\n  var eps_a; stderr SIGMA_A;\nend;\n\n%% Simulation\nstoch_simul(order=1, irf=40, periods=200);",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#key-dynare-commands",
    "href": "chapters/10_dsge_foundations.html#key-dynare-commands",
    "title": "DSGE Foundations",
    "section": "Key Dynare Commands",
    "text": "Key Dynare Commands\n\n\n\nCommand\nPurpose\n\n\n\n\nsteady\nCompute deterministic steady state\n\n\ncheck\nVerify Blanchard-Kahn conditions\n\n\nstoch_simul(order=1)\nFirst-order perturbation solution\n\n\nstoch_simul(irf=40)\nGenerate IRFs for 40 periods\n\n\nestimation(...)\nBayesian estimation (Module 11)\n\n\nmodel_diagnostics\nCheck model specification",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#important-timing-convention",
    "href": "chapters/10_dsge_foundations.html#important-timing-convention",
    "title": "DSGE Foundations",
    "section": "Important Timing Convention",
    "text": "Important Timing Convention\n\n\n\n\n\n\nWarningDynare Timing\n\n\n\nDynare uses end-of-period capital: - k without subscript = \\(K_t\\) (end of period \\(t\\)) - k(-1) = \\(K_{t-1}\\) (beginning of period \\(t\\) = end of \\(t-1\\))\nSo the production function uses k(-1):\ny = a * k(-1)^ALPHA * n^(1-ALPHA);",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#foundational",
    "href": "chapters/10_dsge_foundations.html#foundational",
    "title": "DSGE Foundations",
    "section": "Foundational",
    "text": "Foundational\n\nKydland & Prescott (1982) “Time to Build and Aggregate Fluctuations” Econometrica — RBC origins\nBlanchard & Kahn (1980) “The Solution of Linear Difference Models” Econometrica — BK conditions\nGalí (2015) Monetary Policy, Inflation, and the Business Cycle — NK textbook",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#medium-scale-models",
    "href": "chapters/10_dsge_foundations.html#medium-scale-models",
    "title": "DSGE Foundations",
    "section": "Medium-Scale Models",
    "text": "Medium-Scale Models\n\nSmets & Wouters (2003, 2007) “An Estimated DSGE Model” — Benchmark estimated NK\nChristiano, Eichenbaum & Evans (2005) “Nominal Rigidities” JPE — CEE model",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#solution-methods",
    "href": "chapters/10_dsge_foundations.html#solution-methods",
    "title": "DSGE Foundations",
    "section": "Solution Methods",
    "text": "Solution Methods\n\nUhlig (1999) “A Toolkit for Analysing Nonlinear Dynamic Stochastic Models” — Perturbation\nSims (2002) “Solving Linear Rational Expectations Models” — QZ decomposition",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#calibration-1",
    "href": "chapters/10_dsge_foundations.html#calibration-1",
    "title": "DSGE Foundations",
    "section": "Calibration",
    "text": "Calibration\n\nCooley & Prescott (1995) “Economic Growth and Business Cycles” — Calibration methodology\nChari, Kehoe & McGrattan (2007) “Business Cycle Accounting” Econometrica — Wedges approach",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/10_dsge_foundations.html#software",
    "href": "chapters/10_dsge_foundations.html#software",
    "title": "DSGE Foundations",
    "section": "Software",
    "text": "Software\n\nAdjemian et al. “Dynare” — Standard DSGE solver\nHerbst & Schorfheide (2016) Bayesian Estimation of DSGE Models — Modern estimation\n\n\n\n\n\n\n\nBlanchard, Olivier Jean, and Charles M Kahn. 1980. “The Solution of Linear Difference Models Under Rational Expectations.” Econometrica 48 (5): 1305–11.",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>DSGE Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html",
    "href": "chapters/11_dsge_estimation.html",
    "title": "DSGE Estimation",
    "section": "",
    "text": "From Model to Data\nModule 10 showed how to solve DSGE models. Now we estimate them—finding parameter values that make the model consistent with observed data.",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#the-workflow",
    "href": "chapters/11_dsge_estimation.html#the-workflow",
    "title": "DSGE Estimation",
    "section": "The Workflow",
    "text": "The Workflow\n\n\n\n\n\nDSGE estimation workflow",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#the-state-space-form",
    "href": "chapters/11_dsge_estimation.html#the-state-space-form",
    "title": "DSGE Estimation",
    "section": "The State Space Form",
    "text": "The State Space Form\n\\[\n\\underbrace{s_t = T s_{t-1} + R \\eta_t}_{\\text{Transition}} \\quad \\eta_t \\sim N(0, Q)\n\\]\n\\[\n\\underbrace{y_t = Z s_t + d + \\varepsilon_t}_{\\text{Observation}} \\quad \\varepsilon_t \\sim N(0, H)\n\\]\n\n\n\n\n\n\n\n\nComponent\nMeaning\nFrom DSGE\n\n\n\n\n\\(s_t\\)\nState vector (all model variables)\nSolution states\n\n\n\\(T\\)\nTransition matrix\n\\(G_x\\) from perturbation\n\n\n\\(R\\)\nShock impact\n\\(G_u\\) from perturbation\n\n\n\\(Q\\)\nShock variance-covariance\n\\(\\text{diag}(\\sigma_1^2, \\ldots, \\sigma_k^2)\\)\n\n\n\\(y_t\\)\nObservables (data)\nGDP growth, inflation, rate\n\n\n\\(Z\\)\nSelection/mapping matrix\nWhich states are observed\n\n\n\\(H\\)\nMeasurement error variance\nOften zero or small",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#example-3-variable-nk-model",
    "href": "chapters/11_dsge_estimation.html#example-3-variable-nk-model",
    "title": "DSGE Estimation",
    "section": "Example: 3-Variable NK Model",
    "text": "Example: 3-Variable NK Model\nObservables: \\(y_t = (\\Delta \\log Y_t, \\pi_t, i_t)'\\)\nStates: \\(s_t = (\\hat{y}_t, \\hat{\\pi}_t, \\hat{i}_t, \\hat{a}_t, \\hat{\\varepsilon}^m_t)'\\)\nMapping: \\[\nZ = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\end{pmatrix}\n\\]\nThe first three states are directly observed (up to measurement error).",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#the-algorithm",
    "href": "chapters/11_dsge_estimation.html#the-algorithm",
    "title": "DSGE Estimation",
    "section": "The Algorithm",
    "text": "The Algorithm\nFor each time period \\(t = 1, \\ldots, T\\):\n\nStep 1: Prediction\nGiven information up to \\(t-1\\): \\[\ns_{t|t-1} = T s_{t-1|t-1}\n\\] \\[\nP_{t|t-1} = T P_{t-1|t-1} T' + R Q R'\n\\]\nwhere \\(P_{t|t-1}\\) is the variance of the state forecast.\n\n\nStep 2: Innovation\nCompare prediction to actual observation: \\[\nv_t = y_t - Z s_{t|t-1} - d\n\\] \\[\nF_t = Z P_{t|t-1} Z' + H\n\\]\n\\(v_t\\) is the forecast error and \\(F_t\\) is its variance.\n\n\nStep 3: Update\nIncorporate new observation: \\[\nK_t = P_{t|t-1} Z' F_t^{-1}\n\\] \\[\ns_{t|t} = s_{t|t-1} + K_t v_t\n\\] \\[\nP_{t|t} = (I - K_t Z) P_{t|t-1}\n\\]\n\\(K_t\\) is the Kalman gain—how much to adjust the state given the forecast error.\n\n\nStep 4: Likelihood Contribution\n\\[\n\\log L_t = -\\frac{n_y}{2} \\log(2\\pi) - \\frac{1}{2} \\log|F_t| - \\frac{1}{2} v_t' F_t^{-1} v_t\n\\]\nTotal log-likelihood: \\[\n\\log L(\\theta | Y) = \\sum_{t=1}^{T} \\log L_t\n\\]\n\n\nCode\n# Implement Kalman filter\nkalman_filter &lt;- function(y, T_mat, R_mat, Z_mat, Q_mat, H_mat = NULL) {\n  # y: T_obs × n_y matrix of observations\n  # T_mat: n_s × n_s transition matrix\n  # R_mat: n_s × n_u shock impact matrix\n  # Z_mat: n_y × n_s observation matrix\n  # Q_mat: n_u × n_u shock variance\n  # H_mat: n_y × n_y measurement error variance (optional)\n\n  T_obs &lt;- nrow(y)\n  n_s &lt;- nrow(T_mat)\n  n_y &lt;- nrow(Z_mat)\n\n  if (is.null(H_mat)) H_mat &lt;- diag(0.0001, n_y)\n\n  # Initialize at unconditional distribution\n  s_filt &lt;- matrix(0, T_obs, n_s)\n  P_filt &lt;- array(0, c(n_s, n_s, T_obs))\n\n  # Unconditional variance: vec(P) = (I - T⊗T)^{-1} vec(RQR')\n  RQR &lt;- R_mat %*% Q_mat %*% t(R_mat)\n  P_init &lt;- matrix(solve(diag(n_s^2) - kronecker(T_mat, T_mat)) %*% as.vector(RQR), n_s, n_s)\n  P_init &lt;- (P_init + t(P_init)) / 2  # ensure symmetry\n\n  s_curr &lt;- rep(0, n_s)\n  P_curr &lt;- P_init\n\n  loglik &lt;- 0\n  innovations &lt;- matrix(0, T_obs, n_y)\n  F_series &lt;- array(0, c(n_y, n_y, T_obs))\n\n  for (t in 1:T_obs) {\n    # Prediction\n    s_pred &lt;- T_mat %*% s_curr\n    P_pred &lt;- T_mat %*% P_curr %*% t(T_mat) + RQR\n\n    # Innovation\n    v_t &lt;- y[t, ] - Z_mat %*% s_pred\n    F_t &lt;- Z_mat %*% P_pred %*% t(Z_mat) + H_mat\n    F_t &lt;- (F_t + t(F_t)) / 2  # ensure symmetry\n\n    # Log-likelihood\n    det_F &lt;- det(F_t)\n    if (det_F &lt;= 0) {\n      loglik &lt;- -1e10\n      break\n    }\n    F_inv &lt;- solve(F_t)\n    loglik &lt;- loglik - 0.5 * (n_y * log(2*pi) + log(det_F) + t(v_t) %*% F_inv %*% v_t)\n\n    # Update\n    K_t &lt;- P_pred %*% t(Z_mat) %*% F_inv\n    s_curr &lt;- as.vector(s_pred + K_t %*% v_t)\n    P_curr &lt;- P_pred - K_t %*% Z_mat %*% P_pred\n\n    # Store\n    s_filt[t, ] &lt;- s_curr\n    P_filt[, , t] &lt;- P_curr\n    innovations[t, ] &lt;- v_t\n    F_series[, , t] &lt;- F_t\n  }\n\n  list(\n    loglik = as.numeric(loglik),\n    states = s_filt,\n    P = P_filt,\n    innovations = innovations,\n    F = F_series\n  )\n}\n\n# Simulate a simple state-space model\nT_obs &lt;- 100\nn_s &lt;- 2  # states\nn_y &lt;- 2  # observables\nn_u &lt;- 2  # shocks\n\n# True parameters\nrho &lt;- 0.9\nsigma &lt;- 0.1\n\nT_true &lt;- matrix(c(rho, 0, 0.1, rho), 2, 2)\nR_true &lt;- diag(2)\nZ_true &lt;- diag(2)\nQ_true &lt;- diag(sigma^2, 2)\nH_true &lt;- diag(0.01, 2)\n\n# Simulate data\ns_true &lt;- matrix(0, T_obs, n_s)\ny_obs &lt;- matrix(0, T_obs, n_y)\n\nfor (t in 2:T_obs) {\n  s_true[t, ] &lt;- as.vector(T_true %*% s_true[t-1, ]) + as.vector(rmvnorm(1, sigma = Q_true))\n}\nfor (t in 1:T_obs) {\n  y_obs[t, ] &lt;- as.vector(Z_true %*% s_true[t, ]) + as.vector(rmvnorm(1, sigma = H_true))\n}\n\n# Run Kalman filter\nkf_result &lt;- kalman_filter(y_obs, T_true, R_true, Z_true, Q_true, H_true)\n\n# Plot filtered vs true states\nstates_df &lt;- tibble(\n  t = rep(1:T_obs, 4),\n  value = c(s_true[, 1], kf_result$states[, 1], s_true[, 2], kf_result$states[, 2]),\n  type = rep(c(\"True\", \"Filtered\", \"True\", \"Filtered\"), each = T_obs),\n  state = rep(c(\"State 1\", \"State 1\", \"State 2\", \"State 2\"), each = T_obs)\n)\n\nggplot(states_df, aes(x = t, y = value, color = type, linetype = type)) +\n  geom_line(linewidth = 0.8) +\n  facet_wrap(~state, scales = \"free_y\") +\n  scale_color_manual(values = c(\"#3498db\", \"#e74c3c\")) +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\")) +\n  labs(title = \"Kalman Filter: Filtered vs True States\",\n       x = \"Time\", y = \"Value\", color = NULL, linetype = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nKalman filter: prediction and update cycle",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#kalman-gain-intuition",
    "href": "chapters/11_dsge_estimation.html#kalman-gain-intuition",
    "title": "DSGE Estimation",
    "section": "Kalman Gain Intuition",
    "text": "Kalman Gain Intuition\nThe Kalman gain \\(K_t\\) balances:\n\nModel confidence (small \\(P_{t|t-1}\\)) → trust the prediction, small \\(K_t\\)\nObservation precision (small \\(H\\)) → trust the data, large \\(K_t\\)\n\n\n\nCode\n# Show how Kalman gain evolves\ngain_df &lt;- tibble(\n  t = 1:T_obs,\n  K_11 = sapply(1:T_obs, function(i) {\n    P &lt;- kf_result$P[,,i]\n    F &lt;- kf_result$F[,,i]\n    (P %*% t(Z_true) %*% solve(F))[1,1]\n  })\n)\n\nggplot(gain_df, aes(x = t, y = K_11)) +\n  geom_line(color = \"#3498db\", linewidth = 1) +\n  geom_hline(yintercept = mean(gain_df$K_11[(T_obs-20):T_obs]),\n             linetype = \"dashed\", color = \"#e74c3c\") +\n  labs(title = \"Kalman Gain Over Time\",\n       subtitle = \"Converges to steady state as filter 'learns' the process\",\n       x = \"Time\", y = expression(K[11])) +\n  theme_minimal()\n\n\n\n\n\nKalman gain adapts to uncertainty",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#the-challenge",
    "href": "chapters/11_dsge_estimation.html#the-challenge",
    "title": "DSGE Estimation",
    "section": "The Challenge",
    "text": "The Challenge\nFor each candidate \\(\\theta\\):\n\nSolve the DSGE → get \\(T(\\theta), R(\\theta)\\)\nRun Kalman filter → get \\(\\log L(\\theta | Y)\\)\n\nThis is computationally expensive. Gradient-free optimizers (Nelder-Mead, simulated annealing) or specialized methods (csminwel) are common.\n\n\nCode\n# ML estimation for simple AR(1) state space\n# y_t = s_t + e_t, s_t = rho * s_{t-1} + eta_t\n\nml_loglik &lt;- function(theta, y) {\n  rho &lt;- theta[1]\n  sigma_eta &lt;- exp(theta[2])\n  sigma_e &lt;- exp(theta[3])\n\n  # Stationarity check\n  if (abs(rho) &gt;= 1) return(-1e10)\n\n  T_mat &lt;- matrix(rho, 1, 1)\n  R_mat &lt;- matrix(1, 1, 1)\n  Z_mat &lt;- matrix(1, 1, 1)\n  Q_mat &lt;- matrix(sigma_eta^2, 1, 1)\n  H_mat &lt;- matrix(sigma_e^2, 1, 1)\n\n  y_mat &lt;- matrix(y, ncol = 1)\n  kf &lt;- kalman_filter(y_mat, T_mat, R_mat, Z_mat, Q_mat, H_mat)\n  return(kf$loglik)\n}\n\n# Simulate data from known parameters\nrho_true &lt;- 0.85\nsigma_eta_true &lt;- 0.15\nsigma_e_true &lt;- 0.05\n\ny_sim &lt;- numeric(100)\ns_sim &lt;- numeric(100)\nfor (t in 2:100) {\n  s_sim[t] &lt;- rho_true * s_sim[t-1] + rnorm(1, 0, sigma_eta_true)\n  y_sim[t] &lt;- s_sim[t] + rnorm(1, 0, sigma_e_true)\n}\n\n# Grid search for illustration\nrho_grid &lt;- seq(0.5, 0.99, 0.02)\nloglik_grid &lt;- sapply(rho_grid, function(r) {\n  ml_loglik(c(r, log(sigma_eta_true), log(sigma_e_true)), y_sim)\n})\n\ngrid_df &lt;- tibble(rho = rho_grid, loglik = loglik_grid)\n\nggplot(grid_df, aes(x = rho, y = loglik)) +\n  geom_line(color = \"#3498db\", linewidth = 1.2) +\n  geom_vline(xintercept = rho_true, linetype = \"dashed\", color = \"#e74c3c\") +\n  geom_vline(xintercept = rho_grid[which.max(loglik_grid)],\n             linetype = \"dotted\", color = \"#2ecc71\", linewidth = 1) +\n  annotate(\"text\", x = rho_true + 0.03, y = min(loglik_grid) + 5,\n           label = \"True\", color = \"#e74c3c\") +\n  annotate(\"text\", x = rho_grid[which.max(loglik_grid)] - 0.03,\n           y = max(loglik_grid) - 2, label = \"MLE\", color = \"#2ecc71\") +\n  labs(title = \"Log-Likelihood Profile\",\n       subtitle = \"Conditioning on true shock variances\",\n       x = expression(rho), y = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\nMaximum likelihood: finding the peak",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#standard-errors",
    "href": "chapters/11_dsge_estimation.html#standard-errors",
    "title": "DSGE Estimation",
    "section": "Standard Errors",
    "text": "Standard Errors\nAt the MLE, compute the Hessian (matrix of second derivatives):\n\\[\n\\hat{V}(\\hat{\\theta}) = -H^{-1}, \\quad H_{ij} = \\frac{\\partial^2 \\log L}{\\partial \\theta_i \\partial \\theta_j}\\bigg|_{\\hat{\\theta}}\n\\]\nStandard errors: \\(\\text{SE}(\\hat{\\theta}_j) = \\sqrt{\\hat{V}_{jj}}\\)",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#why-bayesian-for-dsge",
    "href": "chapters/11_dsge_estimation.html#why-bayesian-for-dsge",
    "title": "DSGE Estimation",
    "section": "Why Bayesian for DSGE?",
    "text": "Why Bayesian for DSGE?\n\n\n\n\n\n\n\nAdvantage\nExplanation\n\n\n\n\nRegularization\nPriors prevent extreme/implausible estimates\n\n\nIdentification\nInformative priors help weakly identified parameters\n\n\nFull uncertainty\nPosterior distribution, not just point estimate\n\n\nModel comparison\nMarginal likelihood is natural Bayesian output\n\n\nSmall samples\nWorks well with limited macro data",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#prior-selection",
    "href": "chapters/11_dsge_estimation.html#prior-selection",
    "title": "DSGE Estimation",
    "section": "Prior Selection",
    "text": "Prior Selection\nPriors should reflect economic knowledge without being too restrictive.\n\nStandard Prior Distributions\n\n\n\n\n\n\n\n\nParameter Type\nDistribution\nRationale\n\n\n\n\nPersistence (\\(\\rho\\))\nBeta(a, b)\nBounded [0, 1]\n\n\nElasticities\nGamma(a, b)\nPositive, right-skewed\n\n\nFractions (\\(\\alpha, \\theta\\))\nBeta(a, b)\nBounded [0, 1]\n\n\nShock std (\\(\\sigma\\))\nInv-Gamma(s, \\(\\nu\\))\nPositive, proper\n\n\nPolicy weights (\\(\\phi_\\pi\\))\nGamma or Normal\nPositive or unrestricted\n\n\nDiscount factor (\\(\\beta\\))\nBeta near 0.99\nClose to 1\n\n\n\n\n\nExample: NK Model Priors\n\n\nCode\nprior_table &lt;- tibble(\n  Parameter = c(\"$\\\\beta$\", \"$\\\\sigma$\", \"$\\\\phi$\", \"$\\\\theta$\", \"$\\\\phi_\\\\pi$\",\n                \"$\\\\phi_y$\", \"$\\\\rho_i$\", \"$\\\\rho_a$\", \"$\\\\sigma_a$\", \"$\\\\sigma_m$\"),\n  Distribution = c(\"Beta\", \"Gamma\", \"Gamma\", \"Beta\", \"Gamma\",\n                   \"Gamma\", \"Beta\", \"Beta\", \"Inv-Gamma\", \"Inv-Gamma\"),\n  Mean = c(0.99, 1.5, 2.0, 0.75, 1.5, 0.125, 0.8, 0.9, 0.01, 0.0025),\n  SD = c(0.002, 0.25, 0.5, 0.1, 0.25, 0.05, 0.1, 0.05, 2, 2),\n  Interpretation = c(\"Discount factor\", \"Risk aversion\", \"Frisch elasticity\",\n                     \"Calvo parameter\", \"Taylor: inflation\", \"Taylor: output\",\n                     \"Rate smoothing\", \"Tech persistence\", \"Tech shock std\", \"MP shock std\")\n)\n\nknitr::kable(prior_table, caption = \"Standard NK Model Priors\", escape = FALSE)\n\n\n\nStandard NK Model Priors\n\n\nParameter\nDistribution\nMean\nSD\nInterpretation\n\n\n\n\n\\(\\beta\\)\nBeta\n0.9900\n0.002\nDiscount factor\n\n\n\\(\\sigma\\)\nGamma\n1.5000\n0.250\nRisk aversion\n\n\n\\(\\phi\\)\nGamma\n2.0000\n0.500\nFrisch elasticity\n\n\n\\(\\theta\\)\nBeta\n0.7500\n0.100\nCalvo parameter\n\n\n\\(\\phi_\\pi\\)\nGamma\n1.5000\n0.250\nTaylor: inflation\n\n\n\\(\\phi_y\\)\nGamma\n0.1250\n0.050\nTaylor: output\n\n\n\\(\\rho_i\\)\nBeta\n0.8000\n0.100\nRate smoothing\n\n\n\\(\\rho_a\\)\nBeta\n0.9000\n0.050\nTech persistence\n\n\n\\(\\sigma_a\\)\nInv-Gamma\n0.0100\n2.000\nTech shock std\n\n\n\\(\\sigma_m\\)\nInv-Gamma\n0.0025\n2.000\nMP shock std\n\n\n\n\n\n\n\nCode\n# Plot some priors\nx_grid &lt;- seq(0, 3, 0.01)\n\nprior_df &lt;- bind_rows(\n  tibble(x = x_grid, density = dgamma(x_grid, 4, 4/1.5), param = \"phi_pi (Gamma)\"),\n  tibble(x = seq(0, 1, 0.01), density = dbeta(seq(0, 1, 0.01), 5, 2),\n         param = \"rho_i (Beta)\"),\n  tibble(x = seq(0, 1, 0.01), density = dbeta(seq(0, 1, 0.01), 5, 1.67),\n         param = \"theta (Beta)\")\n)\n\nggplot(prior_df, aes(x = x, y = density, fill = param)) +\n  geom_area(alpha = 0.5) +\n  facet_wrap(~param, scales = \"free\") +\n  labs(title = \"Prior Distributions\",\n       x = \"Parameter Value\", y = \"Density\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nPrior distributions for key parameters",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#the-algorithm-1",
    "href": "chapters/11_dsge_estimation.html#the-algorithm-1",
    "title": "DSGE Estimation",
    "section": "The Algorithm",
    "text": "The Algorithm\n\nInitialize: Start at some \\(\\theta^{(0)}\\) (often the posterior mode)\nPropose: Draw candidate \\(\\theta^* \\sim q(\\theta^* | \\theta^{(g)})\\)\n\nCommon: Random walk \\(\\theta^* = \\theta^{(g)} + \\varepsilon\\), \\(\\varepsilon \\sim N(0, c \\cdot \\Sigma)\\)\n\\(\\Sigma\\) = inverse Hessian at mode; \\(c\\) = scaling factor\n\nAccept/Reject: \\[\n\\alpha = \\min\\left(1, \\frac{p(\\theta^* | Y)}{p(\\theta^{(g)} | Y)}\\right) = \\min\\left(1, \\frac{L(Y|\\theta^*) p(\\theta^*)}{L(Y|\\theta^{(g)}) p(\\theta^{(g)})}\\right)\n\\]\n\nDraw \\(u \\sim \\text{Uniform}(0, 1)\\)\nIf \\(u &lt; \\alpha\\): accept \\(\\theta^{(g+1)} = \\theta^*\\)\nElse: reject \\(\\theta^{(g+1)} = \\theta^{(g)}\\)\n\nRepeat for \\(G\\) draws; discard first \\(B\\) as burn-in\n\n\n\nCode\n# Simple MH for AR(1) model\nmh_sampler &lt;- function(y, n_draw = 5000, n_burn = 1000, c_scale = 0.3) {\n\n  # Log-posterior (log-likelihood + log-prior)\n  log_posterior &lt;- function(theta) {\n    rho &lt;- theta[1]\n    log_sigma &lt;- theta[2]\n\n    # Prior: rho ~ Beta(5, 2) mapped to [0,1], sigma ~ InvGamma(2, 0.1)\n    if (rho &lt;= 0 || rho &gt;= 1) return(-Inf)\n    sigma &lt;- exp(log_sigma)\n    if (sigma &lt;= 0) return(-Inf)\n\n    log_prior &lt;- dbeta(rho, 5, 2, log = TRUE) +\n                 dgamma(1/sigma^2, 2, 0.1, log = TRUE) - 2 * log(sigma)\n\n    # Likelihood via Kalman filter\n    T_mat &lt;- matrix(rho, 1, 1)\n    R_mat &lt;- matrix(1, 1, 1)\n    Z_mat &lt;- matrix(1, 1, 1)\n    Q_mat &lt;- matrix(sigma^2, 1, 1)\n    H_mat &lt;- matrix(0.01, 1, 1)\n\n    y_mat &lt;- matrix(y, ncol = 1)\n    kf &lt;- kalman_filter(y_mat, T_mat, R_mat, Z_mat, Q_mat, H_mat)\n\n    return(kf$loglik + log_prior)\n  }\n\n  # Initialize\n  theta_curr &lt;- c(0.8, log(0.1))\n  log_post_curr &lt;- log_posterior(theta_curr)\n\n  # Proposal covariance\n  Sigma_prop &lt;- diag(c(0.02, 0.1)^2) * c_scale\n\n  # Storage\n  draws &lt;- matrix(0, n_draw, 2)\n  n_accept &lt;- 0\n\n  for (g in 1:(n_draw + n_burn)) {\n    # Propose\n    theta_star &lt;- rmvnorm(1, theta_curr, Sigma_prop)[1,]\n\n    # Compute acceptance probability\n    log_post_star &lt;- log_posterior(theta_star)\n    log_alpha &lt;- log_post_star - log_post_curr\n\n    # Accept/reject\n    if (log(runif(1)) &lt; log_alpha) {\n      theta_curr &lt;- theta_star\n      log_post_curr &lt;- log_post_star\n      if (g &gt; n_burn) n_accept &lt;- n_accept + 1\n    }\n\n    # Store\n    if (g &gt; n_burn) {\n      draws[g - n_burn, ] &lt;- theta_curr\n    }\n  }\n\n  list(\n    draws = draws,\n    acceptance_rate = n_accept / n_draw\n  )\n}\n\n# Run MH\nmh_result &lt;- mh_sampler(y_sim, n_draw = 3000, n_burn = 1000)\ncat(\"Acceptance rate:\", round(mh_result$acceptance_rate, 3), \"\\n\")\n\n\nAcceptance rate: 0.793 \n\n\nCode\n# Trace plots\ntrace_df &lt;- tibble(\n  iteration = rep(1:3000, 2),\n  value = c(mh_result$draws[, 1], exp(mh_result$draws[, 2])),\n  parameter = rep(c(\"rho\", \"sigma\"), each = 3000)\n)\n\nggplot(trace_df, aes(x = iteration, y = value)) +\n  geom_line(alpha = 0.5, color = \"#3498db\") +\n  facet_wrap(~parameter, scales = \"free_y\") +\n  geom_hline(data = tibble(parameter = c(\"rho\", \"sigma\"),\n                           true = c(rho_true, sigma_eta_true)),\n             aes(yintercept = true), color = \"#e74c3c\", linetype = \"dashed\") +\n  labs(title = \"MCMC Trace Plots\",\n       subtitle = \"Red dashed = true value\",\n       x = \"Iteration\", y = \"Value\") +\n  theme_minimal()\n\n\n\n\n\nMetropolis-Hastings sampling",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#tuning-the-proposal",
    "href": "chapters/11_dsge_estimation.html#tuning-the-proposal",
    "title": "DSGE Estimation",
    "section": "Tuning the Proposal",
    "text": "Tuning the Proposal\nTarget acceptance rate: 20-30% for random walk MH\n\n\n\nAcceptance Rate\nDiagnosis\nAction\n\n\n\n\n&lt; 10%\nProposals too bold\nDecrease \\(c\\)\n\n\n20-30%\nOptimal\nKeep\n\n\n&gt; 50%\nProposals too timid\nIncrease \\(c\\)\n\n\n\nDynare’s mh_jscale parameter controls this.",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#complete-example-3-equation-nk-model",
    "href": "chapters/11_dsge_estimation.html#complete-example-3-equation-nk-model",
    "title": "DSGE Estimation",
    "section": "Complete Example: 3-Equation NK Model",
    "text": "Complete Example: 3-Equation NK Model\n%% nk_estimation.mod\n\n%% Preamble\nvar y pi i a eps_m;\nvarexo eta_a eta_m;\nparameters BETA SIGMA KAPPA PHI_PI PHI_Y RHO_I RHO_A;\n\n%% Calibrated parameters\nBETA = 0.99;\nSIGMA = 1;\n\n%% Model\nmodel(linear);\n  % IS curve\n  y = y(+1) - (1/SIGMA) * (i - pi(+1));\n\n  % Phillips curve\n  pi = BETA * pi(+1) + KAPPA * y;\n\n  % Taylor rule\n  i = RHO_I * i(-1) + (1 - RHO_I) * (PHI_PI * pi + PHI_Y * y) + eps_m;\n\n  % Shocks\n  a = RHO_A * a(-1) + eta_a;   % technology (affects natural rate)\n  eps_m = eta_m;                % monetary policy\nend;\n\n%% Steady state\ninitval;\n  y = 0; pi = 0; i = 0; a = 0; eps_m = 0;\nend;\nsteady;\ncheck;\n\n%% Shocks\nshocks;\n  var eta_a; stderr 0.01;\n  var eta_m; stderr 0.0025;\nend;\n\n%% Observables (must match data columns)\nvarobs y pi i;\n\n%% Estimated parameters with priors\nestimated_params;\n  % Structural\n  KAPPA,    gamma_pdf,  0.1,   0.05;\n\n  % Policy rule\n  PHI_PI,   gamma_pdf,  1.5,   0.25;\n  PHI_Y,    gamma_pdf,  0.125, 0.05;\n  RHO_I,    beta_pdf,   0.8,   0.1;\n\n  % Shock processes\n  RHO_A,    beta_pdf,   0.9,   0.05;\n  stderr eta_a,  inv_gamma_pdf, 0.01,  2;\n  stderr eta_m,  inv_gamma_pdf, 0.0025, 2;\nend;\n\n%% Estimation\nestimation(\n  datafile = 'us_macro_data.csv',\n  first_obs = 1,\n  mode_compute = 4,        % csminwel optimizer\n  mode_check,              % plot likelihood around mode\n  mh_replic = 100000,      % MH draws\n  mh_nblocks = 2,          % parallel chains\n  mh_jscale = 0.3,         % proposal scaling\n  mh_drop = 0.5,           % burn-in fraction\n  bayesian_irf,            % posterior IRFs\n  smoother                 % smoothed states\n);",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#key-estimation-options",
    "href": "chapters/11_dsge_estimation.html#key-estimation-options",
    "title": "DSGE Estimation",
    "section": "Key Estimation Options",
    "text": "Key Estimation Options\n\n\n\n\n\n\n\n\nOption\nPurpose\nTypical Value\n\n\n\n\nmode_compute\nOptimizer for mode\n4 (csminwel) or 9 (dynare default)\n\n\nmode_check\nPlot likelihood around mode\nInclude\n\n\nmh_replic\nMH draws\n100,000-500,000\n\n\nmh_nblocks\nParallel chains\n2-4\n\n\nmh_jscale\nProposal scaling\n0.2-0.5 (tune for 25% acceptance)\n\n\nmh_drop\nBurn-in fraction\n0.5\n\n\nbayesian_irf\nCompute posterior IRFs\nInclude\n\n\nsmoother\nCompute smoothed states\nInclude",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#dynare-output",
    "href": "chapters/11_dsge_estimation.html#dynare-output",
    "title": "DSGE Estimation",
    "section": "Dynare Output",
    "text": "Dynare Output\nAfter estimation, Dynare produces:\n\n\n\nOutput\nLocation\nContent\n\n\n\n\nPosterior mode\noo_.posterior_mode\nPoint estimates\n\n\nPosterior draws\noo_.posterior_draws\nMCMC chain\n\n\nPrior/posterior plots\n*_PriorPosterior.pdf\nComparison\n\n\nMCMC diagnostics\n*_MCMCdiagnostics.pdf\nConvergence\n\n\nIRFs\noo_.irfs\nImpulse responses\n\n\nSmoothed variables\noo_.SmoothedVariables\nFiltered states\n\n\nMarginal likelihood\noo_.MarginalDensity\nModel comparison",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#marginal-likelihood",
    "href": "chapters/11_dsge_estimation.html#marginal-likelihood",
    "title": "DSGE Estimation",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\nThe marginal likelihood (or marginal data density) is:\n\\[\np(Y | M) = \\int p(Y | \\theta, M) p(\\theta | M) d\\theta\n\\]\nThis integrates over parameter uncertainty, automatically penalizing complexity.",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#bayes-factors",
    "href": "chapters/11_dsge_estimation.html#bayes-factors",
    "title": "DSGE Estimation",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nCompare models \\(M_1\\) vs \\(M_2\\):\n\\[\nBF_{12} = \\frac{p(Y | M_1)}{p(Y | M_2)}\n\\]\n\n\n\n\\(\\log BF_{12}\\)\nEvidence for \\(M_1\\)\n\n\n\n\n&lt; 0\nFavors \\(M_2\\)\n\n\n0-1\nWeak\n\n\n1-3\nPositive\n\n\n3-5\nStrong\n\n\n&gt; 5\nDecisive",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#computing-marginal-likelihood",
    "href": "chapters/11_dsge_estimation.html#computing-marginal-likelihood",
    "title": "DSGE Estimation",
    "section": "Computing Marginal Likelihood",
    "text": "Computing Marginal Likelihood\nLaplace approximation (fast, less accurate): \\[\n\\log p(Y | M) \\approx \\log p(Y | \\hat{\\theta}) + \\log p(\\hat{\\theta}) + \\frac{k}{2} \\log(2\\pi) - \\frac{1}{2} \\log|H|\n\\]\nModified harmonic mean (Dynare default with MCMC): \\[\n\\hat{p}(Y | M) = \\left[\\frac{1}{G} \\sum_{g=1}^G \\frac{f(\\theta^{(g)})}{\\tilde{p}(\\theta^{(g)} | Y)}\\right]^{-1}\n\\]\nwhere \\(f\\) is a truncated Normal centered at the posterior mode.\n\n\nCode\n# Simulate comparison of two models\n# Model 1: AR(1) with rho = 0.9\n# Model 2: AR(1) with rho = 0.5 (misspecified)\n\n# Approximate marginal likelihoods via Laplace\nlaplace_ml &lt;- function(y, rho_prior_mean) {\n  # Mode finding (simplified)\n  mle_rho &lt;- cor(y[-1], y[-length(y)])\n  mle_sigma &lt;- sd(y - c(0, mle_rho * y[-length(y)]))\n\n  # Log-likelihood at mode\n  T_mat &lt;- matrix(mle_rho, 1, 1)\n  R_mat &lt;- matrix(1, 1, 1)\n  Z_mat &lt;- matrix(1, 1, 1)\n  Q_mat &lt;- matrix(mle_sigma^2, 1, 1)\n  H_mat &lt;- matrix(0.01, 1, 1)\n  y_mat &lt;- matrix(y, ncol = 1)\n\n  kf &lt;- kalman_filter(y_mat, T_mat, R_mat, Z_mat, Q_mat, H_mat)\n  log_lik &lt;- kf$loglik\n\n  # Log-prior at mode (tighter prior centered at rho_prior_mean)\n  log_prior &lt;- dnorm(mle_rho, rho_prior_mean, 0.1, log = TRUE)\n\n  # Laplace approximation (simplified)\n  k &lt;- 2  # number of parameters\n  log_det_H &lt;- 4  # approximate\n\n  ml &lt;- log_lik + log_prior + k/2 * log(2*pi) - 0.5 * log_det_H\n  return(ml)\n}\n\n# Data generated from rho = 0.85\nml_correct &lt;- laplace_ml(y_sim, 0.85)\nml_misspec &lt;- laplace_ml(y_sim, 0.5)\n\nlog_bf &lt;- ml_correct - ml_misspec\n\ncat(\"Log Bayes Factor (correct vs misspecified):\", round(log_bf, 2), \"\\n\")\n\n\nLog Bayes Factor (correct vs misspecified): 1.58 \n\n\nCode\ncat(\"Interpretation:\", ifelse(log_bf &gt; 3, \"Strong evidence for correct model\",\n                               ifelse(log_bf &gt; 1, \"Positive evidence\", \"Weak evidence\")), \"\\n\")\n\n\nInterpretation: Positive evidence",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#mcmc-convergence",
    "href": "chapters/11_dsge_estimation.html#mcmc-convergence",
    "title": "DSGE Estimation",
    "section": "MCMC Convergence",
    "text": "MCMC Convergence\n\nTrace Plots\nChains should: - Explore the full parameter space - Not get stuck in one region - Look like “hairy caterpillars” (good mixing)\n\n\nGelman-Rubin Statistic\nFor multiple chains, compare within-chain vs between-chain variance:\n\\[\n\\hat{R} = \\sqrt{\\frac{\\hat{V}(\\theta)}{W}}\n\\]\nRule of thumb: \\(\\hat{R} &lt; 1.1\\) (ideally \\(&lt; 1.05\\))\n\n\nEffective Sample Size\nAccounts for autocorrelation in the chain:\n\\[\n\\text{ESS} = \\frac{n}{1 + 2\\sum_{k=1}^\\infty \\rho_k}\n\\]\nRule of thumb: ESS &gt; 400 for reliable posterior summaries\n\n\nCode\n# Autocorrelation plot\nacf_df &lt;- tibble(\n  lag = 0:30,\n  acf = acf(mh_result$draws[, 1], lag.max = 30, plot = FALSE)$acf[, 1, 1]\n)\n\np1 &lt;- ggplot(acf_df, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", fill = \"#3498db\", alpha = 0.7) +\n  geom_hline(yintercept = c(-1.96/sqrt(3000), 1.96/sqrt(3000)),\n             linetype = \"dashed\", color = \"#e74c3c\") +\n  labs(title = \"Autocorrelation (rho)\", x = \"Lag\", y = \"ACF\") +\n  theme_minimal()\n\n# Running mean\nrunning_mean &lt;- cumsum(mh_result$draws[, 1]) / (1:3000)\n\np2 &lt;- ggplot(tibble(iter = 1:3000, mean = running_mean), aes(x = iter, y = mean)) +\n  geom_line(color = \"#3498db\") +\n  geom_hline(yintercept = rho_true, linetype = \"dashed\", color = \"#e74c3c\") +\n  labs(title = \"Running Mean (rho)\", x = \"Iteration\", y = \"Cumulative Mean\") +\n  theme_minimal()\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nMCMC diagnostics",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#prior-vs-posterior",
    "href": "chapters/11_dsge_estimation.html#prior-vs-posterior",
    "title": "DSGE Estimation",
    "section": "Prior vs Posterior",
    "text": "Prior vs Posterior\nA key diagnostic: did the data inform the parameter?\n\n\n\nPattern\nInterpretation\n\n\n\n\nPosterior ≠ Prior\nData is informative\n\n\nPosterior ≈ Prior\nWeak identification or uninformative data\n\n\nPosterior outside prior support\nPrior may be misspecified\n\n\n\n\n\nCode\n# Compare prior and posterior for rho\nx_grid &lt;- seq(0.5, 1, 0.01)\nprior_dens &lt;- dbeta(x_grid, 5, 2)\nposterior_dens &lt;- density(mh_result$draws[, 1], from = 0.5, to = 1)\n\npp_df &lt;- bind_rows(\n  tibble(x = x_grid, density = prior_dens / max(prior_dens), type = \"Prior\"),\n  tibble(x = posterior_dens$x, density = posterior_dens$y / max(posterior_dens$y),\n         type = \"Posterior\")\n)\n\nggplot(pp_df, aes(x = x, y = density, fill = type)) +\n  geom_area(alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = rho_true, linetype = \"dashed\", color = \"black\") +\n  scale_fill_manual(values = c(\"#3498db\", \"#e74c3c\")) +\n  labs(title = \"Prior vs Posterior: rho\",\n       subtitle = \"Black dashed = true value\",\n       x = expression(rho), y = \"Density (normalized)\", fill = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nPrior vs posterior comparison",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#identification",
    "href": "chapters/11_dsge_estimation.html#identification",
    "title": "DSGE Estimation",
    "section": "Identification",
    "text": "Identification\nLocal identification: Can different \\(\\theta\\) values produce the same observables?\nDynare’s identification command checks: 1. Rank of the Jacobian at the mode 2. Which parameters are weakly identified\nSymptoms of weak identification: - Flat likelihood surface - Prior ≈ Posterior - High posterior correlation between parameters",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#estimation",
    "href": "chapters/11_dsge_estimation.html#estimation",
    "title": "DSGE Estimation",
    "section": "Estimation",
    "text": "Estimation\n\nAn & Schorfheide (2007) “Bayesian Analysis of DSGE Models” Econometric Reviews — Standard reference\nHerbst & Schorfheide (2016) Bayesian Estimation of DSGE Models — Modern textbook\nDel Negro & Schorfheide (2011) “Bayesian Macroeconometrics” Handbook — Survey chapter",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#kalman-filter",
    "href": "chapters/11_dsge_estimation.html#kalman-filter",
    "title": "DSGE Estimation",
    "section": "Kalman Filter",
    "text": "Kalman Filter\n\nHamilton (1994) Time Series Analysis Ch. 13 — Classic treatment\nDurbin & Koopman (2012) Time Series Analysis by State Space Methods — Comprehensive",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#mcmc",
    "href": "chapters/11_dsge_estimation.html#mcmc",
    "title": "DSGE Estimation",
    "section": "MCMC",
    "text": "MCMC\n\nChib & Greenberg (1995) “Understanding the Metropolis-Hastings Algorithm” American Statistician\nGeweke (1999) “Using Simulation Methods for Bayesian Econometric Models” — Practical guide",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#model-comparison-1",
    "href": "chapters/11_dsge_estimation.html#model-comparison-1",
    "title": "DSGE Estimation",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nGeweke (1999) “Bayesian Analysis of the Multinomial Probit Model” — Marginal likelihood computation\nDel Negro & Schorfheide (2004) “Priors from General Equilibrium Models for VARs” — DSGE-VAR",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11_dsge_estimation.html#software",
    "href": "chapters/11_dsge_estimation.html#software",
    "title": "DSGE Estimation",
    "section": "Software",
    "text": "Software\n\nAdjemian et al. “Dynare” — Standard solver/estimator\nHerbst & Schorfheide companion — MATLAB/Python code",
    "crumbs": [
      "Phase 5: Structural Macro",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>DSGE Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html",
    "href": "chapters/12_regularization.html",
    "title": "Regularization for Macro",
    "section": "",
    "text": "The High-Dimensional Challenge\nModern macroeconomics faces a dimensionality problem:",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#when-regularization-matters",
    "href": "chapters/12_regularization.html#when-regularization-matters",
    "title": "Regularization for Macro",
    "section": "When Regularization Matters",
    "text": "When Regularization Matters\n\n\n\n\n\n\n\n\nSetting\nProblem\nSolution\n\n\n\n\nLarge VARs\nK variables → K²p parameters\nMinnesota prior (shrinkage)\n\n\nForecasting\nMany predictors, short samples\nLASSO/Ridge selection\n\n\nFactor models\nHigh-dimensional data\nPCA, factor extraction\n\n\nCausal inference\nMany potential confounders\nDouble LASSO",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#the-idea",
    "href": "chapters/12_regularization.html#the-idea",
    "title": "Regularization for Macro",
    "section": "The Idea",
    "text": "The Idea\nAdd an \\(\\ell_2\\) penalty to prevent coefficients from exploding:\n\\[\n\\hat{\\beta}^{\\text{ridge}} = \\arg\\min_\\beta \\left\\{ \\sum_{i=1}^n (y_i - x_i'\\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\]\nEquivalently: \\[\n\\hat{\\beta}^{\\text{ridge}} = (X'X + \\lambda I)^{-1} X'y\n\\]",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#key-properties",
    "href": "chapters/12_regularization.html#key-properties",
    "title": "Regularization for Macro",
    "section": "Key Properties",
    "text": "Key Properties\n\n\n\n\n\n\n\nProperty\nImplication\n\n\n\n\nShrinks toward zero\nAll coefficients are shrunk\n\n\nNever exactly zero\nDoes NOT do variable selection\n\n\nContinuous\nStable, smooth as function of \\(\\lambda\\)\n\n\nHandles multicollinearity\n\\((X'X + \\lambda I)\\) always invertible\n\n\n\n\n\nCode\n# Simulate correlated predictors\nn &lt;- 100\np &lt;- 20\nX &lt;- matrix(rnorm(n * p), n, p)\n# Add correlation\nfor (j in 2:p) X[, j] &lt;- 0.7 * X[, j-1] + 0.3 * X[, j]\n\n# True coefficients: sparse\nbeta_true &lt;- c(3, -2, 0, 0, 1.5, rep(0, p - 5))\ny &lt;- X %*% beta_true + rnorm(n, 0, 2)\n\n# Ridge path\nridge_fit &lt;- glmnet(X, y, alpha = 0)  # alpha = 0 is Ridge\n\n# Extract coefficients along lambda path\ncoef_matrix &lt;- as.matrix(coef(ridge_fit)[-1, ])  # remove intercept\nlambda_seq &lt;- ridge_fit$lambda\n\nridge_df &lt;- as_tibble(t(coef_matrix)) %&gt;%\n  mutate(lambda = lambda_seq) %&gt;%\n  pivot_longer(-lambda, names_to = \"variable\", values_to = \"coefficient\")\n\nggplot(ridge_df, aes(x = log(lambda), y = coefficient, color = variable)) +\n  geom_line(alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Ridge Regularization Path\",\n       subtitle = \"Coefficients shrink toward zero as lambda increases\",\n       x = expression(log(lambda)), y = \"Coefficient\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nRidge regularization path",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#connection-to-bayesian-priors",
    "href": "chapters/12_regularization.html#connection-to-bayesian-priors",
    "title": "Regularization for Macro",
    "section": "Connection to Bayesian Priors",
    "text": "Connection to Bayesian Priors\nRidge regression is equivalent to Bayesian regression with: \\[\n\\beta_j \\sim N(0, \\tau^2), \\quad \\tau^2 = \\frac{\\sigma^2}{\\lambda}\n\\]\nThis is exactly the Minnesota prior intuition: shrink toward zero with tightness \\(\\lambda\\).",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#the-idea-1",
    "href": "chapters/12_regularization.html#the-idea-1",
    "title": "Regularization for Macro",
    "section": "The Idea",
    "text": "The Idea\nReplace \\(\\ell_2\\) penalty with \\(\\ell_1\\) (absolute value):\n\\[\n\\hat{\\beta}^{\\text{LASSO}} = \\arg\\min_\\beta \\left\\{ \\sum_{i=1}^n (y_i - x_i'\\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#key-properties-1",
    "href": "chapters/12_regularization.html#key-properties-1",
    "title": "Regularization for Macro",
    "section": "Key Properties",
    "text": "Key Properties\n\n\n\n\n\n\n\nProperty\nImplication\n\n\n\n\nShrinks to exactly zero\nAutomatic variable selection\n\n\nSparse solutions\nMost coefficients are zero\n\n\nNon-smooth\nPath has kinks (not differentiable)\n\n\nSelects one from correlated group\nArbitrary among correlated predictors\n\n\n\n\n\nCode\n# LASSO path\nlasso_fit &lt;- glmnet(X, y, alpha = 1)  # alpha = 1 is LASSO\n\ncoef_lasso &lt;- as.matrix(coef(lasso_fit)[-1, ])\nlambda_lasso &lt;- lasso_fit$lambda\n\nlasso_df &lt;- as_tibble(t(coef_lasso)) %&gt;%\n  mutate(lambda = lambda_lasso) %&gt;%\n  pivot_longer(-lambda, names_to = \"variable\", values_to = \"coefficient\")\n\n# Compare at specific lambda\nlambda_compare &lt;- 0.5\n\nridge_coef &lt;- coef(ridge_fit, s = lambda_compare)[-1]\nlasso_coef &lt;- coef(lasso_fit, s = lambda_compare)[-1]\n\ncompare_df &lt;- tibble(\n  variable = paste0(\"V\", 1:p),\n  Ridge = as.vector(ridge_coef),\n  LASSO = as.vector(lasso_coef),\n  True = beta_true\n) %&gt;%\n  pivot_longer(c(Ridge, LASSO, True), names_to = \"method\", values_to = \"coefficient\")\n\nggplot(compare_df, aes(x = variable, y = coefficient, fill = method)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.7) +\n  scale_fill_manual(values = c(\"#e74c3c\", \"#3498db\", \"#2ecc71\")) +\n  labs(title = paste0(\"Ridge vs LASSO at lambda = \", lambda_compare),\n       subtitle = \"LASSO sets many coefficients exactly to zero\",\n       x = \"Variable\", y = \"Coefficient\", fill = NULL) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\nLASSO vs Ridge: sparsity\n\n\n\n\n\n\nCode\nggplot(lasso_df, aes(x = log(lambda), y = coefficient, color = variable)) +\n  geom_line(alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"LASSO Regularization Path\",\n       subtitle = \"Coefficients hit exactly zero at different lambda values\",\n       x = expression(log(lambda)), y = \"Coefficient\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nLASSO path: coefficients drop to exactly zero",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#combining-ridge-and-lasso",
    "href": "chapters/12_regularization.html#combining-ridge-and-lasso",
    "title": "Regularization for Macro",
    "section": "Combining Ridge and LASSO",
    "text": "Combining Ridge and LASSO\n\\[\n\\hat{\\beta}^{\\text{EN}} = \\arg\\min_\\beta \\left\\{ \\sum_{i=1}^n (y_i - x_i'\\beta)^2 + \\lambda \\left[ \\alpha \\sum_j |\\beta_j| + (1-\\alpha) \\sum_j \\beta_j^2 \\right] \\right\\}\n\\]\n\n\n\n\\(\\alpha\\)\nMethod\n\n\n\n\n0\nRidge\n\n\n1\nLASSO\n\n\n0.5\nElastic Net (balanced)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#when-to-use-elastic-net",
    "href": "chapters/12_regularization.html#when-to-use-elastic-net",
    "title": "Regularization for Macro",
    "section": "When to Use Elastic Net",
    "text": "When to Use Elastic Net\n\nCorrelated predictors: LASSO arbitrarily selects one; Elastic Net keeps groups together\n\\(p &gt; n\\): LASSO selects at most \\(n\\) variables; Elastic Net can select more\nDefault in practice: Often use \\(\\alpha = 0.5\\) as a robust choice\n\n\n\nCode\nalpha_values &lt;- c(0, 0.5, 1)\nen_fits &lt;- map(alpha_values, ~glmnet(X, y, alpha = .x))\n\nen_cv &lt;- map(alpha_values, ~cv.glmnet(X, y, alpha = .x))\noptimal_lambdas &lt;- map_dbl(en_cv, ~.x$lambda.min)\n\nen_coefs &lt;- map2_dfr(en_fits, alpha_values, function(fit, a) {\n  lambda_opt &lt;- optimal_lambdas[which(alpha_values == a)]\n  coef_vec &lt;- coef(fit, s = lambda_opt)[-1]\n  tibble(\n    variable = paste0(\"V\", 1:p),\n    coefficient = as.vector(coef_vec),\n    alpha = paste0(\"alpha = \", a)\n  )\n})\n\nggplot(en_coefs, aes(x = variable, y = coefficient, fill = alpha)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Elastic Net: Effect of Mixing Parameter\",\n       subtitle = \"alpha = 0 (Ridge), 0.5 (Elastic Net), 1 (LASSO)\",\n       x = \"Variable\", y = \"Coefficient at optimal lambda\", fill = NULL) +\n  scale_fill_manual(values = c(\"#3498db\", \"#9b59b6\", \"#e74c3c\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\nElastic Net for different alpha values",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#k-fold-cross-validation",
    "href": "chapters/12_regularization.html#k-fold-cross-validation",
    "title": "Regularization for Macro",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nSplit data into K folds\nFor each \\(\\lambda\\):\n\nTrain on K-1 folds, predict on held-out fold\nRepeat K times\nAverage prediction error\n\nChoose \\(\\lambda\\) that minimizes CV error\n\n\n\nCode\n# Cross-validation for LASSO\ncv_lasso &lt;- cv.glmnet(X, y, alpha = 1, nfolds = 10)\n\ncv_df &lt;- tibble(\n  log_lambda = log(cv_lasso$lambda),\n  cvm = cv_lasso$cvm,\n  cvup = cv_lasso$cvup,\n  cvlo = cv_lasso$cvlo\n)\n\nggplot(cv_df, aes(x = log_lambda, y = cvm)) +\n  geom_point(color = \"#e74c3c\") +\n  geom_errorbar(aes(ymin = cvlo, ymax = cvup), alpha = 0.3) +\n  geom_vline(xintercept = log(cv_lasso$lambda.min), linetype = \"dashed\", color = \"#3498db\") +\n  geom_vline(xintercept = log(cv_lasso$lambda.1se), linetype = \"dotted\", color = \"#2ecc71\") +\n  annotate(\"text\", x = log(cv_lasso$lambda.min) + 0.5, y = max(cv_df$cvm) * 0.9,\n           label = \"lambda.min\", color = \"#3498db\") +\n  annotate(\"text\", x = log(cv_lasso$lambda.1se) + 0.5, y = max(cv_df$cvm) * 0.8,\n           label = \"lambda.1se\", color = \"#2ecc71\") +\n  labs(title = \"10-Fold Cross-Validation for LASSO\",\n       subtitle = \"lambda.min = minimum CV error; lambda.1se = most parsimonious within 1 SE\",\n       x = expression(log(lambda)), y = \"Mean Cross-Validation Error\") +\n  theme_minimal()\n\n\n\n\n\nCross-validation for lambda selection",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#lambda.min-vs-lambda.1se",
    "href": "chapters/12_regularization.html#lambda.min-vs-lambda.1se",
    "title": "Regularization for Macro",
    "section": "lambda.min vs lambda.1se",
    "text": "lambda.min vs lambda.1se\n\n\n\n\n\n\n\n\nChoice\nDescription\nWhen to Use\n\n\n\n\nlambda.min\nMinimizes CV error\nMaximum predictive power\n\n\nlambda.1se\nLargest \\(\\lambda\\) within 1 SE of min\nMore parsimonious, often preferred",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#the-problem-with-naive-lasso-for-inference",
    "href": "chapters/12_regularization.html#the-problem-with-naive-lasso-for-inference",
    "title": "Regularization for Macro",
    "section": "The Problem with Naive LASSO for Inference",
    "text": "The Problem with Naive LASSO for Inference\nLASSO gives biased estimates and invalid standard errors because:\n\nRegularization bias: Shrinkage toward zero\nModel selection uncertainty: Which variables are “truly” zero?\nNo valid confidence intervals in the classical sense",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#the-solution-double-selection",
    "href": "chapters/12_regularization.html#the-solution-double-selection",
    "title": "Regularization for Macro",
    "section": "The Solution: Double Selection",
    "text": "The Solution: Double Selection\nFor causal effect of \\(D\\) on \\(Y\\) with high-dimensional controls \\(X\\):\nStep 1: LASSO of \\(Y\\) on \\(X\\) → select controls \\(\\hat{S}_Y\\)\nStep 2: LASSO of \\(D\\) on \\(X\\) → select controls \\(\\hat{S}_D\\)\nStep 3: OLS of \\(Y\\) on \\(D\\) and \\(\\hat{S}_Y \\cup \\hat{S}_D\\)\nThis is the Belloni, Chernozhukov, Hansen (2014) approach.\n\n\nCode\n# Simulate causal setting\nn &lt;- 200\np &lt;- 50  # many potential confounders\n\n# Generate confounders\nX &lt;- matrix(rnorm(n * p), n, p)\n\n# Treatment depends on some confounders\nD &lt;- 0.5 * X[, 1] + 0.3 * X[, 2] + 0.2 * X[, 5] + rnorm(n, 0, 1)\n\n# Outcome depends on treatment and confounders\ntau_true &lt;- 2  # true causal effect\nY &lt;- tau_true * D + 1.5 * X[, 1] + 0.8 * X[, 3] + 0.5 * X[, 5] + rnorm(n, 0, 1)\n\n# Naive OLS (omitting confounders)\nnaive_ols &lt;- lm(Y ~ D)\ncat(\"Naive OLS (biased):\", round(coef(naive_ols)[2], 3), \"\\n\")\n\n\nNaive OLS (biased): 2.581 \n\n\nCode\n# Double LASSO\n# Step 1: Y on X\nlasso_Y &lt;- cv.glmnet(X, Y, alpha = 1)\nselected_Y &lt;- which(coef(lasso_Y, s = \"lambda.1se\")[-1] != 0)\n\n# Step 2: D on X\nlasso_D &lt;- cv.glmnet(X, D, alpha = 1)\nselected_D &lt;- which(coef(lasso_D, s = \"lambda.1se\")[-1] != 0)\n\n# Step 3: Union and OLS\nselected_union &lt;- unique(c(selected_Y, selected_D))\nX_selected &lt;- X[, selected_union, drop = FALSE]\ndouble_lasso &lt;- lm(Y ~ D + X_selected)\n\ncat(\"Double LASSO:\", round(coef(double_lasso)[2], 3), \"\\n\")\n\n\nDouble LASSO: 1.976 \n\n\nCode\ncat(\"True effect:\", tau_true, \"\\n\")\n\n\nTrue effect: 2 \n\n\nCode\ncat(\"Selected variables:\", length(selected_union), \"\\n\")\n\n\nSelected variables: 4 \n\n\nCode\n# Compare estimates\ncomparison_df &lt;- tibble(\n  Method = c(\"Naive OLS\", \"Double LASSO\", \"Oracle OLS\"),\n  Estimate = c(\n    coef(naive_ols)[2],\n    coef(double_lasso)[2],\n    coef(lm(Y ~ D + X[, c(1, 3, 5)]))[2]\n  ),\n  SE = c(\n    summary(naive_ols)$coefficients[2, 2],\n    summary(double_lasso)$coefficients[2, 2],\n    summary(lm(Y ~ D + X[, c(1, 3, 5)]))$coefficients[2, 2]\n  )\n)\n\nggplot(comparison_df, aes(x = Method, y = Estimate)) +\n  geom_point(size = 4, color = \"#3498db\") +\n  geom_errorbar(aes(ymin = Estimate - 1.96 * SE, ymax = Estimate + 1.96 * SE),\n                width = 0.2, color = \"#3498db\") +\n  geom_hline(yintercept = tau_true, linetype = \"dashed\", color = \"#e74c3c\") +\n  annotate(\"text\", x = 3.3, y = tau_true + 0.1, label = \"True effect\", color = \"#e74c3c\") +\n  labs(title = \"Double LASSO vs Naive OLS\",\n       subtitle = \"Double LASSO reduces omitted variable bias\",\n       y = \"Estimated Treatment Effect\") +\n  theme_minimal()\n\n\n\n\n\nDouble LASSO for causal inference",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#the-hdm-package-in-r",
    "href": "chapters/12_regularization.html#the-hdm-package-in-r",
    "title": "Regularization for Macro",
    "section": "The hdm Package in R",
    "text": "The hdm Package in R\nlibrary(hdm)\n\n# Double selection\nds_result &lt;- rlassoEffect(x = X, y = Y, d = D, method = \"double selection\")\nsummary(ds_result)\n\n# Partialling out\npo_result &lt;- rlassoEffect(x = X, y = Y, d = D, method = \"partialling out\")\nsummary(po_result)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#large-bayesian-vars-banbura-giannone-reichlin-2010",
    "href": "chapters/12_regularization.html#large-bayesian-vars-banbura-giannone-reichlin-2010",
    "title": "Regularization for Macro",
    "section": "Large Bayesian VARs (Banbura, Giannone, Reichlin 2010)",
    "text": "Large Bayesian VARs (Banbura, Giannone, Reichlin 2010)\nProblem: K-variable VAR(p) has \\(K^2 p\\) parameters.\nSolution: Tighter Minnesota prior as dimension grows\n\\[\n\\lambda_1^{\\text{large}} = \\lambda_1^{\\text{small}} \\times \\frac{K_{\\text{small}}}{K_{\\text{large}}}\n\\]\nThis is implicit Ridge regularization.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#factor-augmented-var-favar",
    "href": "chapters/12_regularization.html#factor-augmented-var-favar",
    "title": "Regularization for Macro",
    "section": "Factor-Augmented VAR (FAVAR)",
    "text": "Factor-Augmented VAR (FAVAR)\nExtract factors from large dataset, include in VAR:\n\\[\n\\begin{pmatrix} F_t \\\\ Y_t \\end{pmatrix} = A \\begin{pmatrix} F_{t-1} \\\\ Y_{t-1} \\end{pmatrix} + u_t\n\\]\nwhere \\(F_t\\) are principal components of hundreds of macro series.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#sparse-granger-causality",
    "href": "chapters/12_regularization.html#sparse-granger-causality",
    "title": "Regularization for Macro",
    "section": "Sparse Granger Causality",
    "text": "Sparse Granger Causality\nUse LASSO to identify which variables Granger-cause others in high-dimensional settings.\n\n\nCode\n# Simulate sparse VAR(1)\nK &lt;- 10  # variables\nT_sim &lt;- 200\n\n# True VAR coefficient matrix (sparse)\nA_true &lt;- matrix(0, K, K)\nA_true[1, 1] &lt;- 0.7\nA_true[2, 2] &lt;- 0.6\nA_true[3, 1] &lt;- 0.3  # 1 → 3\nA_true[4, 2] &lt;- 0.25 # 2 → 4\nA_true[5, 5] &lt;- 0.8\n\n# Simulate\nY_var &lt;- matrix(0, T_sim, K)\nfor (t in 2:T_sim) {\n  Y_var[t, ] &lt;- A_true %*% Y_var[t-1, ] + rnorm(K, 0, 0.5)\n}\n\n# Estimate with LASSO (equation by equation)\nlasso_A &lt;- matrix(0, K, K)\nfor (k in 1:K) {\n  y_k &lt;- Y_var[-1, k]\n  X_lag &lt;- Y_var[-T_sim, ]\n  cv_fit &lt;- cv.glmnet(X_lag, y_k, alpha = 1)\n  lasso_A[k, ] &lt;- coef(cv_fit, s = \"lambda.1se\")[-1]\n}\n\n# Compare\ncomparison_var &lt;- tibble(\n  i = rep(1:K, each = K),\n  j = rep(1:K, K),\n  True = as.vector(t(A_true)),\n  LASSO = as.vector(t(lasso_A))\n) %&gt;%\n  filter(True != 0 | abs(LASSO) &gt; 0.05)\n\nggplot(comparison_var, aes(x = True, y = LASSO)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"#e74c3c\") +\n  labs(title = \"Sparse VAR: True vs LASSO Estimates\",\n       subtitle = \"LASSO recovers non-zero coefficients, shrinks others to zero\",\n       x = \"True Coefficient\", y = \"LASSO Estimate\") +\n  theme_minimal()\n\n\n\n\n\nSparse VAR: identifying causal links",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#the-glmnet-workflow",
    "href": "chapters/12_regularization.html#the-glmnet-workflow",
    "title": "Regularization for Macro",
    "section": "The glmnet Workflow",
    "text": "The glmnet Workflow\nlibrary(glmnet)\n\n# Standardize predictors (glmnet does this internally)\nX &lt;- scale(X)\n\n# Ridge (alpha = 0)\nridge_fit &lt;- glmnet(X, y, alpha = 0)\ncv_ridge &lt;- cv.glmnet(X, y, alpha = 0)\ncoef(cv_ridge, s = \"lambda.min\")\n\n# LASSO (alpha = 1)\nlasso_fit &lt;- glmnet(X, y, alpha = 1)\ncv_lasso &lt;- cv.glmnet(X, y, alpha = 1)\ncoef(cv_lasso, s = \"lambda.1se\")\n\n# Elastic Net (alpha = 0.5)\nen_fit &lt;- glmnet(X, y, alpha = 0.5)\ncv_en &lt;- cv.glmnet(X, y, alpha = 0.5)\n\n# Predictions\npredict(cv_lasso, newx = X_new, s = \"lambda.min\")",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#choosing-alpha",
    "href": "chapters/12_regularization.html#choosing-alpha",
    "title": "Regularization for Macro",
    "section": "Choosing Alpha",
    "text": "Choosing Alpha\nUse nested cross-validation:\n# Grid search over alpha\nalpha_grid &lt;- seq(0, 1, 0.1)\ncv_errors &lt;- sapply(alpha_grid, function(a) {\n  cv_fit &lt;- cv.glmnet(X, y, alpha = a)\n  min(cv_fit$cvm)\n})\nbest_alpha &lt;- alpha_grid[which.min(cv_errors)]",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#time-series-considerations",
    "href": "chapters/12_regularization.html#time-series-considerations",
    "title": "Regularization for Macro",
    "section": "Time Series Considerations",
    "text": "Time Series Considerations\nStandard CV is problematic for time series (breaks temporal structure).\nSolutions:\n\nRolling window CV: Train on \\(t=1,\\ldots,s\\), test on \\(s+1,\\ldots,s+h\\)\nExpanding window CV: Train on \\(t=1,\\ldots,s\\), test on \\(s+1\\), then expand\nBlock bootstrap: Resample blocks to preserve autocorrelation\n\n\n\nCode\n# Illustrate rolling window CV\nT_total &lt;- 100\ntrain_size &lt;- 60\nh &lt;- 10  # horizon\n\ncv_scheme &lt;- tibble(\n  fold = rep(1:3, each = T_total),\n  t = rep(1:T_total, 3),\n  set = NA_character_\n)\n\nfor (f in 1:3) {\n  train_end &lt;- train_size + (f - 1) * h\n  test_end &lt;- train_end + h\n  cv_scheme &lt;- cv_scheme %&gt;%\n    mutate(set = case_when(\n      fold == f & t &lt;= train_end ~ \"Train\",\n      fold == f & t &gt; train_end & t &lt;= test_end ~ \"Test\",\n      TRUE ~ set\n    ))\n}\n\nggplot(cv_scheme %&gt;% filter(!is.na(set)), aes(x = t, y = factor(fold), fill = set)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"#2ecc71\", \"#e74c3c\")) +\n  labs(title = \"Rolling Window Cross-Validation\",\n       subtitle = \"Train window moves forward; test window follows\",\n       x = \"Time\", y = \"CV Fold\", fill = NULL) +\n  theme_minimal()\n\n\n\n\n\nTime series cross-validation schemes",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#regularization-methods",
    "href": "chapters/12_regularization.html#regularization-methods",
    "title": "Regularization for Macro",
    "section": "Regularization Methods",
    "text": "Regularization Methods\n\nTibshirani (1996) “Regression Shrinkage and Selection via the Lasso” JRSS-B\nZou & Hastie (2005) “Regularization and Variable Selection via the Elastic Net” JRSS-B\nHastie, Tibshirani & Wainwright (2015) Statistical Learning with Sparsity — Comprehensive textbook",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#high-dimensional-inference",
    "href": "chapters/12_regularization.html#high-dimensional-inference",
    "title": "Regularization for Macro",
    "section": "High-Dimensional Inference",
    "text": "High-Dimensional Inference\n\nBelloni, Chernozhukov & Hansen (2014) “Inference on Treatment Effects after Selection” RES — Double LASSO\nChernozhukov et al. (2018) “Double/Debiased Machine Learning” Econometrics Journal\nAthey & Imbens (2019) “Machine Learning Methods That Economists Should Know About” Annual Review",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#macro-applications",
    "href": "chapters/12_regularization.html#macro-applications",
    "title": "Regularization for Macro",
    "section": "Macro Applications",
    "text": "Macro Applications\n\nBanbura, Giannone & Reichlin (2010) “Large Bayesian VARs” JAE\nStock & Watson (2012) “Generalized Shrinkage Methods for Forecasting” JAE\nGiannone, Lenza & Primiceri (2021) “Economic Predictions with Big Data” JAE",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/12_regularization.html#software",
    "href": "chapters/12_regularization.html#software",
    "title": "Regularization for Macro",
    "section": "Software",
    "text": "Software\n\nglmnet: Friedman, Hastie & Tibshirani — R package for penalized regression\nhdm: Chernozhukov et al. — High-dimensional metrics in R",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regularization for Macro</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html",
    "href": "chapters/13_causal_ml.html",
    "title": "Causal Machine Learning",
    "section": "",
    "text": "From Prediction to Causation\nMachine learning excels at prediction: minimizing \\(\\mathbb{E}[(Y - \\hat{f}(X))^2]\\). But economists care about causation: what happens to \\(Y\\) if we intervene on \\(X\\)?",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#key-papers",
    "href": "chapters/13_causal_ml.html#key-papers",
    "title": "Causal Machine Learning",
    "section": "Key Papers",
    "text": "Key Papers\n\n\n\n\n\n\n\nPaper\nContribution\n\n\n\n\nAthey & Imbens (2016)\nRecursive partitioning for heterogeneous causal effects\n\n\nWager & Athey (2018)\nCausal forests with valid asymptotic inference\n\n\nChernozhukov et al. (2018)\nDouble/Debiased ML for high-dimensional controls\n\n\nKünzel et al. (2019)\nMeta-learners (X-learner) for CATE\n\n\nAthey & Wager (2021)\nPolicy learning with observational data",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#setup",
    "href": "chapters/13_causal_ml.html#setup",
    "title": "Causal Machine Learning",
    "section": "Setup",
    "text": "Setup\nFor each unit \\(i\\):\n\nTreatment: \\(W_i \\in \\{0, 1\\}\\)\nPotential outcomes: \\(Y_i(0), Y_i(1)\\) — what would happen under control/treatment\nObserved outcome: \\(Y_i = W_i \\cdot Y_i(1) + (1 - W_i) \\cdot Y_i(0)\\)\nCovariates: \\(X_i\\) (pre-treatment characteristics)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#treatment-effects-taxonomy",
    "href": "chapters/13_causal_ml.html#treatment-effects-taxonomy",
    "title": "Causal Machine Learning",
    "section": "Treatment Effects Taxonomy",
    "text": "Treatment Effects Taxonomy\n\n\n\n\n\n\n\n\nEstimand\nDefinition\nInterpretation\n\n\n\n\nITE\n\\(\\tau_i = Y_i(1) - Y_i(0)\\)\nIndividual effect (unobservable)\n\n\nCATE\n\\(\\tau(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]\\)\nConditional average effect\n\n\nATE\n\\(\\mathbb{E}[\\tau_i]\\)\nAverage treatment effect\n\n\nATT\n\\(\\mathbb{E}[\\tau_i \\mid W_i = 1]\\)\nEffect on the treated",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#the-fundamental-problem-of-causal-inference",
    "href": "chapters/13_causal_ml.html#the-fundamental-problem-of-causal-inference",
    "title": "Causal Machine Learning",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nWe observe either \\(Y_i(1)\\) OR \\(Y_i(0)\\), never both. The individual treatment effect \\(\\tau_i\\) is fundamentally unidentifiable.\nSolution: Under unconfoundedness (selection on observables): \\[\n(Y_i(0), Y_i(1)) \\perp\\!\\!\\!\\perp W_i \\mid X_i\n\\]\nPlus overlap (positivity): \\(0 &lt; P(W_i = 1 \\mid X_i = x) &lt; 1\\) for all \\(x\\).",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#why-heterogeneity-matters",
    "href": "chapters/13_causal_ml.html#why-heterogeneity-matters",
    "title": "Causal Machine Learning",
    "section": "Why Heterogeneity Matters",
    "text": "Why Heterogeneity Matters\nThe ATE \\(= 2\\) could mask:\n\nSubgroup A: \\(\\tau = 5\\) (strong benefit)\nSubgroup B: \\(\\tau = -1\\) (harm)\n\nUnderstanding heterogeneity enables:\n\nTargeting: Treat those who benefit most\nMechanism identification: What drives variation?\nPolicy optimization: Maximize welfare under constraints\n\n\n\nCode\n# Simulate heterogeneous effects\nn &lt;- 1000\nX1 &lt;- rnorm(n)\nX2 &lt;- rnorm(n)\n\n# Treatment effect depends on X1\ntau_true &lt;- 2 + 1.5 * X1\n\n# Treatment assignment (randomized)\nW &lt;- rbinom(n, 1, 0.5)\n\n# Potential outcomes\nY0 &lt;- 1 + 0.5 * X1 + 0.3 * X2 + rnorm(n)\nY1 &lt;- Y0 + tau_true\nY &lt;- W * Y1 + (1 - W) * Y0\n\n# Create data frame\ndf &lt;- tibble(Y = Y, W = W, X1 = X1, X2 = X2, tau_true = tau_true)\n\n# Show heterogeneity\nggplot(df, aes(x = X1, y = tau_true)) +\n  geom_point(alpha = 0.3, color = \"#3498db\") +\n  geom_smooth(method = \"lm\", color = \"#e74c3c\", se = FALSE, linewidth = 1.2) +\n  geom_hline(yintercept = mean(tau_true), linetype = \"dashed\", color = \"#2ecc71\") +\n  annotate(\"text\", x = 2, y = mean(tau_true) + 0.3, label = \"ATE\", color = \"#2ecc71\") +\n  labs(title = \"True Treatment Effect Varies with X₁\",\n       subtitle = \"ATE masks substantial heterogeneity\",\n       x = expression(X[1]), y = \"True Treatment Effect τ(x)\") +\n  theme_minimal()\n\n\n\n\n\nTreatment effect heterogeneity: τ(x) = 2 + 1.5X₁",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#traditional-approaches-and-their-limitations",
    "href": "chapters/13_causal_ml.html#traditional-approaches-and-their-limitations",
    "title": "Causal Machine Learning",
    "section": "Traditional Approaches and Their Limitations",
    "text": "Traditional Approaches and Their Limitations\nSubgroup analysis: Pre-specify groups, estimate effects within each.\n\nProblem: Many possible subgroups → multiple testing\nProblem: Boundaries are arbitrary\n\nInteraction terms: \\(Y = \\alpha + \\tau W + \\gamma W \\cdot X + \\beta X + \\varepsilon\\)\n\nProblem: Must specify functional form\nProblem: Doesn’t scale to many \\(X\\)\n\nCausal ML: Learn \\(\\tau(x)\\) flexibly from data with valid inference.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#the-key-idea-wager-athey-2018",
    "href": "chapters/13_causal_ml.html#the-key-idea-wager-athey-2018",
    "title": "Causal Machine Learning",
    "section": "The Key Idea (Wager & Athey 2018)",
    "text": "The Key Idea (Wager & Athey 2018)\nAdapt random forests from predicting \\(\\mathbb{E}[Y|X]\\) to predicting \\(\\tau(x) = \\mathbb{E}[Y(1) - Y(0)|X=x]\\).\nKey innovations:\n\nHonest splitting: Separate samples for tree structure vs. leaf estimation\nHeterogeneity-maximizing splits: Split to maximize treatment effect variation\nValid inference: Asymptotic normality of estimates",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#algorithm",
    "href": "chapters/13_causal_ml.html#algorithm",
    "title": "Causal Machine Learning",
    "section": "Algorithm",
    "text": "Algorithm\nFor each tree \\(b = 1, \\ldots, B\\):\n\nSubsample data into tree-building (\\(\\mathcal{I}_1\\)) and estimation (\\(\\mathcal{I}_2\\)) sets\nBuild tree on \\(\\mathcal{I}_1\\): at each node, find split maximizing heterogeneity\nEstimate leaf effects using \\(\\mathcal{I}_2\\) only (honesty)\nAggregate: \\(\\hat{\\tau}(x) = \\frac{1}{B} \\sum_b \\hat{\\tau}_b(x)\\)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#r-implementation-with-grf",
    "href": "chapters/13_causal_ml.html#r-implementation-with-grf",
    "title": "Causal Machine Learning",
    "section": "R Implementation with grf",
    "text": "R Implementation with grf\n\n\nCode\nlibrary(grf)\n\n# Prepare data\nX &lt;- cbind(X1, X2)\n\n# Fit causal forest\ncf &lt;- causal_forest(\n  X = X,\n  Y = Y,\n  W = W,\n  num.trees = 2000,\n  honesty = TRUE,           # honest splitting (default)\n  tune.parameters = \"all\"   # automatic hyperparameter tuning\n)\n\n# Predict treatment effects\ntau_hat &lt;- predict(cf)$predictions\n\n# Compare to truth\ncomparison_df &lt;- tibble(\n  true = tau_true,\n  estimated = tau_hat,\n  X1 = X1\n)\n\n# Scatter: estimated vs true\np1 &lt;- ggplot(comparison_df, aes(x = true, y = estimated)) +\n  geom_point(alpha = 0.3, color = \"#3498db\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"#e74c3c\") +\n  labs(title = \"Estimated vs True CATE\",\n       x = \"True τ(x)\", y = \"Estimated τ̂(x)\") +\n  theme_minimal()\n\n# By X1\np2 &lt;- ggplot(comparison_df, aes(x = X1)) +\n  geom_point(aes(y = true), alpha = 0.2, color = \"#2ecc71\") +\n  geom_point(aes(y = estimated), alpha = 0.2, color = \"#3498db\") +\n  geom_smooth(aes(y = estimated), method = \"loess\", color = \"#e74c3c\", se = FALSE) +\n  labs(title = \"CATE by X₁\",\n       subtitle = \"Green = true, Blue = estimated\",\n       x = expression(X[1]), y = \"Treatment Effect\") +\n  theme_minimal()\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nCausal forest recovers heterogeneous treatment effects",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#key-grf-functions",
    "href": "chapters/13_causal_ml.html#key-grf-functions",
    "title": "Causal Machine Learning",
    "section": "Key grf Functions",
    "text": "Key grf Functions\nlibrary(grf)\n\n# Fit causal forest\ncf &lt;- causal_forest(X, Y, W, num.trees = 2000)\n\n# Point predictions\ntau_hat &lt;- predict(cf)$predictions\n\n# Predictions with variance (for CIs)\ntau_ci &lt;- predict(cf, estimate.variance = TRUE)\nlower &lt;- tau_ci$predictions - 1.96 * sqrt(tau_ci$variance.estimates)\nupper &lt;- tau_ci$predictions + 1.96 * sqrt(tau_ci$variance.estimates)\n\n# Average treatment effect with SE\nate &lt;- average_treatment_effect(cf, target.sample = \"all\")\ncat(\"ATE:\", ate[1], \"SE:\", ate[2], \"\\n\")\n\n# ATT\natt &lt;- average_treatment_effect(cf, target.sample = \"treated\")\n\n# Variable importance: which X drive heterogeneity?\nvi &lt;- variable_importance(cf)\n\n# Best linear projection: linear approximation of τ(x)\nblp &lt;- best_linear_projection(cf, X)\n\n# Calibration test: is there heterogeneity?\ntest_calibration(cf)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#variable-importance",
    "href": "chapters/13_causal_ml.html#variable-importance",
    "title": "Causal Machine Learning",
    "section": "Variable Importance",
    "text": "Variable Importance\nWhich covariates drive treatment effect heterogeneity?\n\n\nCode\n# Variable importance\nvi &lt;- variable_importance(cf)\nvi_df &lt;- tibble(\n  variable = c(\"X1\", \"X2\"),\n  importance = vi\n)\n\nggplot(vi_df, aes(x = reorder(variable, importance), y = importance)) +\n  geom_bar(stat = \"identity\", fill = \"#3498db\") +\n  coord_flip() +\n  labs(title = \"Variable Importance for Treatment Heterogeneity\",\n       subtitle = \"X₁ drives heterogeneity (as designed)\",\n       x = NULL, y = \"Importance\") +\n  theme_minimal()\n\n\n\n\n\nVariable importance identifies X₁ as driver of heterogeneity",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#observational-data-pre-estimated-nuisance-functions",
    "href": "chapters/13_causal_ml.html#observational-data-pre-estimated-nuisance-functions",
    "title": "Causal Machine Learning",
    "section": "Observational Data: Pre-Estimated Nuisance Functions",
    "text": "Observational Data: Pre-Estimated Nuisance Functions\nFor observational studies, pre-fit propensity and outcome models:\n# Pre-fit nuisance models (recommended for observational data)\nW.hat &lt;- predict(regression_forest(X, W))$predictions  # propensity\nY.hat &lt;- predict(regression_forest(X, Y))$predictions  # outcome\n\n# Causal forest with pre-estimated nuisance\ncf_obs &lt;- causal_forest(X, Y, W, W.hat = W.hat, Y.hat = Y.hat)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#the-problem",
    "href": "chapters/13_causal_ml.html#the-problem",
    "title": "Causal Machine Learning",
    "section": "The Problem",
    "text": "The Problem\nWhen using ML for nuisance parameters (propensity score, outcome model), regularization introduces bias that invalidates standard inference.\nExample: LASSO shrinks coefficients → biased treatment effect → invalid t-statistics.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#the-solution-chernozhukov-et-al.-2018",
    "href": "chapters/13_causal_ml.html#the-solution-chernozhukov-et-al.-2018",
    "title": "Causal Machine Learning",
    "section": "The Solution (Chernozhukov et al. 2018)",
    "text": "The Solution (Chernozhukov et al. 2018)\nTwo key ingredients:\n\nCross-fitting: Train ML on fold \\(-k\\), predict on fold \\(k\\)\nNeyman orthogonality: Use score function robust to nuisance estimation error",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#partially-linear-model",
    "href": "chapters/13_causal_ml.html#partially-linear-model",
    "title": "Causal Machine Learning",
    "section": "Partially Linear Model",
    "text": "Partially Linear Model\n\\[\nY = \\theta D + g(X) + U, \\quad \\mathbb{E}[U|X,D] = 0\n\\] \\[\nD = m(X) + V, \\quad \\mathbb{E}[V|X] = 0\n\\]\nTarget: \\(\\theta\\) (treatment effect)\nNuisance: \\(g(X)\\) (outcome confounding), \\(m(X)\\) (propensity/treatment model)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#the-algorithm",
    "href": "chapters/13_causal_ml.html#the-algorithm",
    "title": "Causal Machine Learning",
    "section": "The Algorithm",
    "text": "The Algorithm\n\nSplit data into \\(K\\) folds (typically \\(K = 5\\))\nFor each fold \\(k\\):\n\nTrain \\(\\hat{g}_{-k}(X)\\) and \\(\\hat{m}_{-k}(X)\\) on all other folds\nCompute residuals on fold \\(k\\):\n\n\\(\\tilde{Y}_i = Y_i - \\hat{g}_{-k}(X_i)\\)\n\\(\\tilde{D}_i = D_i - \\hat{m}_{-k}(X_i)\\)\n\n\nEstimate: \\(\\hat{\\theta} = \\frac{\\sum_i \\tilde{D}_i \\tilde{Y}_i}{\\sum_i \\tilde{D}_i^2}\\)\nStandard error: standard OLS formula on residualized data",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#why-it-works-orthogonality",
    "href": "chapters/13_causal_ml.html#why-it-works-orthogonality",
    "title": "Causal Machine Learning",
    "section": "Why It Works: Orthogonality",
    "text": "Why It Works: Orthogonality\nThe orthogonal moment condition: \\[\n\\psi(W; \\theta, \\eta) = (Y - g(X) - \\theta D)(D - m(X))\n\\]\nhas the property that small errors in \\(\\hat{g}, \\hat{m}\\) don’t bias \\(\\hat{\\theta}\\): \\[\n\\frac{\\partial}{\\partial \\eta} \\mathbb{E}[\\psi(W; \\theta_0, \\eta)] \\bigg|_{\\eta = \\eta_0} = 0\n\\]",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#python-implementation-with-doubleml",
    "href": "chapters/13_causal_ml.html#python-implementation-with-doubleml",
    "title": "Causal Machine Learning",
    "section": "Python Implementation with doubleml",
    "text": "Python Implementation with doubleml\n# Double ML estimation in Python\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_predict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Simulate data\nn = 2000\np = 10\nX = np.random.randn(n, p)\ntheta_true = 0.5  # true treatment effect\nD = X[:, 0] + 0.5 * X[:, 1] + np.random.randn(n)  # treatment depends on X\nY = theta_true * D + X[:, 0] + X[:, 1] + np.random.randn(n)  # outcome\n\n# Manual Double ML\ndef double_ml_plr(Y, D, X, K=5):\n    \"\"\"\n    Double ML for Partially Linear Regression\n    Y = theta * D + g(X) + U\n    D = m(X) + V\n    \"\"\"\n    n = len(Y)\n\n    # Cross-fitted predictions\n    g_hat = cross_val_predict(\n        RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n        X, Y, cv=K\n    )\n    m_hat = cross_val_predict(\n        RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n        X, D, cv=K\n    )\n\n    # Residualize\n    Y_tilde = Y - g_hat\n    D_tilde = D - m_hat\n\n    # Estimate theta\n    theta_hat = np.sum(D_tilde * Y_tilde) / np.sum(D_tilde ** 2)\n\n    # Standard error\n    residuals = Y_tilde - theta_hat * D_tilde\n    se_hat = np.sqrt(np.sum(residuals ** 2 * D_tilde ** 2) / (np.sum(D_tilde ** 2) ** 2))\n\n    return {\n        'estimate': theta_hat,\n        'se': se_hat,\n        'ci_lower': theta_hat - 1.96 * se_hat,\n        'ci_upper': theta_hat + 1.96 * se_hat\n    }\n\n# Run Double ML\nresult = double_ml_plr(Y, D, X)\n\nprint(f\"True θ: {theta_true}\")\nprint(f\"Double ML estimate: {result['estimate']:.4f}\")\nprint(f\"Standard error: {result['se']:.4f}\")\nprint(f\"95% CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#using-the-doubleml-package",
    "href": "chapters/13_causal_ml.html#using-the-doubleml-package",
    "title": "Causal Machine Learning",
    "section": "Using the DoubleML Package",
    "text": "Using the DoubleML Package\nfrom doubleml import DoubleMLPLR, DoubleMLData\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create data object\ndf = pd.DataFrame(X, columns=[f'X{i}' for i in range(p)])\ndf['Y'] = Y\ndf['D'] = D\n\ndml_data = DoubleMLData(\n    df, y_col='Y', d_cols='D',\n    x_cols=[f'X{i}' for i in range(p)]\n)\n\n# Specify learners\nml_l = RandomForestRegressor(n_estimators=500, max_depth=5)\nml_m = RandomForestRegressor(n_estimators=500, max_depth=5)\n\n# Fit\ndml_plr = DoubleMLPLR(dml_data, ml_l, ml_m, n_folds=5)\ndml_plr.fit()\nprint(dml_plr.summary)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#r-implementation",
    "href": "chapters/13_causal_ml.html#r-implementation",
    "title": "Causal Machine Learning",
    "section": "R Implementation",
    "text": "R Implementation\n\n\nCode\n# Manual Double ML for ATE (binary treatment)\ndouble_ml_ate &lt;- function(Y, W, X, K = 5) {\n  n &lt;- length(Y)\n  folds &lt;- sample(rep(1:K, length.out = n))\n\n  psi &lt;- numeric(n)\n\n  for (k in 1:K) {\n    train_idx &lt;- folds != k\n    test_idx &lt;- folds == k\n\n    # Train outcome models (T-learner style)\n    rf_1 &lt;- grf::regression_forest(X[train_idx & W == 1, , drop = FALSE],\n                                    Y[train_idx & W == 1])\n    rf_0 &lt;- grf::regression_forest(X[train_idx & W == 0, , drop = FALSE],\n                                    Y[train_idx & W == 0])\n\n    # Train propensity model\n    rf_e &lt;- grf::regression_forest(X[train_idx, , drop = FALSE],\n                                    W[train_idx])\n\n    # Predict on test fold\n    mu1_hat &lt;- predict(rf_1, X[test_idx, , drop = FALSE])$predictions\n    mu0_hat &lt;- predict(rf_0, X[test_idx, , drop = FALSE])$predictions\n    e_hat &lt;- predict(rf_e, X[test_idx, , drop = FALSE])$predictions\n    e_hat &lt;- pmax(pmin(e_hat, 0.99), 0.01)  # clip propensity\n\n    # AIPW pseudo-outcome (doubly robust)\n    Y_test &lt;- Y[test_idx]\n    W_test &lt;- W[test_idx]\n\n    psi[test_idx] &lt;- W_test * (Y_test - mu1_hat) / e_hat -\n                     (1 - W_test) * (Y_test - mu0_hat) / (1 - e_hat) +\n                     mu1_hat - mu0_hat\n  }\n\n  # ATE and SE\n  tau_hat &lt;- mean(psi)\n  se_hat &lt;- sd(psi) / sqrt(n)\n\n  list(\n    estimate = tau_hat,\n    se = se_hat,\n    ci_lower = tau_hat - 1.96 * se_hat,\n    ci_upper = tau_hat + 1.96 * se_hat\n  )\n}\n\n# Apply\ndml_result &lt;- double_ml_ate(Y, W, as.matrix(cbind(X1, X2)))\ntrue_ate &lt;- mean(tau_true)\n\ncat(\"Double ML ATE:\", round(dml_result$estimate, 3), \"\\n\")\n\n\nDouble ML ATE: 1.864 \n\n\nCode\ncat(\"SE:\", round(dml_result$se, 3), \"\\n\")\n\n\nSE: 0.084 \n\n\nCode\ncat(\"True ATE:\", round(true_ate, 3), \"\\n\")\n\n\nTrue ATE: 1.961 \n\n\nCode\ncat(\"95% CI: [\", round(dml_result$ci_lower, 3), \",\",\n    round(dml_result$ci_upper, 3), \"]\\n\")\n\n\n95% CI: [ 1.699 , 2.029 ]",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#overview",
    "href": "chapters/13_causal_ml.html#overview",
    "title": "Causal Machine Learning",
    "section": "Overview",
    "text": "Overview\nMeta-learners are strategies for combining base ML models to estimate CATE.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#t-learner-two-models",
    "href": "chapters/13_causal_ml.html#t-learner-two-models",
    "title": "Causal Machine Learning",
    "section": "T-Learner (Two Models)",
    "text": "T-Learner (Two Models)\nTrain separate models for treatment and control:\n\\[\n\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)\n\\]\n# T-Learner implementation\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Binary treatment simulation\nn = 1000\nX_sim = np.random.randn(n, 5)\nW_sim = np.random.binomial(1, 0.5, n)\ntau_sim = X_sim[:, 0] + 0.5 * X_sim[:, 1]\nY_sim = tau_sim * W_sim + X_sim[:, 0] + np.random.randn(n)\n\n# T-Learner: separate models for treatment and control\nmodel_0 = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel_1 = RandomForestRegressor(n_estimators=100, random_state=42)\n\nmodel_0.fit(X_sim[W_sim == 0], Y_sim[W_sim == 0])\nmodel_1.fit(X_sim[W_sim == 1], Y_sim[W_sim == 1])\n\ntau_t = model_1.predict(X_sim) - model_0.predict(X_sim)\n\nprint(f\"T-Learner correlation with true τ: {np.corrcoef(tau_sim, tau_t)[0,1]:.3f}\")\nPros: Simple, no propensity needed\nCons: High variance, especially with imbalanced treatment",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#s-learner-single-model",
    "href": "chapters/13_causal_ml.html#s-learner-single-model",
    "title": "Causal Machine Learning",
    "section": "S-Learner (Single Model)",
    "text": "S-Learner (Single Model)\nSingle model with treatment as feature:\n\\[\n\\hat{\\mu}(x, w) \\rightarrow \\hat{\\tau}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0)\n\\]\n# S-Learner: single model with treatment as feature\nX_aug = np.column_stack([X_sim, W_sim])\nmodel_s = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel_s.fit(X_aug, Y_sim)\n\n# Predict under treatment and control\ntau_s = (model_s.predict(np.column_stack([X_sim, np.ones(n)])) -\n         model_s.predict(np.column_stack([X_sim, np.zeros(n)])))\n\nprint(f\"S-Learner correlation with true τ: {np.corrcoef(tau_sim, tau_s)[0,1]:.3f}\")\nPros: Simple, regularization shared\nCons: Treatment effect can be shrunk to zero",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#x-learner-künzel-et-al.-2019",
    "href": "chapters/13_causal_ml.html#x-learner-künzel-et-al.-2019",
    "title": "Causal Machine Learning",
    "section": "X-Learner (Künzel et al. 2019)",
    "text": "X-Learner (Künzel et al. 2019)\nBest for imbalanced treatment (few treated or few controls):\n\nFit \\(\\hat{\\mu}_0, \\hat{\\mu}_1\\) (T-learner)\nImpute treatment effects:\n\nTreated: \\(\\tilde{D}_1 = Y_1 - \\hat{\\mu}_0(X_1)\\)\nControl: \\(\\tilde{D}_0 = \\hat{\\mu}_1(X_0) - Y_0\\)\n\nFit models: \\(\\hat{\\tau}_0(x), \\hat{\\tau}_1(x)\\)\nCombine: \\(\\hat{\\tau}(x) = e(x) \\hat{\\tau}_0(x) + (1-e(x)) \\hat{\\tau}_1(x)\\)",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#comparison",
    "href": "chapters/13_causal_ml.html#comparison",
    "title": "Causal Machine Learning",
    "section": "Comparison",
    "text": "Comparison\n\n\n\n\n\n\n\n\nLearner\nBest When\nWeakness\n\n\n\n\nT-Learner\nBalanced, large samples\nHigh variance\n\n\nS-Learner\nSmall effects, regularization needed\nShrinks effects\n\n\nX-Learner\nImbalanced treatment\nComplex, needs propensity",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#evaluating-heterogeneity",
    "href": "chapters/13_causal_ml.html#evaluating-heterogeneity",
    "title": "Causal Machine Learning",
    "section": "Evaluating Heterogeneity",
    "text": "Evaluating Heterogeneity\nGATES groups units by predicted CATE and estimates average effects within each group:\n\n\nCode\n# GATES analysis\ntau_hat &lt;- predict(cf)$predictions\n\n# Create quartiles\ndf &lt;- df %&gt;%\n  mutate(\n    tau_hat = tau_hat,\n    quartile = cut(tau_hat,\n                   breaks = quantile(tau_hat, c(0, 0.25, 0.5, 0.75, 1)),\n                   labels = c(\"Q1 (lowest)\", \"Q2\", \"Q3\", \"Q4 (highest)\"),\n                   include.lowest = TRUE)\n  )\n\n# Compute GATES\ngates &lt;- df %&gt;%\n  group_by(quartile) %&gt;%\n  summarise(\n    mean_tau_true = mean(tau_true),\n    mean_tau_hat = mean(tau_hat),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# Plot\nggplot(gates, aes(x = quartile)) +\n  geom_bar(aes(y = mean_tau_true, fill = \"True\"), stat = \"identity\",\n           position = position_dodge(width = 0.8), width = 0.35) +\n  geom_bar(aes(y = mean_tau_hat, fill = \"Estimated\"), stat = \"identity\",\n           position = position_dodge(width = 0.8), width = 0.35) +\n  scale_fill_manual(values = c(\"True\" = \"#2ecc71\", \"Estimated\" = \"#3498db\")) +\n  labs(title = \"Group Average Treatment Effects (GATES)\",\n       subtitle = \"Causal forest correctly ranks effect heterogeneity\",\n       x = \"Quartile of Predicted CATE\", y = \"Average Treatment Effect\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nGATES: Average effects by predicted CATE quartile",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#best-linear-predictor-blp",
    "href": "chapters/13_causal_ml.html#best-linear-predictor-blp",
    "title": "Causal Machine Learning",
    "section": "Best Linear Predictor (BLP)",
    "text": "Best Linear Predictor (BLP)\nWhich covariates explain heterogeneity?\n\n\nCode\n# Best linear projection\nblp &lt;- best_linear_projection(cf, X)\nprint(blp)\n\n\n\nBest linear projection of the conditional average treatment effect.\nConfidence intervals are cluster- and heteroskedasticity-robust (HC3):\n\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.923589   0.067767 28.3853   &lt;2e-16 ***\nX1           1.522921   0.074177 20.5308   &lt;2e-16 ***\nX2          -0.038501   0.071713 -0.5369   0.5915    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Visualize\nblp_df &lt;- tibble(\n  variable = c(\"Intercept\", \"X1\", \"X2\"),\n  estimate = blp[, 1],\n  se = blp[, 2]\n)\n\nggplot(blp_df, aes(x = variable, y = estimate)) +\n  geom_point(size = 3, color = \"#3498db\") +\n  geom_errorbar(aes(ymin = estimate - 1.96 * se, ymax = estimate + 1.96 * se),\n                width = 0.2, color = \"#3498db\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Best Linear Predictor of CATE\",\n       subtitle = \"X₁ significantly predicts heterogeneity (coefficient ≈ 1.5)\",\n       x = NULL, y = \"Coefficient\") +\n  theme_minimal()\n\n\n\n\n\nBest linear predictor identifies X₁ as heterogeneity driver",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#overview-1",
    "href": "chapters/13_causal_ml.html#overview-1",
    "title": "Causal Machine Learning",
    "section": "Overview",
    "text": "Overview\nEconML provides a unified API for heterogeneous treatment effect estimation in Python.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#key-estimators",
    "href": "chapters/13_causal_ml.html#key-estimators",
    "title": "Causal Machine Learning",
    "section": "Key Estimators",
    "text": "Key Estimators\n\n\n\nEstimator\nDescription\n\n\n\n\nLinearDML\nDML with linear final stage\n\n\nCausalForestDML\nCausal forest with DML\n\n\nForestDRLearner\nDoubly robust forest\n\n\nOrthoIV\nOrthogonal IV learner\n\n\nDynamicDML\nPanel data",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#python-implementation",
    "href": "chapters/13_causal_ml.html#python-implementation",
    "title": "Causal Machine Learning",
    "section": "Python Implementation",
    "text": "Python Implementation\n# EconML example\nfrom econml.dml import LinearDML, CausalForestDML\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\n# Setup\nX_hetero = X_sim[:, :3]  # features for heterogeneity\nW_confound = X_sim[:, 3:]  # confounders\n\n# LinearDML\nest_linear = LinearDML(\n    model_y=RandomForestRegressor(n_estimators=100),\n    model_t=RandomForestClassifier(n_estimators=100),\n    cv=5\n)\nest_linear.fit(Y_sim, W_sim, X=X_hetero, W=W_confound)\n\n# Effects\ntau_linear = est_linear.effect(X_hetero)\n\n# Inference\ntau_lower, tau_upper = est_linear.effect_interval(X_hetero, alpha=0.05)\n\n# Summary\nprint(est_linear.summary())",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#unified-api-pattern",
    "href": "chapters/13_causal_ml.html#unified-api-pattern",
    "title": "Causal Machine Learning",
    "section": "Unified API Pattern",
    "text": "Unified API Pattern\nAll EconML estimators follow:\nest.fit(Y, T, X=X, W=W)           # fit (Y=outcome, T=treatment, X=hetero, W=confound)\nest.effect(X_test)                 # point estimates\nest.effect_interval(X_test)        # confidence intervals\nest.summary()                      # inference summary",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#heterogeneous-policy-effects",
    "href": "chapters/13_causal_ml.html#heterogeneous-policy-effects",
    "title": "Causal Machine Learning",
    "section": "Heterogeneous Policy Effects",
    "text": "Heterogeneous Policy Effects\nQuestion: How do monetary policy effects vary across firms/regions/time?\nApproach:\n\nIdentify policy shocks (high-frequency, narrative)\nEstimate heterogeneous effects using causal forests\nCharacterize which observables predict sensitivity",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#example-cross-country-monetary-transmission",
    "href": "chapters/13_causal_ml.html#example-cross-country-monetary-transmission",
    "title": "Causal Machine Learning",
    "section": "Example: Cross-Country Monetary Transmission",
    "text": "Example: Cross-Country Monetary Transmission\n# Do bank holdings predict constrained monetary response?\ncf_policy &lt;- causal_forest(\n  X = cbind(bank_holdings, cbi_index, debt_gdp, trade_openness),\n  Y = delta_inflation,\n  W = tightening_dummy\n)\n\n# Which characteristics drive heterogeneity?\nvariable_importance(cf_policy)\n\n# Linear approximation\nbest_linear_projection(cf_policy,\n                       cbind(bank_holdings, cbi_index, debt_gdp))",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#fiscal-multiplier-heterogeneity",
    "href": "chapters/13_causal_ml.html#fiscal-multiplier-heterogeneity",
    "title": "Causal Machine Learning",
    "section": "Fiscal Multiplier Heterogeneity",
    "text": "Fiscal Multiplier Heterogeneity\nDo fiscal multipliers vary by:\n\nSlack (output gap)?\nMonetary policy stance (ZLB)?\nDebt levels?\n\nCausal forests can flexibly estimate: \\[\n\\text{Multiplier}(x) = \\mathbb{E}[\\Delta Y \\mid \\text{Fiscal shock}, X = x]\n\\]",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#sample-size-requirements",
    "href": "chapters/13_causal_ml.html#sample-size-requirements",
    "title": "Causal Machine Learning",
    "section": "Sample Size Requirements",
    "text": "Sample Size Requirements\n\n\n\nMethod\nMinimum N\nRecommended N\n\n\n\n\nATE (Double ML)\n200\n500+\n\n\nCATE (Causal Forest)\n500\n2000+\n\n\nGATES\n1000\n3000+\n\n\nVariable Importance\n2000\n5000+",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#diagnostics",
    "href": "chapters/13_causal_ml.html#diagnostics",
    "title": "Causal Machine Learning",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nOverlap Check\n# Propensity score distribution\ne_hat &lt;- cf$W.hat\nhist(e_hat, breaks = 50, main = \"Propensity Scores\")\nabline(v = c(0.1, 0.9), col = \"red\", lty = 2)\n\n# Extreme values\ncat(\"Extreme propensity:\", mean(e_hat &lt; 0.1 | e_hat &gt; 0.9) * 100, \"%\\n\")\n\n\nCalibration Test\n# Is there actually heterogeneity?\ntest_calibration(cf)\n# Look for significant \"differential.forest.prediction\"\n\n\nAUTOC (Targeting Quality)\n# Area Under the TOC Curve\nrate &lt;- rank_average_treatment_effect(cf, X[, 1])\nprint(rate)  # CI should exclude 0",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#common-pitfalls",
    "href": "chapters/13_causal_ml.html#common-pitfalls",
    "title": "Causal Machine Learning",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nCausal ML doesn’t solve identification: Still need unconfoundedness\nOverfitting CATE: Use honest forests, cross-validation\nNoise as heterogeneity: Run calibration tests\nOverlap violations: Check propensity scores, trim extremes\nSmall samples: CATE unreliable with N &lt; 500",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#foundational",
    "href": "chapters/13_causal_ml.html#foundational",
    "title": "Causal Machine Learning",
    "section": "Foundational",
    "text": "Foundational\n\nAthey & Imbens (2016) “Recursive Partitioning for Heterogeneous Causal Effects” PNAS\nWager & Athey (2018) “Estimation and Inference of Heterogeneous Treatment Effects using Random Forests” JASA\nChernozhukov et al. (2018) “Double/Debiased Machine Learning” Econometrics Journal",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#extensions",
    "href": "chapters/13_causal_ml.html#extensions",
    "title": "Causal Machine Learning",
    "section": "Extensions",
    "text": "Extensions\n\nKünzel et al. (2019) “Metalearners for Heterogeneous Treatment Effects” PNAS\nAthey & Wager (2021) “Policy Learning with Observational Data” Econometrica\nKennedy (2022) “Optimal Doubly Robust Estimation of Heterogeneous Causal Effects”",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/13_causal_ml.html#resources",
    "href": "chapters/13_causal_ml.html#resources",
    "title": "Causal Machine Learning",
    "section": "Resources",
    "text": "Resources\n\nCausal ML Book: https://causalml-book.org/\nML for Economists: https://github.com/ml4econ/lecture-notes-2025\ngrf Documentation: https://grf-labs.github.io/grf/\nEconML: https://github.com/py-why/EconML\nDoubleML: https://github.com/DoubleML/doubleml-for-py\n\n\n\n\n\n\n\nAthey, Susan, and Guido W Imbens. 2019. “Machine Learning Methods That Economists Should Know About.” Annual Review of Economics 11: 685–725.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters.” The Econometrics Journal 21 (1): C1–68.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42.",
    "crumbs": [
      "Phase 6: Causal Machine Learning",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Causal Machine Learning</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\nAthey, Susan, and Guido W Imbens. 2019. “Machine Learning Methods\nThat Economists Should Know About.” Annual Review of\nEconomics 11: 685–725.\n\n\nBlanchard, Olivier Jean, and Charles M Kahn. 1980. “The Solution\nof Linear Difference Models Under Rational Expectations.”\nEconometrica 48 (5): 1305–11.\n\n\nCallaway, Brantly, and Pedro HC Sant’Anna. 2021.\n“Difference-in-Differences with Multiple Time Periods.”\nJournal of Econometrics 225 (2): 200–230.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo,\nChristian Hansen, Whitney Newey, and James Robins. 2018.\n“Double/Debiased Machine Learning for Treatment and Structural\nParameters.” The Econometrics Journal 21 (1): C1–68.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254–77.\n\n\nJordà, Òscar. 2005. “Estimation and Inference of Impulse Responses\nby Local Projections.” American Economic Review 95 (1):\n161–82.\n\n\nKilian, Lutz, and Helmut Lütkepohl. 2017. Structural Vector\nAutoregressive Analysis. Cambridge University Press.\n\n\nRoth, Jonathan, Pedro HC Sant’Anna, Alyssa Bilinski, and John Poe. 2023.\n“What’s Trending in Difference-in-Differences? A Synthesis of the\nRecent Econometrics Literature.” Journal of Econometrics\n235 (2): 2218–44.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of\nHeterogeneous Treatment Effects Using Random Forests.”\nJournal of the American Statistical Association 113 (523):\n1228–42.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>References</span>"
    ]
  }
]